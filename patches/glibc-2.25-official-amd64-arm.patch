diff -N -r --unified glibc-2.25.orig/csu/libc-start.c glibc-2.25/csu/libc-start.c
--- glibc-2.25.orig/csu/libc-start.c	2017-09-11 17:14:55.241208737 +0000
+++ glibc-2.25/csu/libc-start.c	2017-09-13 19:55:55.826730718 +0000
@@ -104,6 +104,14 @@
 # define MAIN_AUXVEC_PARAM
 #endif
 
+#ifdef USE_MVEE_LIBC
+# ifdef MVEE_USE_TOTALPARTIAL_AGENT
+#  include "mvee-totalpartial-agent.c"
+# else
+#  include "mvee-woc-agent.c"
+# endif
+#endif
+
 STATIC int LIBC_START_MAIN (int (*main) (int, char **, char **
 					 MAIN_AUXVEC_DECL),
 			    int argc,
@@ -274,6 +282,11 @@
 #ifndef SHARED
   _dl_debug_initialize (0, LM_ID_BASE);
 #endif
+
+  (void) syscall(MVEE_RUNS_UNDER_MVEE_CONTROL, &mvee_sync_enabled, &mvee_infinite_loop, 
+				 NULL, NULL, &mvee_master_variant);
+  mvee_libc_initialized = 1;
+
 #ifdef HAVE_CLEANUP_JMP_BUF
   /* Memory for the cancellation buffer.  */
   struct pthread_unwind_buf unwind_buf;
diff -N -r --unified glibc-2.25.orig/csu/mvee-totalpartial-agent.c glibc-2.25/csu/mvee-totalpartial-agent.c
--- glibc-2.25.orig/csu/mvee-totalpartial-agent.c	1970-01-01 00:00:00.000000000 +0000
+++ glibc-2.25/csu/mvee-totalpartial-agent.c	2017-09-13 19:55:55.839732858 +0000
@@ -0,0 +1,879 @@
+static __thread int   mvee_master_thread_id         = 0;
+static unsigned char  mvee_sync_enabled             = 0;
+static unsigned char  mvee_libc_initialized         = 0;
+static unsigned char  mvee_master_variant           = 0;
+static unsigned char  mvee_buffer_valid             = 0;
+static unsigned short mvee_num_childs               = 0;
+static unsigned short mvee_child_num                = 0;
+#ifdef MVEE_EXTENDED_QUEUE
+static unsigned short mvee_op_number                = 1;
+#endif
+#ifdef MVEE_LOG_EIPS
+DEFINE_MVEE_QUEUE(lock, 1);
+#else
+DEFINE_MVEE_QUEUE(lock, 0);
+#endif
+static __thread unsigned long  mvee_prev_flush_cnt           = 0;
+static __thread unsigned long  mvee_lock_buffer_prev_pos     = 0;
+#if defined(MVEE_EXTENDED_QUEUE) && defined(MVEE_CHECK_LOCK_TYPE)
+static __thread unsigned long  mvee_original_call_site       = 0;
+#endif
+
+#define likely(x)       __builtin_expect((x),1)
+#define unlikely(x)     __builtin_expect((x),0)
+
+#ifdef MVEE_DEBUG_MALLOC
+DEFINE_MVEE_QUEUE(malloc, 1);
+#endif
+
+/* MVEE PATCH:
+   Checks wether or not all variants got ALIGNMENT aligned heaps from
+   the previous mmap request. If some of them have not, ALL variants
+   have to bail out and fall back to another heap allocation method.
+   This ensures that the variants stay in sync with respect to future mm
+   requests.
+*/
+#define ALIGNMENT 0x4000000
+
+int
+mvee_all_heaps_aligned(char* heap, unsigned long alloc_size)
+{
+  // if we're not running under MVEE control,
+  // just check the alignment of the current heap
+  if (!mvee_master_thread_id)
+  {
+      if ((unsigned long)heap & (ALIGNMENT-1))
+		  return 0;
+      return 1;
+  }
+
+  // We ARE running under MVEE control
+  // => ask the MVEE to check the alignments
+  // of ALL heaps
+  return syscall(MVEE_ALL_HEAPS_ALIGNED, heap, ALIGNMENT, alloc_size);
+}
+
+/* 
+ * mvee_infinite_loop:
+ * this function is used for both thread transfering and signal delivery 
+ * 
+ * 1) to transfer threads to a new monitor, the original monitor (i.e. the 
+ * monitor that monitors the childs that instigated the fork event) needs to
+ * detach from the threads first. While the threads are detached, they can
+ * run freely, without the intervention of a debugger.
+ * As such, we have to move the program counter to an infinite loop while
+ * the threads are detached. This way, the threads will all be in an equivalent
+ * state when the new monitor attaches to them.
+ * Because we're going to replace the registers by their original contents
+ * when the new monitor attaches, we can use sys_pause calls in the infinite
+ * loop.
+ * 
+ * 2) delivering signals through the ptrace API happens asynchronously 
+ * (I found out the hard way). As such, we should wait for the threads to be
+ * in equivalent states (e.g. stopped on the same syscall). Then the registers
+ * should be backed up and the syscall nr should be replaced by a harmless
+ * syscall that doesn't modify the program state. We use sys_getpid for this
+ * purpose. When that replaced syscall returns, we change the pc to this
+ * infinite loop while we wait for async signal delivery.
+ * We probably cannot use syscalls while waiting for signal delivery. 
+ * One possible exception is sys_sched_yield. Our modified MVEE kernel does
+ * not report this syscall to the ptracer
+ * 
+ * the with_syscalls parameter is passed through the ecx register!
+ */
+void mvee_infinite_loop(int with_syscalls, int dummy)
+{
+	if (with_syscalls)
+    {
+		for (;;)
+			syscall(__NR_pause);
+    }
+	else
+    {
+		for (;;)
+		{
+			dummy = dummy << 2;
+		}
+    }
+}
+
+/*
+ * logs a (truncated) stack into the specified "eip" buffer. This stack is logged for EVERY variant,
+ * which greatly facilitates debugging. The MVEE can dump the contents of this buffer very efficiently, 
+ * including source lines/info
+ * 
+ * Do note that __builtin_return_address() can trap (SEGV) if the entire stack contains less than
+ * MVEE_MAX_STACK_DEPTH entries. The MVEE will however trap the SEGV and if it comes from this function,
+ * the offending instruction will be skipped.
+ */
+static void __attribute__ ((noinline)) mvee_log_stack(void* eip_buffer, int eip_buffer_slot_size, volatile unsigned int* eip_buffer_pos_ptr, int start_depth)
+{
+#ifdef MVEE_LOG_EIPS
+	int entries_logged = 0;
+	int next_entry = start_depth;
+
+	while (entries_logged < MVEE_STACK_DEPTH)
+    {
+		unsigned long ret_addr = 0;
+		switch (next_entry)
+		{
+			// __builtin_* intrinics need const arguments so unfortunately, we have to do the following...
+#define DEF_CASE(x)														\
+			case x:														\
+				ret_addr = (unsigned long)__builtin_return_address(x);	\
+				break;
+			DEF_CASE(0);
+#if defined(MVEE_EXTENDED_QUEUE) && defined (MVEE_CHECK_LOCK_TYPE)
+			case 1:
+				ret_addr = (unsigned long)mvee_original_call_site;
+				break;
+#else
+			DEF_CASE(1);
+#endif
+
+#if MVEE_STACK_DEPTH > 1
+			DEF_CASE(2);
+# if MVEE_STACK_DEPTH > 2
+			DEF_CASE(3);
+#  if MVEE_STACK_DEPTH > 3
+			DEF_CASE(4);
+#   if MVEE_STACK_DEPTH > 4
+			DEF_CASE(5);
+#    if MVEE_STACK_DEPTH > 5
+			DEF_CASE(6);
+#     if MVEE_STACK_DEPTH > 6
+			DEF_CASE(7);
+#      if MVEE_STACK_DEPTH > 7
+			DEF_CASE(8);
+#       if MVEE_STACK_DEPTH > 8
+			DEF_CASE(9);
+#        if MVEE_STACK_DEPTH > 9
+			DEF_CASE(10);
+#        endif
+#       endif
+#      endif
+#     endif
+#    endif
+#   endif
+#  endif
+# endif
+#endif
+		}
+		*(unsigned long*)((unsigned long)eip_buffer + 
+						  eip_buffer_slot_size * (*eip_buffer_pos_ptr) + 
+						  MVEE_STACK_DEPTH * sizeof(unsigned long) * mvee_child_num + 
+						  sizeof(unsigned long) * entries_logged) 
+			= ret_addr;
+		entries_logged++;
+		next_entry++;
+    }
+#endif
+}
+
+static void mvee_check_buffer(void)
+{
+	register unsigned short tid asm("al") = mvee_master_thread_id;
+	if (unlikely(!tid))
+    {
+		int tmp_id;
+		__asm__ volatile("int $0x80" : "=a" (tmp_id) : "0" (MVEE_GET_MASTERTHREAD_ID) : "memory", "cc");
+		mvee_master_thread_id = tmp_id;
+
+		if (!mvee_buffer_valid)
+		{
+			mvee_buffer_valid = 1;
+#ifdef MVEE_PARTIAL_ORDER_REPLICATION
+			INIT_MVEE_QUEUE(lock, MVEE_LOCK_QUEUE_SLOT_SIZE, MVEE_LIBC_LOCK_BUFFER_PARTIAL);
+#else
+			INIT_MVEE_QUEUE(lock, MVEE_LOCK_QUEUE_SLOT_SIZE, MVEE_LIBC_LOCK_BUFFER);
+#endif
+
+#ifdef MVEE_DEBUG_MALLOC
+			INIT_MVEE_QUEUE(malloc, 4*sizeof(int) + 3*sizeof(long), MVEE_LIBC_MALLOC_DEBUG_BUFFER);
+#endif
+		}
+    }
+}
+
+
+// This function is a bit tricky, especially on x86_64!
+// In some contexts, such as syscalls that enable asynchronous cancellation,
+// libc expects none of the code it executes to touch registers other than
+// %rax and %r11. Consequently, we have to make sure that at most 2 registers
+// live at any point during our mvee funcs!
+static inline int mvee_should_sync(void)
+{
+	if (unlikely(!mvee_libc_initialized))
+	{
+		long res = syscall(MVEE_RUNS_UNDER_MVEE_CONTROL, &mvee_sync_enabled, &mvee_infinite_loop, 
+						   &mvee_num_childs, &mvee_child_num, &mvee_master_variant);
+		if (!(res < 0 && res > -4095))
+			mvee_check_buffer();
+		mvee_libc_initialized = 1;
+	}
+	return mvee_sync_enabled;
+}
+
+
+int mvee_should_sync_tid(void)
+{
+	return mvee_should_sync();
+}
+
+// the buffer initialization is NOT thread-safe. Therefore, it must be initialized
+// in single threaded context!!!
+void mvee_invalidate_buffer(void)
+{
+	mvee_buffer_valid = 0;
+	mvee_master_thread_id = 0;
+}
+
+#define cpu_relax() asm volatile("rep; nop" ::: "memory")
+
+#define gcc_barrier() asm volatile("" ::: "memory")
+
+static inline unsigned int mvee_write_lock_result_prepare(void)
+{
+	while (1)
+    {
+		if (orig_atomic_decrement_and_test(mvee_lock_buffer_lock))
+			return *mvee_lock_buffer_pos;
+      
+		while (*mvee_lock_buffer_lock <= 0)
+			cpu_relax();
+    }
+
+	return *mvee_lock_buffer_pos;
+}
+
+static inline void mvee_write_lock_result_finish(void)
+{
+	gcc_barrier();
+	orig_atomic_increment(mvee_lock_buffer_pos);
+	*mvee_lock_buffer_lock = 1;
+}
+
+#ifdef MVEE_EXTENDED_QUEUE
+static inline void mvee_write_lock_result_write(unsigned int pos, unsigned short op_type, void* word_ptr, unsigned char is_store)
+#else
+	static inline void mvee_write_lock_result_write(unsigned int pos, void* word_ptr, unsigned char is_store)
+#endif
+{
+restart:
+	if (likely(pos < mvee_lock_buffer_slots))
+    {
+#if defined(MVEE_LOG_EIPS) && defined(MVEE_EXTENDED_QUEUE)
+		MVEE_LOG_STACK(lock, 1, &pos);
+#endif
+		gcc_barrier();
+
+#ifdef MVEE_PARTIAL_ORDER_REPLICATION
+		MVEE_LOG_QUEUE_DATA(lock, pos, 0, (unsigned long)(((unsigned long)word_ptr) | is_store));
+# ifdef MVEE_EXTENDED_QUEUE
+		MVEE_LOG_QUEUE_DATA(lock, pos, sizeof(long) + sizeof(short), op_type);
+# endif
+		MVEE_LOG_QUEUE_DATA(lock, pos, sizeof(long), ((unsigned short)mvee_master_thread_id));
+#else // MVEE_TOTAL_ORDER_REPLICATION
+# ifdef MVEE_EXTENDED_QUEUE
+		MVEE_LOG_QUEUE_DATA(lock, pos, sizeof(long), ((unsigned long)word_ptr));
+		MVEE_LOG_QUEUE_DATA(lock, pos, sizeof(short), op_type);
+# endif
+		MVEE_LOG_QUEUE_DATA(lock, pos, 0, ((unsigned short)mvee_master_thread_id));
+#endif // !MVEE_PARTIAL_ORDER_REPLICATION
+    }
+	else
+    {      
+		// we log the tid of the flushing thread into the last slot
+#ifdef MVEE_PARTIAL_ORDER_REPLICATION
+		MVEE_LOG_QUEUE_DATA(lock, pos, sizeof(long), ((unsigned short)mvee_master_thread_id));
+		syscall(MVEE_FLUSH_SHARED_BUFFER, MVEE_LIBC_LOCK_BUFFER_PARTIAL);
+#else
+		MVEE_LOG_QUEUE_DATA(lock, pos, 0, ((unsigned short)mvee_master_thread_id));
+		syscall(MVEE_FLUSH_SHARED_BUFFER, MVEE_LIBC_LOCK_BUFFER);
+#endif
+		*mvee_lock_buffer_pos = pos = 0;
+		goto restart;
+    }
+}
+
+static inline unsigned char mvee_op_is_tagged(unsigned long pos)
+{
+#ifdef MVEE_EXTENDED_QUEUE
+	unsigned short tagged;
+	MVEE_READ_QUEUE_DATA(lock, pos, sizeof(long) + sizeof(short) * (mvee_child_num + 1), tagged);
+#else
+	unsigned char tagged;
+	MVEE_READ_QUEUE_DATA(lock, pos, sizeof(long) + sizeof(short) + (mvee_child_num - 1), tagged);
+#endif
+	return tagged ? 1 : 0;
+}
+
+static inline unsigned char mvee_pos_still_valid(void)
+{
+	if (*mvee_lock_buffer_flushing || *mvee_lock_buffer_flush_cnt != mvee_prev_flush_cnt)
+		return 0;
+	return 1;
+}
+
+#ifdef MVEE_EXTENDED_QUEUE
+static inline void mvee_read_lock_result_wait(unsigned short op_type, void* word_ptr)
+#else
+static inline void mvee_read_lock_result_wait(void)
+#endif
+{
+#ifdef MVEE_PARTIAL_ORDER_REPLICATION
+	unsigned char is_store = 0;
+	unsigned char all_tagged;
+	unsigned long master_word_ptr;
+	unsigned int i;
+	unsigned int orig_pos;
+	unsigned int nextpos = 0;
+	unsigned int temppos;
+
+	while(true)
+	{
+		// STEP 1: FIND THE CORRESPONDING MASTER WORD POINTER
+		//
+		// Our algorithm relies on the fact that all replicae
+		// are semantically equivalent.
+		//
+		// Therefore, we may safely assume that the first non-tagged
+		// operation of this thread's corresponding master thread.
+		// is an operation on the same logical word.
+		//
+		// Caching:
+		// We start the search at the previously replicated operation 
+		// by this thread. The index of the previously replicated
+		// operation is stored in mvee_lock_buffer_prev_pos.
+		// It remains valid until the queue is flushed.
+		// When the queue is flushed, we increment the flush counter
+		// which is stored in the upper 4 bytes of mvee_lock_buffer_pos.
+		//
+
+		// check if the queue has been flushed since the last operation we've replicated
+		if (unlikely(*mvee_lock_buffer_flush_cnt != mvee_prev_flush_cnt))
+		{
+			// it has been flushed, update flush cnt and reset the 
+			// position of our previously replicated operation
+			mvee_prev_flush_cnt = *mvee_lock_buffer_flush_cnt;
+			mvee_lock_buffer_prev_pos = 0;
+			nextpos = 0;
+		}
+
+		temppos = *mvee_lock_buffer_pos;
+
+		// Meanwhile, some other thread _MAY_ have moved the position
+		// pointer past our previously replicated operation
+		//
+		// If so, we start the search there instead
+		orig_pos = temppos;
+		if (temppos < mvee_lock_buffer_prev_pos)
+			nextpos = mvee_lock_buffer_prev_pos;
+		else if (!nextpos)
+			nextpos = orig_pos;
+
+		master_word_ptr = 0;
+
+		for (temppos = nextpos; temppos <= mvee_lock_buffer_slots; ++temppos)
+		{
+			unsigned short tid;
+			MVEE_READ_QUEUE_DATA(lock, temppos, sizeof(long), tid);
+			// MVEE_READ_QUEUE_DATA(lock, temppos, 0, tid);
+
+			// no tid => the slaves are running ahead of the master
+			// or the master stores are not visible yet
+			if (!tid)
+			{
+				master_word_ptr = 0;
+				break;
+			}
+
+			if (tid != (unsigned short)mvee_master_thread_id || mvee_op_is_tagged(temppos))
+				continue;
+
+			// if tid is visible, then this will be too
+			MVEE_READ_QUEUE_DATA(lock, temppos, 0, master_word_ptr);
+			if (master_word_ptr & 1)
+				is_store = 1;
+			master_word_ptr &= ~(sizeof(long) - 1);
+
+			// this will only happen if we're at the end of the queue
+			// at which point we log a pseudo-operation with master_word_ptr == 0
+			if (!master_word_ptr)
+			{
+				if (temppos == mvee_lock_buffer_slots)
+				{
+					while (true)
+					{
+						all_tagged = 1;
+
+						for (temppos = orig_pos; temppos < mvee_lock_buffer_slots; ++temppos)
+						{		    
+							if (!mvee_op_is_tagged(temppos))
+							{
+								all_tagged = 0;
+								orig_pos = temppos;
+								break;
+							}
+						}
+
+						if (!all_tagged)
+							syscall(__NR_sched_yield);
+						else
+							break;
+					}
+
+					*mvee_lock_buffer_flushing = 1;
+					atomic_full_barrier();
+
+					syscall(MVEE_FLUSH_SHARED_BUFFER, MVEE_LIBC_LOCK_BUFFER_PARTIAL);
+
+#ifdef MVEE_EXTENDED_QUEUE
+					mvee_op_number = 1;
+#endif
+
+					*mvee_lock_buffer_pos = 0;
+					atomic_full_barrier();
+					*mvee_lock_buffer_flush_cnt = (++mvee_prev_flush_cnt);
+					*mvee_lock_buffer_flushing = 0;
+
+					temppos = mvee_lock_buffer_prev_pos = 0;
+					break;
+				}
+
+				// we could also see a NULLed out master_word_ptr if another thread is flushing
+				temppos = mvee_lock_buffer_prev_pos = 0;
+				break;
+			}
+
+			// a weird corner case could happen here.
+			// it is possible that by the time we get here, some other thread has flushed and the master
+			// thread has caught up with us again. In other words,
+			// we MIGHT have missed a complete flush cycle
+			//
+			// => we need to check if our data is still valid!!!
+			if (!mvee_pos_still_valid())
+			{
+				master_word_ptr = 0;
+				break;
+			}
+
+#if defined(MVEE_EXTENDED_QUEUE) && defined(MVEE_CHECK_LOCK_TYPE)
+			unsigned short master_op_type;
+			MVEE_READ_QUEUE_DATA(lock, temppos, sizeof(long) + sizeof(short), master_op_type);
+
+			if (master_op_type != op_type && mvee_pos_still_valid())
+			{
+#  ifdef MVEE_LOG_EIPS
+				MVEE_LOG_STACK(lock, 1, &temppos);
+#  endif
+
+				syscall(__NR_gettid, 1337, 10000001, 60, mvee_lock_buffer_slot_size, temppos);
+				syscall(__NR_gettid, 1337, 10000001, 59, master_op_type, op_type);
+				return;
+			}
+/*
+			if ((master_word_ptr & 0xfff) != (((unsigned long)word_ptr & ~(sizeof(long) - 1) & 0xfff)) && mvee_pos_still_valid())
+			{
+#  ifdef MVEE_LOG_EIPS
+				MVEE_LOG_STACK(lock, 1, &temppos);
+#  endif
+
+				syscall(__NR_gettid, 1337, 10000001, 61, word_ptr, temppos);
+			}
+*/
+#endif
+
+			break;
+		}
+
+		if (master_word_ptr)
+			break;
+
+		// if we get to this point, it means that the slave 
+		// has caught up with the master
+		// => we restart the iteration but this time
+		// we start at the position we were at
+		nextpos = temppos;
+		syscall(__NR_sched_yield);
+	}
+
+	// STEP 2: FIND PRECEDING OPERATIONS ON THIS LOCATION
+	// 
+	// Rules:
+	// * Stores can continue if all preceding loads and stores
+	// have been replicated
+	// * Loads can continue if all preceding stores 
+	// have been replicated
+	//
+	unsigned char seen_preceding_op = 1;
+	while (seen_preceding_op)
+    {
+		seen_preceding_op = 0;
+		for (i = orig_pos; i < temppos; ++i)
+		{
+			unsigned long temp_word_ptr;
+			MVEE_READ_QUEUE_DATA(lock, i, 0, temp_word_ptr);
+
+			// check if the results are being logged out of order
+			if (!temp_word_ptr)
+			{
+				seen_preceding_op = 1;
+				orig_pos = i;
+				break;
+			}
+
+			if ((temp_word_ptr & ~(sizeof(long) - 1)) == master_word_ptr)
+			{
+				if (is_store || (temp_word_ptr & (sizeof(long) - 1)))
+				{
+					if (!mvee_op_is_tagged(i))
+					{
+						seen_preceding_op = 1;
+						orig_pos = i;
+						break;
+					}
+				}
+			}
+		}
+
+		// We haven't seen a preceding operation that must be completed
+		// before this one
+		if (!seen_preceding_op)
+		{
+			mvee_lock_buffer_prev_pos = temppos;
+			break;
+		}
+
+		syscall(__NR_sched_yield);
+    } 
+ 
+#ifdef MVEE_LOG_EIPS
+	MVEE_LOG_STACK(lock, 1, &temppos);
+#endif
+
+#else
+
+	while (true)
+    {
+		int temppos = *(volatile int*)mvee_lock_buffer_pos;
+      
+		if (temppos < mvee_lock_buffer_slots)
+		{
+			unsigned short tid, type;
+
+			MVEE_READ_QUEUE_DATA(lock, temppos, 0, tid);
+			if (tid == (unsigned short)mvee_master_thread_id)
+			{
+# if defined(MVEE_EXTENDED_QUEUE) && defined(MVEE_CHECK_LOCK_TYPE)
+	      
+				MVEE_READ_QUEUE_DATA(lock, temppos, sizeof(short), type);
+
+				if (type != op_type)
+				{
+#  ifdef MVEE_LOG_EIPS
+					MVEE_LOG_STACK(lock, 1, &temppos);
+#  endif
+					syscall(__NR_gettid, 1337, 10000001, 60, mvee_lock_buffer_slot_size, temppos);
+					syscall(__NR_gettid, 1337, 10000001, 59, type, op_type);
+				}
+# endif
+				break;
+			}
+
+			syscall(__NR_sched_yield);
+		}
+		else
+		{
+			// we have to flush... figure out which thread does the flush
+			unsigned short tid;
+			MVEE_READ_QUEUE_DATA(lock, temppos, 0, tid);
+
+			if (tid == (unsigned short)mvee_master_thread_id)
+			{
+				// we should do it...
+				syscall(MVEE_FLUSH_SHARED_BUFFER, MVEE_LIBC_LOCK_BUFFER);
+				*mvee_lock_buffer_pos = 0;
+			}
+			else if (!tid)
+			{
+				// we don't know who should do it yet. wait for more info
+				while (temppos >= mvee_lock_buffer_slots && 
+					   !tid)
+				{
+					cpu_relax();
+					temppos = *(volatile int*)mvee_lock_buffer_pos;
+					MVEE_READ_QUEUE_DATA(lock, temppos, 0, tid);					
+				}
+			}
+			else
+			{
+				// some other thread should do it. wait for the flush
+				while (*mvee_lock_buffer_pos >= mvee_lock_buffer_slots)
+					syscall(__NR_sched_yield);
+			}
+		}
+    }
+#endif
+}
+
+static inline void mvee_read_lock_result_wake(void)
+{
+#ifdef MVEE_EXTENDED_QUEUE
+	int pos;
+
+# ifdef MVEE_PARTIAL_ORDER_REPLICATION
+	pos = mvee_lock_buffer_prev_pos;
+# else
+	pos = *mvee_lock_buffer_pos;
+# endif
+
+# ifdef MVEE_LOG_EIPS
+//	MVEE_LOG_STACK(lock, 1, &pos);
+# endif
+
+	// check call site, should be ASLR proof
+# ifdef MVEE_CHECK_LOCK_TYPE
+#  ifdef MVEE_LOG_EIPS
+	unsigned long parent_eip = *(unsigned long*)((unsigned long)mvee_lock_eip_buffer + sizeof(unsigned long)*MVEE_STACK_DEPTH*mvee_num_childs * pos);
+	unsigned long our_eip = (unsigned long)mvee_original_call_site;
+
+	//  if (parent_eip != our_eip)
+	if ((parent_eip & 0xfff) != (our_eip & 0xfff))
+#  endif
+    {
+		unsigned short lock_type;
+#  ifdef MVEE_PARTIAL_ORDER_REPLICATION
+		MVEE_READ_QUEUE_DATA(lock, pos, sizeof(long) + sizeof(short), lock_type);
+#  else
+		MVEE_READ_QUEUE_DATA(lock, pos, sizeof(short), lock_type);
+#  endif
+#  ifdef MVEE_LOG_EIPS
+		syscall(__NR_gettid, 1337, 10000001, 90, lock_type, parent_eip, our_eip);
+#  endif
+    }
+# endif // !MVEE_CHECK_LOCK_TYPE
+
+#endif // !MVEE_EXTENDED_QUEUE
+
+#ifdef MVEE_PARTIAL_ORDER_REPLICATION
+	// see if we can move the position pointer
+	unsigned int orig_pos = *mvee_lock_buffer_pos;
+	if (!mvee_pos_still_valid())
+		return;
+
+	for (unsigned int i = orig_pos; i < mvee_lock_buffer_slots; ++i)
+    {
+		if (!mvee_op_is_tagged(i) && i != mvee_lock_buffer_prev_pos)
+		{
+			// if it looks like we're able to move the position pointer, 
+			// attempt to do so with a CAS. Again, keep in mind that this
+			// operation might happen at the same time as a flush from another thread
+			// => check if the original pos pointer is still in place
+			// If not, another thread has either increased the pos or has flushed => bail out
+			// If yes, the flushing thread will just need an extra iteration to flush
+			//
+			// we cannot use a regular store here since that might cause us to not flush!
+			// i.e. some thread might move us to the last position, which will cause the next atomic op to flush first
+			// but some other thread might move use back!
+			if (i > orig_pos)
+				__sync_bool_compare_and_swap(mvee_lock_buffer_pos, orig_pos, i);
+			break;
+		}
+    }
+
+	// now tag the slot
+  
+# ifdef MVEE_EXTENDED_QUEUE
+	//  unsigned short tag = __sync_fetch_and_add(&mvee_op_number, 1);
+	unsigned short tag = 1;
+	MVEE_LOG_QUEUE_DATA(lock, mvee_lock_buffer_prev_pos, sizeof(long) + sizeof(short) * (mvee_child_num + 1), tag);
+# else  
+	unsigned char tag = 1;
+	MVEE_LOG_QUEUE_DATA(lock, mvee_lock_buffer_prev_pos, sizeof(long) + sizeof(short) + (mvee_child_num - 1), tag);
+# endif
+
+#else // MVEE_TOTAL_ORDER_REPLICATION
+    (*mvee_lock_buffer_pos)++;
+#endif // !MVEE_PARTIAL_ORDER_REPLICATION
+}
+
+#ifdef MVEE_EXTENDED_QUEUE
+unsigned char mvee_atomic_preop_internal(unsigned char is_store, void* word_ptr, unsigned short op_type)
+#else
+	unsigned char mvee_atomic_preop_internal(unsigned char is_store, void* word_ptr)
+#endif
+{
+	if (unlikely(!mvee_should_sync()))
+		return 0;
+	mvee_check_buffer();
+#if defined(MVEE_EXTENDED_QUEUE) && defined(MVEE_CHECK_LOCK_TYPE)
+	if (!mvee_original_call_site)
+		mvee_original_call_site = (unsigned long)__builtin_return_address(0);
+#endif
+	if (likely(mvee_master_variant))
+    {
+		unsigned int pos = mvee_write_lock_result_prepare();
+#ifdef MVEE_EXTENDED_QUEUE
+		mvee_write_lock_result_write(pos, op_type, word_ptr, is_store);
+#else
+		mvee_write_lock_result_write(pos, word_ptr, is_store);
+#endif
+		return 1;
+    }
+	else
+    {
+#ifdef MVEE_EXTENDED_QUEUE
+		mvee_read_lock_result_wait(op_type, word_ptr);
+#else
+		mvee_read_lock_result_wait();
+#endif
+		return 2;
+    }
+}
+
+void mvee_atomic_postop_internal(unsigned char preop_result)
+{
+	if(likely(preop_result) == 1)
+		mvee_write_lock_result_finish();
+	else if (likely(preop_result) == 2)
+		mvee_read_lock_result_wake();
+#if defined(MVEE_EXTENDED_QUEUE) && defined(MVEE_CHECK_LOCK_TYPE)		
+	mvee_original_call_site = 0;
+#endif   
+}
+
+unsigned char mvee_atomic_preop(unsigned short op_type, void* word_ptr)
+{
+#ifdef MVEE_EXTENDED_QUEUE
+# ifdef MVEE_CHECK_LOCK_TYPE
+	mvee_original_call_site = (unsigned long)__builtin_return_address(0);
+# endif
+	return mvee_atomic_preop_internal(op_type > mvee_atomic_load ? 1 : 0, word_ptr, op_type + __MVEE_BASE_ATOMICS_MAX__);
+#else
+	return mvee_atomic_preop_internal(op_type > mvee_atomic_load ? 1 : 0, word_ptr);
+#endif
+}
+
+void mvee_atomic_postop(unsigned char preop_result)
+{
+	mvee_atomic_postop_internal(preop_result);
+}
+
+
+#ifdef MVEE_DEBUG_MALLOC
+void mvee_malloc_hook(int alloc_type, int msg, long chunksize, void* ar_ptr, void* chunk_ptr)
+{
+	if (mvee_num_childs)
+    {
+		mvee_check_buffer();
+		if (mvee_master_variant)
+			mvee_write_malloc_info(alloc_type, msg, chunksize, ar_ptr, chunk_ptr);
+		else
+			mvee_verify_malloc_info(alloc_type, msg, chunksize, ar_ptr, chunk_ptr);
+    }
+}
+
+void mvee_write_malloc_info(int alloc_type, int msg, long chunksize, void* ar_ptr, void* chunk_ptr)
+{
+	while (1)
+    {
+		if (orig_atomic_decrement_and_test(mvee_malloc_buffer_lock))
+			break;
+      
+		while (*mvee_lock_buffer_lock <= 0)
+			cpu_relax();
+    }
+
+	unsigned int pos = *mvee_malloc_buffer_pos;
+
+	MVEE_LOG_QUEUE_DATA(malloc, pos, 4*sizeof(int) + 2 * sizeof(long), (unsigned long)chunk_ptr);
+	MVEE_LOG_QUEUE_DATA(malloc, pos, 4*sizeof(int) +     sizeof(long), (unsigned long)ar_ptr);
+	MVEE_LOG_QUEUE_DATA(malloc, pos, 4*sizeof(int)                   , chunksize);
+	MVEE_LOG_QUEUE_DATA(malloc, pos, 3*sizeof(int)                   , msg);
+	MVEE_LOG_QUEUE_DATA(malloc, pos, 2*sizeof(int)                   , alloc_type);
+
+	MVEE_LOG_STACK(malloc, 1, mvee_malloc_buffer_pos);
+
+	MVEE_LOG_QUEUE_DATA(malloc, pos, 0, mvee_master_thread_id);
+
+	gcc_barrier();
+	if (++(*mvee_malloc_buffer_pos) >= mvee_malloc_buffer_slots)
+    {
+		syscall(MVEE_FLUSH_SHARED_BUFFER, MVEE_LIBC_MALLOC_DEBUG_BUFFER);
+		*mvee_malloc_buffer_pos = 0;
+    }
+
+	*mvee_malloc_buffer_lock = 1;
+}
+
+struct mvee_malloc_error
+{
+  int alloc_type, msg;
+  long chunksize;
+  void* ar_ptr, * chunk_ptr;
+};
+
+void mvee_verify_malloc_info(int alloc_type, int msg, long chunksize, void* ar_ptr, void* chunk_ptr)
+{
+	unsigned int prevpos = 0;
+	int master_alloc_type, master_msg;
+	long master_chunksize;
+	void* master_ar_ptr, * master_chunk_ptr;
+
+	while (true)
+    {
+		volatile unsigned int temppos = *mvee_malloc_buffer_pos;
+
+		if (temppos < mvee_malloc_buffer_slots)
+		{
+			int tid;
+			MVEE_READ_QUEUE_DATA(malloc, temppos, 0, tid);
+			if (tid == mvee_master_thread_id)
+			{
+				MVEE_READ_QUEUE_DATA(malloc, temppos, 2*sizeof(int), master_alloc_type);
+				MVEE_READ_QUEUE_DATA(malloc, temppos, 3*sizeof(int), master_msg);
+				MVEE_READ_QUEUE_DATA(malloc, temppos, 4*sizeof(int), master_chunksize);
+				MVEE_READ_QUEUE_DATA(malloc, temppos, 4*sizeof(int)+sizeof(long), master_ar_ptr);
+				MVEE_READ_QUEUE_DATA(malloc, temppos, 4*sizeof(int)+2*sizeof(long), master_chunk_ptr);
+				MVEE_LOG_STACK(malloc, 1, mvee_malloc_buffer_pos);
+	      
+				if (alloc_type != master_alloc_type
+					|| msg != master_msg
+					|| chunksize != master_chunksize
+#ifndef MVEE_MALLOC_IGNORE_ASLR
+					|| ar_ptr != master_ar_ptr
+					|| chunk_ptr != master_chunk_ptr
+#endif
+					)
+				{
+					struct mvee_malloc_error err;
+					err.alloc_type = master_alloc_type;
+					err.msg = master_msg;
+					err.chunksize = master_chunksize;
+					err.ar_ptr = master_ar_ptr;
+					err.chunk_ptr = master_chunk_ptr;
+					syscall(__NR_gettid, 1337, 10000001, 74, &err);
+		  
+					err.alloc_type = alloc_type;
+					err.msg = msg;
+					err.chunksize = chunksize;
+					err.ar_ptr = ar_ptr;
+					err.chunk_ptr = chunk_ptr;
+					syscall(__NR_gettid, 1337, 10000001, 75, &err);
+				}
+				break;
+			}
+		}
+    }
+
+	if (*mvee_malloc_buffer_pos +1 >= mvee_malloc_buffer_slots)
+    {
+		syscall(MVEE_FLUSH_SHARED_BUFFER, MVEE_LIBC_MALLOC_DEBUG_BUFFER);
+		*mvee_malloc_buffer_pos = 0;
+    }
+	else
+		(*mvee_malloc_buffer_pos)++;
+}
+
+#endif // !MVEE_DEBUG_MALLOC
diff -N -r --unified glibc-2.25.orig/csu/mvee-woc-agent.c glibc-2.25/csu/mvee-woc-agent.c
--- glibc-2.25.orig/csu/mvee-woc-agent.c	1970-01-01 00:00:00.000000000 +0000
+++ glibc-2.25/csu/mvee-woc-agent.c	2017-09-13 19:55:55.843733418 +0000
@@ -0,0 +1,223 @@
+#define MVEE_SLAVE_YIELD
+#define MVEE_TOTAL_CLOCK_COUNT   2048
+#define MVEE_CLOCK_GROUP_SIZE    64
+#define MVEE_TOTAL_CLOCK_GROUPS  (MVEE_TOTAL_CLOCK_COUNT / MVEE_CLOCK_GROUP_SIZE)
+
+struct mvee_counter
+{
+  volatile unsigned long lock;
+  volatile unsigned long counter;
+  unsigned char padding[64 - 2 * sizeof(unsigned long)]; // prevents false sharing
+};
+
+struct mvee_op_entry
+{
+  volatile unsigned long  counter_and_idx; // the value we must see in mvee_counters[idx] before we can replay the operation
+};
+
+static unsigned char                  mvee_sync_enabled             = 0;
+static unsigned char                  mvee_libc_initialized         = 0;
+static unsigned char                  mvee_master_variant           = 0;
+static __thread unsigned long         mvee_thread_local_pos         = 0; // our position in the thread local queue
+static __thread  
+    struct mvee_op_entry*             mvee_thread_local_queue       = NULL;
+static __thread unsigned long         mvee_thread_local_queue_size  = 0; // nr of slots in the thread local queue
+static __thread unsigned short        mvee_prev_idx                 = 0;
+
+__attribute__((aligned (64)))
+static struct mvee_counter            mvee_counters[MVEE_TOTAL_CLOCK_COUNT + 1];
+
+#define likely(x)       __builtin_expect((x),1)
+#define unlikely(x)     __builtin_expect((x),0)
+#define gcc_barrier()  asm volatile("" ::: "memory")
+#ifndef arch_cpu_relax
+#define arch_cpu_relax()
+#endif
+
+void mvee_invalidate_buffer(void)
+{
+	mvee_thread_local_queue = NULL;
+}
+
+unsigned char mvee_atomic_preop_internal(volatile void* word_ptr)
+{
+	if (unlikely(!mvee_sync_enabled))
+		return 0;
+
+	if (unlikely(!mvee_thread_local_queue))
+    {
+		long mvee_thread_local_queue_id = syscall(MVEE_GET_SHARED_BUFFER, &mvee_counters, MVEE_LIBC_ATOMIC_BUFFER, &mvee_thread_local_queue_size, &mvee_thread_local_pos, NULL);
+		mvee_thread_local_queue_size   /= sizeof(struct mvee_op_entry);
+		mvee_thread_local_queue         = (void*)syscall(__NR_shmat, mvee_thread_local_queue_id, NULL, 0);     
+    }
+
+	if (unlikely(mvee_thread_local_pos >= mvee_thread_local_queue_size))
+    {
+		syscall(MVEE_FLUSH_SHARED_BUFFER, MVEE_LIBC_ATOMIC_BUFFER);
+		mvee_thread_local_pos = 0;
+    }
+
+	if (likely(mvee_master_variant))
+    {
+		// page number defines the clock group
+		// offset within page defines the clock within that group
+		mvee_prev_idx = (((((unsigned long)word_ptr >> 24) % MVEE_TOTAL_CLOCK_GROUPS) * (MVEE_CLOCK_GROUP_SIZE) 
+						  + ((((unsigned long)word_ptr & 4095) >> 6) % MVEE_CLOCK_GROUP_SIZE))
+						 & 0xFFF) + 1;
+
+		while (!__sync_bool_compare_and_swap(&mvee_counters[mvee_prev_idx].lock, 0, 1))
+			arch_cpu_relax();
+		
+		unsigned long pos = mvee_counters[mvee_prev_idx].counter;    
+
+		mvee_thread_local_queue[mvee_thread_local_pos++].counter_and_idx 
+			= (pos << 12) | mvee_prev_idx;
+
+		atomic_full_barrier();
+
+		return 1;
+    }
+	else
+    {
+		unsigned long counter_and_idx = 0;
+
+		while (unlikely(1))
+		{
+			counter_and_idx = mvee_thread_local_queue[mvee_thread_local_pos].counter_and_idx;
+
+			if (likely(counter_and_idx))
+				break;
+
+#ifdef MVEE_SLAVE_YIELD
+			syscall(__NR_sched_yield);
+#else
+			arch_cpu_relax();
+#endif
+		}
+
+		mvee_prev_idx = counter_and_idx & 0xFFF;
+		counter_and_idx &= ~0xFFF;
+
+		atomic_full_barrier();
+
+		while ((mvee_counters[mvee_prev_idx].counter << 12) != counter_and_idx)
+#ifdef MVEE_SLAVE_YIELD
+			syscall(__NR_sched_yield);
+#else
+		arch_cpu_relax();
+#endif
+
+		atomic_full_barrier();
+		
+		return 2;
+    }
+}
+
+void mvee_atomic_postop_internal(unsigned char preop_result)
+{
+	atomic_full_barrier();
+	
+	if(likely(preop_result) == 1)
+	{
+		gcc_barrier();
+		orig_atomic_increment(&mvee_counters[mvee_prev_idx].counter);
+		atomic_full_barrier();
+		mvee_counters[mvee_prev_idx].lock = 0;
+	}
+	else if (likely(preop_result) == 2)
+	{
+		gcc_barrier();
+		mvee_counters[mvee_prev_idx].counter++;
+		mvee_thread_local_pos++;
+	}
+}
+
+/* Checks if all variants got ALIGNMENT aligned heaps from
+   the previous mmap request. If some of them have not, ALL variants
+   have to bail out and fall back to another heap allocation method.
+   This ensures that the variants stay in sync with respect to future mm
+   requests.
+*/
+#define ALIGNMENT 0x4000000
+
+int
+mvee_all_heaps_aligned(char* heap, unsigned long alloc_size)
+{
+	// if we're not running under MVEE control,
+	// just check the alignment of the current heap
+	if (!mvee_thread_local_queue)
+	{
+		if ((unsigned long)heap & (ALIGNMENT-1))
+			return 0;
+		return 1;
+	}
+
+	// We ARE running under MVEE control
+	// => ask the MVEE to check the alignments
+	// of ALL heaps
+	return syscall(MVEE_ALL_HEAPS_ALIGNED, heap, ALIGNMENT, alloc_size);
+}
+
+/* 
+ * mvee_infinite_loop:
+ * this function is used for both thread transfering and signal delivery 
+ * 
+ * 1) to transfer threads to a new monitor, the original monitor (i.e. the 
+ * monitor that monitors the childs that instigated the fork event) needs to
+ * detach from the threads first. While the threads are detached, they can
+ * run freely, without the intervention of a debugger.
+ * As such, we have to move the program counter to an infinite loop while
+ * the threads are detached. This way, the threads will all be in an equivalent
+ * state when the new monitor attaches to them.
+ * Because we're going to replace the registers by their original contents
+ * when the new monitor attaches, we can use sys_pause calls in the infinite
+ * loop.
+ * 
+ * 2) delivering signals through the ptrace API happens asynchronously 
+ * (I found out the hard way). As such, we should wait for the threads to be
+ * in equivalent states (e.g. stopped on the same syscall). Then the registers
+ * should be backed up and the syscall nr should be replaced by a harmless
+ * syscall that doesn't modify the program state. We use sys_getpid for this
+ * purpose. When that replaced syscall returns, we change the pc to this
+ * infinite loop while we wait for async signal delivery.
+ * We probably cannot use syscalls while waiting for signal delivery. 
+ * One possible exception is sys_sched_yield. Our modified MVEE kernel does
+ * not report this syscall to the ptracer
+ * 
+ * the with_syscalls parameter is passed through the ecx register!
+ */
+void mvee_infinite_loop(int with_syscalls, int dummy)
+{
+	if (with_syscalls)
+    {
+		for (;;)
+			syscall(__NR_pause);
+    }
+	else
+    {
+		for (;;)
+		{
+			dummy = dummy << 2;
+		}
+    }
+}
+
+int mvee_should_sync_tid(void)
+{
+	return mvee_sync_enabled ? 1 : 0;
+}
+
+unsigned char mvee_atomic_preop(unsigned short op_type, void* word_ptr)
+{
+	return mvee_atomic_preop_internal(word_ptr);
+}
+
+void mvee_atomic_postop(unsigned char preop_result)
+{
+	mvee_atomic_postop_internal(preop_result);
+}
+
+unsigned char mvee_should_futex_unlock(void)
+{
+	return (!mvee_master_variant && mvee_sync_enabled) ? 1 : 0;
+}
diff -N -r --unified glibc-2.25.orig/csu/Versions glibc-2.25/csu/Versions
--- glibc-2.25.orig/csu/Versions	2017-09-11 17:14:55.239208594 +0000
+++ glibc-2.25/csu/Versions	2017-09-13 19:55:55.845733686 +0000
@@ -2,6 +2,14 @@
   GLIBC_2.0 {
     # helper functions
     __libc_init_first; __libc_start_main;
+    mvee_atomic_preop;
+    mvee_atomic_postop;
+    mvee_atomic_preop_internal;
+    mvee_atomic_postop_internal;
+    mvee_invalidate_buffer;
+    mvee_all_heaps_aligned;
+    mvee_should_sync_tid;
+    mvee_should_futex_unlock;
   }
   GLIBC_2.1 {
     # New special glibc functions.
diff -N -r --unified glibc-2.25.orig/elf/rtld.c glibc-2.25/elf/rtld.c
--- glibc-2.25.orig/elf/rtld.c	2017-09-11 17:14:55.334215390 +0000
+++ glibc-2.25/elf/rtld.c	2017-09-13 19:55:55.853734683 +0000
@@ -932,6 +932,18 @@
       main_map->l_name = (char *) "";
       *user_entry = main_map->l_entry;
 
+     /* GHUMVEE patch: ask the MVEE for the "virtualized" argv[0].  This can be
+     necessary if we run compile-time diversified variants that print out their
+     argv[0] value. */                                                                                                                                                                                             
+	 char virtualized_argv0[4096];
+	 if (syscall(MVEE_GET_VIRTUALIZED_ARGV0, _dl_argv[0], virtualized_argv0, 4096) == 0)
+	 {
+		 // should we do this?
+		 if (_dl_argv[0])
+			 free(_dl_argv[0]);
+		 _dl_argv[0] = strdup(virtualized_argv0);
+	 }
+
 #ifdef HAVE_AUX_VECTOR
       /* Adjust the on-stack auxiliary vector so that it looks like the
 	 binary was executed directly.  */
diff -N -r --unified glibc-2.25.orig/gmon/bb_init_func.c glibc-2.25/gmon/bb_init_func.c
--- glibc-2.25.orig/gmon/bb_init_func.c	2017-09-11 17:14:55.383218895 +0000
+++ glibc-2.25/gmon/bb_init_func.c	2017-09-13 19:55:55.856735030 +0000
@@ -26,6 +26,7 @@
 #include <sys/gmon.h>
 
 #include <stdlib.h>
+#include <atomic.h>
 
 void
 __bb_init_func (struct __bb *bb)
@@ -42,7 +43,7 @@
   bb->next = __bb_head;
   __bb_head = bb;
 
-  if (bb->next == 0 && p->state != GMON_PROF_ON)
+  if (bb->next == 0 && atomic_load(p->state) != GMON_PROF_ON)
     {
       /* we didn't register _mcleanup yet and pc profiling doesn't seem
 	 to be active, so let's register it now: */
diff -N -r --unified glibc-2.25.orig/gmon/gmon.c glibc-2.25/gmon/gmon.c
--- glibc-2.25.orig/gmon/gmon.c	2017-09-11 17:14:55.383218895 +0000
+++ glibc-2.25/gmon/gmon.c	2017-09-13 19:55:55.860735473 +0000
@@ -77,20 +77,20 @@
   struct gmonparam *p = &_gmonparam;
 
   /* Don't change the state if we ran into an error.  */
-  if (p->state == GMON_PROF_ERROR)
+  if (atomic_load(p->state) == GMON_PROF_ERROR)
     return;
 
   if (mode)
     {
       /* start */
       __profil((void *) p->kcount, p->kcountsize, p->lowpc, s_scale);
-      p->state = GMON_PROF_ON;
+      atomic_store(p->state, GMON_PROF_ON);
     }
   else
     {
       /* stop */
       __profil(NULL, 0, 0, 0);
-      p->state = GMON_PROF_OFF;
+      atomic_store(p->state, GMON_PROF_OFF);
     }
 }
 weak_alias (__moncontrol, moncontrol)
@@ -133,7 +133,7 @@
     {
       ERR("monstartup: out of memory\n");
       p->tos = NULL;
-      p->state = GMON_PROF_ERROR;
+      atomic_store(p->state, GMON_PROF_ERROR);
       return;
     }
   p->tos = (struct tostruct *)cp;
@@ -397,11 +397,11 @@
 void
 __write_profiling (void)
 {
-  int save = _gmonparam.state;
-  _gmonparam.state = GMON_PROF_OFF;
+  int save = atomic_load(_gmonparam.state);
+  atomic_store(_gmonparam.state, GMON_PROF_OFF);
   if (save == GMON_PROF_ON)
     write_gmon ();
-  _gmonparam.state = save;
+  atomic_store(_gmonparam.state, save);
 }
 #ifndef SHARED
 /* This symbol isn't used anywhere in the DSO and it is not exported.
@@ -418,7 +418,7 @@
 {
   __moncontrol (0);
 
-  if (_gmonparam.state != GMON_PROF_ERROR)
+  if (atomic_load(_gmonparam.state) != GMON_PROF_ERROR)
     write_gmon ();
 
   /* free the memory. */
diff -N -r --unified glibc-2.25.orig/gmon/mcount.c glibc-2.25/gmon/mcount.c
--- glibc-2.25.orig/gmon/mcount.c	2017-09-11 17:14:55.384218967 +0000
+++ glibc-2.25/gmon/mcount.c	2017-09-13 19:55:55.862735688 +0000
@@ -166,10 +166,10 @@
 
 	}
 done:
-	p->state = GMON_PROF_ON;
+	atomic_store(p->state, GMON_PROF_ON);
 	return;
 overflow:
-	p->state = GMON_PROF_ERROR;
+	atomic_store(p->state, GMON_PROF_ERROR);
 	return;
 }
 
diff -N -r --unified glibc-2.25.orig/include/atomic.h glibc-2.25/include/atomic.h
--- glibc-2.25.orig/include/atomic.h	2017-09-11 17:14:56.867325058 +0000
+++ glibc-2.25/include/atomic.h	2017-09-13 19:55:55.881737800 +0000
@@ -86,73 +86,73 @@
 
 /* Atomically store NEWVAL in *MEM if *MEM is equal to OLDVAL.
    Return the old *MEM value.  */
-#if !defined atomic_compare_and_exchange_val_acq \
-    && defined __arch_compare_and_exchange_val_32_acq
-# define atomic_compare_and_exchange_val_acq(mem, newval, oldval) \
-  __atomic_val_bysize (__arch_compare_and_exchange_val,acq,		      \
+#if !defined orig_atomic_compare_and_exchange_val_acq \
+    && defined orig___arch_compare_and_exchange_val_32_acq
+# define orig_atomic_compare_and_exchange_val_acq(mem, newval, oldval) \
+  __atomic_val_bysize (orig___arch_compare_and_exchange_val,acq,		      \
 		       mem, newval, oldval)
 #endif
 
 
-#ifndef catomic_compare_and_exchange_val_acq
-# ifdef __arch_c_compare_and_exchange_val_32_acq
-#  define catomic_compare_and_exchange_val_acq(mem, newval, oldval) \
-  __atomic_val_bysize (__arch_c_compare_and_exchange_val,acq,		      \
+#ifndef orig_catomic_compare_and_exchange_val_acq
+# ifdef orig___arch_c_compare_and_exchange_val_32_acq
+#  define orig_catomic_compare_and_exchange_val_acq(mem, newval, oldval) \
+  __atomic_val_bysize (orig___arch_c_compare_and_exchange_val,acq,		      \
 		       mem, newval, oldval)
 # else
-#  define catomic_compare_and_exchange_val_acq(mem, newval, oldval) \
-  atomic_compare_and_exchange_val_acq (mem, newval, oldval)
+#  define orig_catomic_compare_and_exchange_val_acq(mem, newval, oldval) \
+  orig_atomic_compare_and_exchange_val_acq (mem, newval, oldval)
 # endif
 #endif
 
 
-#ifndef catomic_compare_and_exchange_val_rel
-# ifndef atomic_compare_and_exchange_val_rel
-#  define catomic_compare_and_exchange_val_rel(mem, newval, oldval)	      \
-  catomic_compare_and_exchange_val_acq (mem, newval, oldval)
+#ifndef orig_catomic_compare_and_exchange_val_rel
+# ifndef orig_atomic_compare_and_exchange_val_rel
+#  define orig_catomic_compare_and_exchange_val_rel(mem, newval, oldval)	      \
+  orig_catomic_compare_and_exchange_val_acq (mem, newval, oldval)
 # else
-#  define catomic_compare_and_exchange_val_rel(mem, newval, oldval)	      \
-  atomic_compare_and_exchange_val_rel (mem, newval, oldval)
+#  define orig_catomic_compare_and_exchange_val_rel(mem, newval, oldval)	      \
+  orig_atomic_compare_and_exchange_val_rel (mem, newval, oldval)
 # endif
 #endif
 
 
-#ifndef atomic_compare_and_exchange_val_rel
-# define atomic_compare_and_exchange_val_rel(mem, newval, oldval)	      \
-  atomic_compare_and_exchange_val_acq (mem, newval, oldval)
+#ifndef orig_atomic_compare_and_exchange_val_rel
+# define orig_atomic_compare_and_exchange_val_rel(mem, newval, oldval)	      \
+  orig_atomic_compare_and_exchange_val_acq (mem, newval, oldval)
 #endif
 
 
 /* Atomically store NEWVAL in *MEM if *MEM is equal to OLDVAL.
    Return zero if *MEM was changed or non-zero if no exchange happened.  */
-#ifndef atomic_compare_and_exchange_bool_acq
-# ifdef __arch_compare_and_exchange_bool_32_acq
-#  define atomic_compare_and_exchange_bool_acq(mem, newval, oldval) \
-  __atomic_bool_bysize (__arch_compare_and_exchange_bool,acq,		      \
+#ifndef orig_atomic_compare_and_exchange_bool_acq
+# ifdef orig___arch_compare_and_exchange_bool_32_acq
+#  define orig_atomic_compare_and_exchange_bool_acq(mem, newval, oldval) \
+  __atomic_bool_bysize (orig___arch_compare_and_exchange_bool,acq,		      \
 		        mem, newval, oldval)
 # else
-#  define atomic_compare_and_exchange_bool_acq(mem, newval, oldval) \
+#  define orig_atomic_compare_and_exchange_bool_acq(mem, newval, oldval) \
   ({ /* Cannot use __oldval here, because macros later in this file might     \
 	call this macro with __oldval argument.	 */			      \
      __typeof (oldval) __atg3_old = (oldval);				      \
-     atomic_compare_and_exchange_val_acq (mem, newval, __atg3_old)	      \
+     orig_atomic_compare_and_exchange_val_acq (mem, newval, __atg3_old)	      \
        != __atg3_old;							      \
   })
 # endif
 #endif
 
 
-#ifndef catomic_compare_and_exchange_bool_acq
-# ifdef __arch_c_compare_and_exchange_bool_32_acq
-#  define catomic_compare_and_exchange_bool_acq(mem, newval, oldval) \
-  __atomic_bool_bysize (__arch_c_compare_and_exchange_bool,acq,		      \
+#ifndef orig_catomic_compare_and_exchange_bool_acq
+# ifdef orig___arch_c_compare_and_exchange_bool_32_acq
+#  define orig_catomic_compare_and_exchange_bool_acq(mem, newval, oldval) \
+  __atomic_bool_bysize (orig___arch_c_compare_and_exchange_bool,acq,		      \
 		        mem, newval, oldval)
 # else
-#  define catomic_compare_and_exchange_bool_acq(mem, newval, oldval) \
+#  define orig_catomic_compare_and_exchange_bool_acq(mem, newval, oldval) \
   ({ /* Cannot use __oldval here, because macros later in this file might     \
 	call this macro with __oldval argument.	 */			      \
      __typeof (oldval) __atg4_old = (oldval);				      \
-     catomic_compare_and_exchange_val_acq (mem, newval, __atg4_old)	      \
+     orig_catomic_compare_and_exchange_val_acq (mem, newval, __atg4_old)	      \
        != __atg4_old;							      \
   })
 # endif
@@ -160,8 +160,8 @@
 
 
 /* Store NEWVALUE in *MEM and return the old value.  */
-#ifndef atomic_exchange_acq
-# define atomic_exchange_acq(mem, newvalue) \
+#ifndef orig_atomic_exchange_acq
+# define orig_atomic_exchange_acq(mem, newvalue) \
   ({ __typeof (*(mem)) __atg5_oldval;					      \
      __typeof (mem) __atg5_memp = (mem);				      \
      __typeof (*(mem)) __atg5_value = (newvalue);			      \
@@ -169,24 +169,24 @@
      do									      \
        __atg5_oldval = *__atg5_memp;					      \
      while (__builtin_expect						      \
-	    (atomic_compare_and_exchange_bool_acq (__atg5_memp, __atg5_value, \
+	    (orig_atomic_compare_and_exchange_bool_acq (__atg5_memp, __atg5_value, \
 						   __atg5_oldval), 0));	      \
 									      \
      __atg5_oldval; })
 #endif
 
-#ifndef atomic_exchange_rel
-# define atomic_exchange_rel(mem, newvalue) atomic_exchange_acq (mem, newvalue)
+#ifndef orig_atomic_exchange_rel
+# define orig_atomic_exchange_rel(mem, newvalue) orig_atomic_exchange_acq (mem, newvalue)
 #endif
 
 
 /* Add VALUE to *MEM and return the old value of *MEM.  */
-#ifndef atomic_exchange_and_add_acq
-# ifdef atomic_exchange_and_add
-#  define atomic_exchange_and_add_acq(mem, value) \
-  atomic_exchange_and_add (mem, value)
+#ifndef orig_atomic_exchange_and_add_acq
+# ifdef orig_atomic_exchange_and_add
+#  define orig_atomic_exchange_and_add_acq(mem, value) \
+  orig_atomic_exchange_and_add (mem, value)
 # else
-#  define atomic_exchange_and_add_acq(mem, value) \
+#  define orig_atomic_exchange_and_add_acq(mem, value) \
   ({ __typeof (*(mem)) __atg6_oldval;					      \
      __typeof (mem) __atg6_memp = (mem);				      \
      __typeof (*(mem)) __atg6_value = (value);				      \
@@ -194,7 +194,7 @@
      do									      \
        __atg6_oldval = *__atg6_memp;					      \
      while (__builtin_expect						      \
-	    (atomic_compare_and_exchange_bool_acq (__atg6_memp,		      \
+	    (orig_atomic_compare_and_exchange_bool_acq (__atg6_memp,		      \
 						   __atg6_oldval	      \
 						   + __atg6_value,	      \
 						   __atg6_oldval), 0));	      \
@@ -203,18 +203,18 @@
 # endif
 #endif
 
-#ifndef atomic_exchange_and_add_rel
-# define atomic_exchange_and_add_rel(mem, value) \
-  atomic_exchange_and_add_acq(mem, value)
+#ifndef orig_atomic_exchange_and_add_rel
+# define orig_atomic_exchange_and_add_rel(mem, value) \
+  orig_atomic_exchange_and_add_acq(mem, value)
 #endif
 
-#ifndef atomic_exchange_and_add
-# define atomic_exchange_and_add(mem, value) \
-  atomic_exchange_and_add_acq(mem, value)
+#ifndef orig_atomic_exchange_and_add
+# define orig_atomic_exchange_and_add(mem, value) \
+  orig_atomic_exchange_and_add_acq(mem, value)
 #endif
 
-#ifndef catomic_exchange_and_add
-# define catomic_exchange_and_add(mem, value) \
+#ifndef orig_catomic_exchange_and_add
+# define orig_catomic_exchange_and_add(mem, value) \
   ({ __typeof (*(mem)) __atg7_oldv;					      \
      __typeof (mem) __atg7_memp = (mem);				      \
      __typeof (*(mem)) __atg7_value = (value);				      \
@@ -222,7 +222,7 @@
      do									      \
        __atg7_oldv = *__atg7_memp;					      \
      while (__builtin_expect						      \
-	    (catomic_compare_and_exchange_bool_acq (__atg7_memp,	      \
+	    (orig_catomic_compare_and_exchange_bool_acq (__atg7_memp,	      \
 						    __atg7_oldv		      \
 						    + __atg7_value,	      \
 						    __atg7_oldv), 0));	      \
@@ -231,8 +231,8 @@
 #endif
 
 
-#ifndef atomic_max
-# define atomic_max(mem, value) \
+#ifndef orig_atomic_max
+# define orig_atomic_max(mem, value) \
   do {									      \
     __typeof (*(mem)) __atg8_oldval;					      \
     __typeof (mem) __atg8_memp = (mem);					      \
@@ -242,14 +242,14 @@
       if (__atg8_oldval >= __atg8_value)				      \
 	break;								      \
     } while (__builtin_expect						      \
-	     (atomic_compare_and_exchange_bool_acq (__atg8_memp, __atg8_value,\
+	     (orig_atomic_compare_and_exchange_bool_acq (__atg8_memp, __atg8_value,\
 						    __atg8_oldval), 0));      \
   } while (0)
 #endif
 
 
-#ifndef catomic_max
-# define catomic_max(mem, value) \
+#ifndef orig_catomic_max
+# define orig_catomic_max(mem, value) \
   do {									      \
     __typeof (*(mem)) __atg9_oldv;					      \
     __typeof (mem) __atg9_memp = (mem);					      \
@@ -259,15 +259,15 @@
       if (__atg9_oldv >= __atg9_value)					      \
 	break;								      \
     } while (__builtin_expect						      \
-	     (catomic_compare_and_exchange_bool_acq (__atg9_memp,	      \
+	     (orig_catomic_compare_and_exchange_bool_acq (__atg9_memp,	      \
 						     __atg9_value,	      \
 						     __atg9_oldv), 0));	      \
   } while (0)
 #endif
 
 
-#ifndef atomic_min
-# define atomic_min(mem, value) \
+#ifndef orig_atomic_min
+# define orig_atomic_min(mem, value) \
   do {									      \
     __typeof (*(mem)) __atg10_oldval;					      \
     __typeof (mem) __atg10_memp = (mem);				      \
@@ -277,81 +277,81 @@
       if (__atg10_oldval <= __atg10_value)				      \
 	break;								      \
     } while (__builtin_expect						      \
-	     (atomic_compare_and_exchange_bool_acq (__atg10_memp,	      \
+	     (orig_atomic_compare_and_exchange_bool_acq (__atg10_memp,	      \
 						    __atg10_value,	      \
 						    __atg10_oldval), 0));     \
   } while (0)
 #endif
 
 
-#ifndef atomic_add
-# define atomic_add(mem, value) (void) atomic_exchange_and_add ((mem), (value))
+#ifndef orig_atomic_add
+# define orig_atomic_add(mem, value) (void) orig_atomic_exchange_and_add ((mem), (value))
 #endif
 
 
-#ifndef catomic_add
-# define catomic_add(mem, value) \
-  (void) catomic_exchange_and_add ((mem), (value))
+#ifndef orig_catomic_add
+# define orig_catomic_add(mem, value) \
+  (void) orig_catomic_exchange_and_add ((mem), (value))
 #endif
 
 
-#ifndef atomic_increment
-# define atomic_increment(mem) atomic_add ((mem), 1)
+#ifndef orig_atomic_increment
+# define orig_atomic_increment(mem) orig_atomic_add ((mem), 1)
 #endif
 
 
-#ifndef catomic_increment
-# define catomic_increment(mem) catomic_add ((mem), 1)
+#ifndef orig_catomic_increment
+# define orig_catomic_increment(mem) orig_catomic_add ((mem), 1)
 #endif
 
 
-#ifndef atomic_increment_val
-# define atomic_increment_val(mem) (atomic_exchange_and_add ((mem), 1) + 1)
+#ifndef orig_atomic_increment_val
+# define orig_atomic_increment_val(mem) (orig_atomic_exchange_and_add ((mem), 1) + 1)
 #endif
 
 
-#ifndef catomic_increment_val
-# define catomic_increment_val(mem) (catomic_exchange_and_add ((mem), 1) + 1)
+#ifndef orig_catomic_increment_val
+# define orig_catomic_increment_val(mem) (orig_catomic_exchange_and_add ((mem), 1) + 1)
 #endif
 
 
 /* Add one to *MEM and return true iff it's now zero.  */
-#ifndef atomic_increment_and_test
-# define atomic_increment_and_test(mem) \
-  (atomic_exchange_and_add ((mem), 1) + 1 == 0)
+#ifndef orig_atomic_increment_and_test
+# define orig_atomic_increment_and_test(mem) \
+  (orig_atomic_exchange_and_add ((mem), 1) + 1 == 0)
 #endif
 
 
-#ifndef atomic_decrement
-# define atomic_decrement(mem) atomic_add ((mem), -1)
+#ifndef orig_atomic_decrement
+# define orig_atomic_decrement(mem) orig_atomic_add ((mem), -1)
 #endif
 
 
-#ifndef catomic_decrement
-# define catomic_decrement(mem) catomic_add ((mem), -1)
+#ifndef orig_catomic_decrement
+# define orig_catomic_decrement(mem) orig_catomic_add ((mem), -1)
 #endif
 
 
-#ifndef atomic_decrement_val
-# define atomic_decrement_val(mem) (atomic_exchange_and_add ((mem), -1) - 1)
+#ifndef orig_atomic_decrement_val
+# define orig_atomic_decrement_val(mem) (orig_atomic_exchange_and_add ((mem), -1) - 1)
 #endif
 
 
-#ifndef catomic_decrement_val
-# define catomic_decrement_val(mem) (catomic_exchange_and_add ((mem), -1) - 1)
+#ifndef orig_catomic_decrement_val
+# define orig_catomic_decrement_val(mem) (orig_catomic_exchange_and_add ((mem), -1) - 1)
 #endif
 
 
 /* Subtract 1 from *MEM and return true iff it's now zero.  */
-#ifndef atomic_decrement_and_test
-# define atomic_decrement_and_test(mem) \
-  (atomic_exchange_and_add ((mem), -1) == 1)
+#ifndef orig_atomic_decrement_and_test
+# define orig_atomic_decrement_and_test(mem) \
+  (orig_atomic_exchange_and_add ((mem), -1) == 1)
 #endif
 
 
 /* Decrement *MEM if it is > 0, and return the old value.  */
-#ifndef atomic_decrement_if_positive
-# define atomic_decrement_if_positive(mem) \
+#ifndef orig_atomic_decrement_if_positive
+# define orig_atomic_decrement_if_positive(mem) \
   ({ __typeof (*(mem)) __atg11_oldval;					      \
      __typeof (mem) __atg11_memp = (mem);				      \
 									      \
@@ -362,35 +362,35 @@
 	   break;							      \
        }								      \
      while (__builtin_expect						      \
-	    (atomic_compare_and_exchange_bool_acq (__atg11_memp,	      \
+	    (orig_atomic_compare_and_exchange_bool_acq (__atg11_memp,	      \
 						   __atg11_oldval - 1,	      \
 						   __atg11_oldval), 0));      \
      __atg11_oldval; })
 #endif
 
 
-#ifndef atomic_add_negative
-# define atomic_add_negative(mem, value)				      \
+#ifndef orig_atomic_add_negative
+# define orig_atomic_add_negative(mem, value)				      \
   ({ __typeof (value) __atg12_value = (value);				      \
-     atomic_exchange_and_add (mem, __atg12_value) < -__atg12_value; })
+     orig_atomic_exchange_and_add (mem, __atg12_value) < -__atg12_value; })
 #endif
 
 
-#ifndef atomic_add_zero
-# define atomic_add_zero(mem, value)					      \
+#ifndef orig_atomic_add_zero
+# define orig_atomic_add_zero(mem, value)					      \
   ({ __typeof (value) __atg13_value = (value);				      \
-     atomic_exchange_and_add (mem, __atg13_value) == -__atg13_value; })
+     orig_atomic_exchange_and_add (mem, __atg13_value) == -__atg13_value; })
 #endif
 
 
-#ifndef atomic_bit_set
-# define atomic_bit_set(mem, bit) \
-  (void) atomic_bit_test_set(mem, bit)
+#ifndef orig_atomic_bit_set
+# define orig_atomic_bit_set(mem, bit) \
+  (void) orig_atomic_bit_test_set(mem, bit)
 #endif
 
 
-#ifndef atomic_bit_test_set
-# define atomic_bit_test_set(mem, bit) \
+#ifndef orig_atomic_bit_test_set
+# define orig_atomic_bit_test_set(mem, bit) \
   ({ __typeof (*(mem)) __atg14_old;					      \
      __typeof (mem) __atg14_memp = (mem);				      \
      __typeof (*(mem)) __atg14_mask = ((__typeof (*(mem))) 1 << (bit));	      \
@@ -398,7 +398,7 @@
      do									      \
        __atg14_old = (*__atg14_memp);					      \
      while (__builtin_expect						      \
-	    (atomic_compare_and_exchange_bool_acq (__atg14_memp,	      \
+	    (orig_atomic_compare_and_exchange_bool_acq (__atg14_memp,	      \
 						   __atg14_old | __atg14_mask,\
 						   __atg14_old), 0));	      \
 									      \
@@ -406,8 +406,8 @@
 #endif
 
 /* Atomically *mem &= mask.  */
-#ifndef atomic_and
-# define atomic_and(mem, mask) \
+#ifndef orig_atomic_and
+# define orig_atomic_and(mem, mask) \
   do {									      \
     __typeof (*(mem)) __atg15_old;					      \
     __typeof (mem) __atg15_memp = (mem);				      \
@@ -416,14 +416,14 @@
     do									      \
       __atg15_old = (*__atg15_memp);					      \
     while (__builtin_expect						      \
-	   (atomic_compare_and_exchange_bool_acq (__atg15_memp,		      \
+	   (orig_atomic_compare_and_exchange_bool_acq (__atg15_memp,		      \
 						  __atg15_old & __atg15_mask, \
 						  __atg15_old), 0));	      \
   } while (0)
 #endif
 
-#ifndef catomic_and
-# define catomic_and(mem, mask) \
+#ifndef orig_catomic_and
+# define orig_catomic_and(mem, mask) \
   do {									      \
     __typeof (*(mem)) __atg20_old;					      \
     __typeof (mem) __atg20_memp = (mem);				      \
@@ -432,15 +432,15 @@
     do									      \
       __atg20_old = (*__atg20_memp);					      \
     while (__builtin_expect						      \
-	   (catomic_compare_and_exchange_bool_acq (__atg20_memp,	      \
+	   (orig_catomic_compare_and_exchange_bool_acq (__atg20_memp,	      \
 						   __atg20_old & __atg20_mask,\
 						   __atg20_old), 0));	      \
   } while (0)
 #endif
 
 /* Atomically *mem &= mask and return the old value of *mem.  */
-#ifndef atomic_and_val
-# define atomic_and_val(mem, mask) \
+#ifndef orig_atomic_and_val
+# define orig_atomic_and_val(mem, mask) \
   ({ __typeof (*(mem)) __atg16_old;					      \
      __typeof (mem) __atg16_memp = (mem);				      \
      __typeof (*(mem)) __atg16_mask = (mask);				      \
@@ -448,7 +448,7 @@
      do									      \
        __atg16_old = (*__atg16_memp);					      \
      while (__builtin_expect						      \
-	    (atomic_compare_and_exchange_bool_acq (__atg16_memp,	      \
+	    (orig_atomic_compare_and_exchange_bool_acq (__atg16_memp,	      \
 						   __atg16_old & __atg16_mask,\
 						   __atg16_old), 0));	      \
 									      \
@@ -456,8 +456,8 @@
 #endif
 
 /* Atomically *mem |= mask and return the old value of *mem.  */
-#ifndef atomic_or
-# define atomic_or(mem, mask) \
+#ifndef orig_atomic_or
+# define orig_atomic_or(mem, mask) \
   do {									      \
     __typeof (*(mem)) __atg17_old;					      \
     __typeof (mem) __atg17_memp = (mem);				      \
@@ -466,14 +466,14 @@
     do									      \
       __atg17_old = (*__atg17_memp);					      \
     while (__builtin_expect						      \
-	   (atomic_compare_and_exchange_bool_acq (__atg17_memp,		      \
+	   (orig_atomic_compare_and_exchange_bool_acq (__atg17_memp,		      \
 						  __atg17_old | __atg17_mask, \
 						  __atg17_old), 0));	      \
   } while (0)
 #endif
 
-#ifndef catomic_or
-# define catomic_or(mem, mask) \
+#ifndef orig_catomic_or
+# define orig_catomic_or(mem, mask) \
   do {									      \
     __typeof (*(mem)) __atg18_old;					      \
     __typeof (mem) __atg18_memp = (mem);				      \
@@ -482,15 +482,15 @@
     do									      \
       __atg18_old = (*__atg18_memp);					      \
     while (__builtin_expect						      \
-	   (catomic_compare_and_exchange_bool_acq (__atg18_memp,	      \
+	   (orig_catomic_compare_and_exchange_bool_acq (__atg18_memp,	      \
 						   __atg18_old | __atg18_mask,\
 						   __atg18_old), 0));	      \
   } while (0)
 #endif
 
 /* Atomically *mem |= mask and return the old value of *mem.  */
-#ifndef atomic_or_val
-# define atomic_or_val(mem, mask) \
+#ifndef orig_atomic_or_val
+# define orig_atomic_or_val(mem, mask) \
   ({ __typeof (*(mem)) __atg19_old;					      \
      __typeof (mem) __atg19_memp = (mem);				      \
      __typeof (*(mem)) __atg19_mask = (mask);				      \
@@ -498,7 +498,7 @@
      do									      \
        __atg19_old = (*__atg19_memp);					      \
      while (__builtin_expect						      \
-	    (atomic_compare_and_exchange_bool_acq (__atg19_memp,	      \
+	    (orig_atomic_compare_and_exchange_bool_acq (__atg19_memp,	      \
 						   __atg19_old | __atg19_mask,\
 						   __atg19_old), 0));	      \
 									      \
@@ -520,8 +520,8 @@
 #endif
 
 
-#ifndef atomic_forced_read
-# define atomic_forced_read(x) \
+#ifndef orig_atomic_forced_read
+# define orig_atomic_forced_read(x) \
   ({ __typeof (x) __x; __asm ("" : "=r" (__x) : "0" (x)); __x; })
 #endif
 
@@ -572,82 +572,82 @@
 # define atomic_thread_fence_seq_cst() \
   __atomic_thread_fence (__ATOMIC_SEQ_CST)
 
-# define atomic_load_relaxed(mem) \
+# define orig_atomic_load_relaxed(mem) \
   ({ __atomic_check_size_ls((mem));					      \
      __atomic_load_n ((mem), __ATOMIC_RELAXED); })
-# define atomic_load_acquire(mem) \
+# define orig_atomic_load_acquire(mem) \
   ({ __atomic_check_size_ls((mem));					      \
      __atomic_load_n ((mem), __ATOMIC_ACQUIRE); })
 
-# define atomic_store_relaxed(mem, val) \
+# define orig_atomic_store_relaxed(mem, val) \
   do {									      \
     __atomic_check_size_ls((mem));					      \
     __atomic_store_n ((mem), (val), __ATOMIC_RELAXED);			      \
   } while (0)
-# define atomic_store_release(mem, val) \
+# define orig_atomic_store_release(mem, val) \
   do {									      \
     __atomic_check_size_ls((mem));					      \
     __atomic_store_n ((mem), (val), __ATOMIC_RELEASE);			      \
   } while (0)
 
 /* On failure, this CAS has memory_order_relaxed semantics.  */
-# define atomic_compare_exchange_weak_relaxed(mem, expected, desired) \
+# define orig_atomic_compare_exchange_weak_relaxed(mem, expected, desired) \
   ({ __atomic_check_size((mem));					      \
   __atomic_compare_exchange_n ((mem), (expected), (desired), 1,		      \
     __ATOMIC_RELAXED, __ATOMIC_RELAXED); })
-# define atomic_compare_exchange_weak_acquire(mem, expected, desired) \
+# define orig_atomic_compare_exchange_weak_acquire(mem, expected, desired) \
   ({ __atomic_check_size((mem));					      \
   __atomic_compare_exchange_n ((mem), (expected), (desired), 1,		      \
     __ATOMIC_ACQUIRE, __ATOMIC_RELAXED); })
-# define atomic_compare_exchange_weak_release(mem, expected, desired) \
+# define orig_atomic_compare_exchange_weak_release(mem, expected, desired) \
   ({ __atomic_check_size((mem));					      \
   __atomic_compare_exchange_n ((mem), (expected), (desired), 1,		      \
     __ATOMIC_RELEASE, __ATOMIC_RELAXED); })
 
-# define atomic_exchange_relaxed(mem, desired) \
+# define orig_atomic_exchange_relaxed(mem, desired) \
   ({ __atomic_check_size((mem));					      \
   __atomic_exchange_n ((mem), (desired), __ATOMIC_RELAXED); })
-# define atomic_exchange_acquire(mem, desired) \
+# define orig_atomic_exchange_acquire(mem, desired) \
   ({ __atomic_check_size((mem));					      \
   __atomic_exchange_n ((mem), (desired), __ATOMIC_ACQUIRE); })
-# define atomic_exchange_release(mem, desired) \
+# define orig_atomic_exchange_release(mem, desired) \
   ({ __atomic_check_size((mem));					      \
   __atomic_exchange_n ((mem), (desired), __ATOMIC_RELEASE); })
 
-# define atomic_fetch_add_relaxed(mem, operand) \
+# define orig_atomic_fetch_add_relaxed(mem, operand) \
   ({ __atomic_check_size((mem));					      \
   __atomic_fetch_add ((mem), (operand), __ATOMIC_RELAXED); })
-# define atomic_fetch_add_acquire(mem, operand) \
+# define orig_atomic_fetch_add_acquire(mem, operand) \
   ({ __atomic_check_size((mem));					      \
   __atomic_fetch_add ((mem), (operand), __ATOMIC_ACQUIRE); })
-# define atomic_fetch_add_release(mem, operand) \
+# define orig_atomic_fetch_add_release(mem, operand) \
   ({ __atomic_check_size((mem));					      \
   __atomic_fetch_add ((mem), (operand), __ATOMIC_RELEASE); })
-# define atomic_fetch_add_acq_rel(mem, operand) \
+# define orig_atomic_fetch_add_acq_rel(mem, operand) \
   ({ __atomic_check_size((mem));					      \
   __atomic_fetch_add ((mem), (operand), __ATOMIC_ACQ_REL); })
 
-# define atomic_fetch_and_relaxed(mem, operand) \
+# define orig_atomic_fetch_and_relaxed(mem, operand) \
   ({ __atomic_check_size((mem));					      \
   __atomic_fetch_and ((mem), (operand), __ATOMIC_RELAXED); })
-# define atomic_fetch_and_acquire(mem, operand) \
+# define orig_atomic_fetch_and_acquire(mem, operand) \
   ({ __atomic_check_size((mem));					      \
   __atomic_fetch_and ((mem), (operand), __ATOMIC_ACQUIRE); })
-# define atomic_fetch_and_release(mem, operand) \
+# define orig_atomic_fetch_and_release(mem, operand) \
   ({ __atomic_check_size((mem));					      \
   __atomic_fetch_and ((mem), (operand), __ATOMIC_RELEASE); })
 
-# define atomic_fetch_or_relaxed(mem, operand) \
+# define orig_atomic_fetch_or_relaxed(mem, operand) \
   ({ __atomic_check_size((mem));					      \
   __atomic_fetch_or ((mem), (operand), __ATOMIC_RELAXED); })
-# define atomic_fetch_or_acquire(mem, operand) \
+# define orig_atomic_fetch_or_acquire(mem, operand) \
   ({ __atomic_check_size((mem));					      \
   __atomic_fetch_or ((mem), (operand), __ATOMIC_ACQUIRE); })
-# define atomic_fetch_or_release(mem, operand) \
+# define orig_atomic_fetch_or_release(mem, operand) \
   ({ __atomic_check_size((mem));					      \
   __atomic_fetch_or ((mem), (operand), __ATOMIC_RELEASE); })
 
-# define atomic_fetch_xor_release(mem, operand) \
+# define orig_atomic_fetch_xor_release(mem, operand) \
   ({ __atomic_check_size((mem));					      \
   __atomic_fetch_xor ((mem), (operand), __ATOMIC_RELEASE); })
 
@@ -666,25 +666,25 @@
 #  define atomic_thread_fence_seq_cst() atomic_full_barrier ()
 # endif
 
-# ifndef atomic_load_relaxed
-#  define atomic_load_relaxed(mem) \
+# ifndef orig_atomic_load_relaxed
+#  define orig_atomic_load_relaxed(mem) \
    ({ __typeof (*(mem)) __atg100_val;					      \
    __asm ("" : "=r" (__atg100_val) : "0" (*(mem)));			      \
    __atg100_val; })
 # endif
-# ifndef atomic_load_acquire
-#  define atomic_load_acquire(mem) \
+# ifndef orig_atomic_load_acquire
+#  define orig_atomic_load_acquire(mem) \
    ({ __typeof (*(mem)) __atg101_val = atomic_load_relaxed (mem);	      \
    atomic_thread_fence_acquire ();					      \
    __atg101_val; })
 # endif
 
-# ifndef atomic_store_relaxed
+# ifndef orig_atomic_store_relaxed
 /* XXX Use inline asm here?  */
-#  define atomic_store_relaxed(mem, val) do { *(mem) = (val); } while (0)
+#  define orig_atomic_store_relaxed(mem, val) do { *(mem) = (val); } while (0)
 # endif
-# ifndef atomic_store_release
-#  define atomic_store_release(mem, val) \
+# ifndef orig_atomic_store_release
+#  define orig_atomic_store_release(mem, val) \
    do {									      \
      atomic_thread_fence_release ();					      \
      atomic_store_relaxed ((mem), (val));				      \
@@ -695,107 +695,107 @@
 /* XXX This potentially has one branch more than necessary, but archs
    currently do not define a CAS that returns both the previous value and
    the success flag.  */
-# ifndef atomic_compare_exchange_weak_acquire
-#  define atomic_compare_exchange_weak_acquire(mem, expected, desired) \
+# ifndef orig_atomic_compare_exchange_weak_acquire
+#  define orig_atomic_compare_exchange_weak_acquire(mem, expected, desired) \
    ({ typeof (*(expected)) __atg102_expected = *(expected);		      \
    *(expected) =							      \
-     atomic_compare_and_exchange_val_acq ((mem), (desired), *(expected));     \
+     orig_atomic_compare_and_exchange_val_acq ((mem), (desired), *(expected));     \
    *(expected) == __atg102_expected; })
 # endif
-# ifndef atomic_compare_exchange_weak_relaxed
+# ifndef orig_atomic_compare_exchange_weak_relaxed
 /* XXX Fall back to CAS with acquire MO because archs do not define a weaker
    CAS.  */
-#  define atomic_compare_exchange_weak_relaxed(mem, expected, desired) \
-   atomic_compare_exchange_weak_acquire ((mem), (expected), (desired))
+#  define orig_atomic_compare_exchange_weak_relaxed(mem, expected, desired) \
+   orig_atomic_compare_exchange_weak_acquire ((mem), (expected), (desired))
 # endif
-# ifndef atomic_compare_exchange_weak_release
-#  define atomic_compare_exchange_weak_release(mem, expected, desired) \
+# ifndef orig_atomic_compare_exchange_weak_release
+#  define orig_atomic_compare_exchange_weak_release(mem, expected, desired) \
    ({ typeof (*(expected)) __atg103_expected = *(expected);		      \
    *(expected) =							      \
-     atomic_compare_and_exchange_val_rel ((mem), (desired), *(expected));     \
+     orig_atomic_compare_and_exchange_val_rel ((mem), (desired), *(expected));     \
    *(expected) == __atg103_expected; })
 # endif
 
 /* XXX Fall back to acquire MO because archs do not define a weaker
    atomic_exchange.  */
-# ifndef atomic_exchange_relaxed
-#  define atomic_exchange_relaxed(mem, val) \
-   atomic_exchange_acq ((mem), (val))
+# ifndef orig_atomic_exchange_relaxed
+#  define orig_atomic_exchange_relaxed(mem, val) \
+   orig_atomic_exchange_acq ((mem), (val))
 # endif
-# ifndef atomic_exchange_acquire
-#  define atomic_exchange_acquire(mem, val) \
-   atomic_exchange_acq ((mem), (val))
+# ifndef orig_atomic_exchange_acquire
+#  define orig_atomic_exchange_acquire(mem, val) \
+   orig_atomic_exchange_acq ((mem), (val))
 # endif
-# ifndef atomic_exchange_release
-#  define atomic_exchange_release(mem, val) \
-   atomic_exchange_rel ((mem), (val))
+# ifndef orig_atomic_exchange_release
+#  define orig_atomic_exchange_release(mem, val) \
+   orig_atomic_exchange_rel ((mem), (val))
 # endif
 
-# ifndef atomic_fetch_add_acquire
-#  define atomic_fetch_add_acquire(mem, operand) \
-   atomic_exchange_and_add_acq ((mem), (operand))
+# ifndef orig_atomic_fetch_add_acquire
+#  define orig_atomic_fetch_add_acquire(mem, operand) \
+   orig_atomic_exchange_and_add_acq ((mem), (operand))
 # endif
-# ifndef atomic_fetch_add_relaxed
+# ifndef orig_atomic_fetch_add_relaxed
 /* XXX Fall back to acquire MO because the MO semantics of
    atomic_exchange_and_add are not documented; the generic version falls back
    to atomic_exchange_and_add_acq if atomic_exchange_and_add is not defined,
    and vice versa.  */
-#  define atomic_fetch_add_relaxed(mem, operand) \
-   atomic_fetch_add_acquire ((mem), (operand))
+#  define orig_atomic_fetch_add_relaxed(mem, operand) \
+   orig_atomic_fetch_add_acquire ((mem), (operand))
 # endif
-# ifndef atomic_fetch_add_release
-#  define atomic_fetch_add_release(mem, operand) \
-   atomic_exchange_and_add_rel ((mem), (operand))
+# ifndef orig_atomic_fetch_add_release
+#  define orig_atomic_fetch_add_release(mem, operand) \
+   orig_atomic_exchange_and_add_rel ((mem), (operand))
 # endif
-# ifndef atomic_fetch_add_acq_rel
-#  define atomic_fetch_add_acq_rel(mem, operand) \
+# ifndef orig_atomic_fetch_add_acq_rel
+#  define orig_atomic_fetch_add_acq_rel(mem, operand) \
    ({ atomic_thread_fence_release ();					      \
-   atomic_exchange_and_add_acq ((mem), (operand)); })
+   orig_atomic_exchange_and_add_acq ((mem), (operand)); })
 # endif
 
 /* XXX Fall back to acquire MO because archs do not define a weaker
    atomic_and_val.  */
-# ifndef atomic_fetch_and_relaxed
-#  define atomic_fetch_and_relaxed(mem, operand) \
-   atomic_fetch_and_acquire ((mem), (operand))
+# ifndef orig_atomic_fetch_and_relaxed
+#  define orig_atomic_fetch_and_relaxed(mem, operand) \
+   orig_atomic_fetch_and_acquire ((mem), (operand))
 # endif
 /* XXX The default for atomic_and_val has acquire semantics, but this is not
    documented.  */
-# ifndef atomic_fetch_and_acquire
-#  define atomic_fetch_and_acquire(mem, operand) \
-   atomic_and_val ((mem), (operand))
+# ifndef orig_atomic_fetch_and_acquire
+#  define orig_atomic_fetch_and_acquire(mem, operand) \
+   orig_atomic_and_val ((mem), (operand))
 # endif
-# ifndef atomic_fetch_and_release
+# ifndef orig_atomic_fetch_and_release
 /* XXX This unnecessarily has acquire MO.  */
-#  define atomic_fetch_and_release(mem, operand) \
+#  define orig_atomic_fetch_and_release(mem, operand) \
    ({ atomic_thread_fence_release ();					      \
-   atomic_and_val ((mem), (operand)); })
+   orig_atomic_and_val ((mem), (operand)); })
 # endif
 
 /* XXX The default for atomic_or_val has acquire semantics, but this is not
    documented.  */
-# ifndef atomic_fetch_or_acquire
-#  define atomic_fetch_or_acquire(mem, operand) \
-   atomic_or_val ((mem), (operand))
+# ifndef orig_atomic_fetch_or_acquire
+#  define orig_atomic_fetch_or_acquire(mem, operand) \
+   orig_atomic_or_val ((mem), (operand))
 # endif
 /* XXX Fall back to acquire MO because archs do not define a weaker
    atomic_or_val.  */
-# ifndef atomic_fetch_or_relaxed
-#  define atomic_fetch_or_relaxed(mem, operand) \
-   atomic_fetch_or_acquire ((mem), (operand))
+# ifndef orig_atomic_fetch_or_relaxed
+#  define orig_atomic_fetch_or_relaxed(mem, operand) \
+   orig_atomic_fetch_or_acquire ((mem), (operand))
 # endif
 /* XXX Contains an unnecessary acquire MO because archs do not define a weaker
    atomic_or_val.  */
-# ifndef atomic_fetch_or_release
-#  define atomic_fetch_or_release(mem, operand) \
+# ifndef orig_atomic_fetch_or_release
+#  define orig_atomic_fetch_or_release(mem, operand) \
    ({ atomic_thread_fence_release ();					      \
-   atomic_fetch_or_acquire ((mem), (operand)); })
+   orig_atomic_fetch_or_acquire ((mem), (operand)); })
 # endif
 
-# ifndef atomic_fetch_xor_release
+# ifndef orig_atomic_fetch_xor_release
 /* Failing the atomic_compare_exchange_weak_release reloads the value in
    __atg104_expected, so we need only do the XOR again and retry.  */
-# define atomic_fetch_xor_release(mem, operand) \
+# define orig_atomic_fetch_xor_release(mem, operand) \
   ({ __typeof (mem) __atg104_memp = (mem);				      \
      __typeof (*(mem)) __atg104_expected = (*__atg104_memp);		      \
      __typeof (*(mem)) __atg104_desired;				      \
@@ -804,7 +804,7 @@
      do									      \
        __atg104_desired = __atg104_expected ^ __atg104_op;		      \
      while (__glibc_unlikely						      \
-	    (atomic_compare_exchange_weak_release (			      \
+	    (orig_atomic_compare_exchange_weak_release (			      \
 	       __atg104_memp, &__atg104_expected, __atg104_desired)	      \
 	     == 0));							      \
      __atg104_expected; })
diff -N -r --unified glibc-2.25.orig/inet/getnameinfo.c glibc-2.25/inet/getnameinfo.c
--- glibc-2.25.orig/inet/getnameinfo.c	2017-09-11 17:14:56.983333356 +0000
+++ glibc-2.25/inet/getnameinfo.c	2017-09-13 19:55:55.884738078 +0000
@@ -91,8 +91,6 @@
 {
   static int not_first;
 
-  if (! not_first)
-    {
       __libc_lock_define_initialized (static, lock);
       __libc_lock_lock (lock);
 
@@ -182,7 +180,6 @@
 	}
 
       __libc_lock_unlock (lock);
-    }
 
   return domain;
 };
diff -N -r --unified glibc-2.25.orig/malloc/arena.c glibc-2.25/malloc/arena.c
--- glibc-2.25.orig/malloc/arena.c	2017-09-11 17:15:01.342645189 +0000
+++ glibc-2.25/malloc/arena.c	2017-09-13 19:55:55.890738655 +0000
@@ -456,7 +456,7 @@
 new_heap (size_t size, size_t top_pad)
 {
   size_t pagesize = GLRO (dl_pagesize);
-  char *p1, *p2;
+  char *p1, *p2, *prev_heap_area;
   unsigned long ul;
   heap_info *h;
 
@@ -475,12 +475,13 @@
      mapping (on Linux, this is the case for all non-writable mappings
      anyway). */
   p2 = MAP_FAILED;
-  if (aligned_heap_area)
+  prev_heap_area = atomic_load(aligned_heap_area);
+  if (prev_heap_area)
     {
-      p2 = (char *) MMAP (aligned_heap_area, HEAP_MAX_SIZE, PROT_NONE,
+      p2 = (char *) MMAP (prev_heap_area, HEAP_MAX_SIZE, PROT_NONE,
                           MAP_NORESERVE);
-      aligned_heap_area = NULL;
-      if (p2 != MAP_FAILED && ((unsigned long) p2 & (HEAP_MAX_SIZE - 1)))
+      atomic_store(aligned_heap_area, NULL);
+      if (p2 != MAP_FAILED && !mvee_all_heaps_aligned(p2, HEAP_MAX_SIZE))
         {
           __munmap (p2, HEAP_MAX_SIZE);
           p2 = MAP_FAILED;
@@ -488,6 +489,7 @@
     }
   if (p2 == MAP_FAILED)
     {
+	  (void) mvee_all_heaps_aligned(0, HEAP_MAX_SIZE);
       p1 = (char *) MMAP (0, HEAP_MAX_SIZE << 1, PROT_NONE, MAP_NORESERVE);
       if (p1 != MAP_FAILED)
         {
@@ -496,9 +498,8 @@
           ul = p2 - p1;
           if (ul)
             __munmap (p1, ul);
-          else
-            aligned_heap_area = p2 + HEAP_MAX_SIZE;
           __munmap (p2 + HEAP_MAX_SIZE, HEAP_MAX_SIZE - ul);
+		  atomic_store(aligned_heap_area, p2 + HEAP_MAX_SIZE);
         }
       else
         {
@@ -508,7 +509,7 @@
           if (p2 == MAP_FAILED)
             return 0;
 
-          if ((unsigned long) p2 & (HEAP_MAX_SIZE - 1))
+          if (!mvee_all_heaps_aligned(p2, HEAP_MAX_SIZE))
             {
               __munmap (p2, HEAP_MAX_SIZE);
               return 0;
@@ -590,8 +591,8 @@
 
 #define delete_heap(heap) \
   do {									      \
-      if ((char *) (heap) + HEAP_MAX_SIZE == aligned_heap_area)		      \
-        aligned_heap_area = NULL;					      \
+	if ((char *) (heap) + HEAP_MAX_SIZE == atomic_load(aligned_heap_area)) \
+	  atomic_store(aligned_heap_area, NULL);							\
       __munmap ((char *) (heap), HEAP_MAX_SIZE);			      \
     } while (0)
 
@@ -761,9 +762,7 @@
 get_free_list (void)
 {
   mstate replaced_arena = thread_arena;
-  mstate result = free_list;
-  if (result != NULL)
-    {
+  mstate result;
       __libc_lock_lock (free_list_lock);
       result = free_list;
       if (result != NULL)
@@ -784,7 +783,6 @@
           __libc_lock_lock (result->mutex);
 	  thread_arena = result;
         }
-    }
 
   return result;
 }
@@ -901,7 +899,7 @@
         {
           if (mp_.arena_max != 0)
             narenas_limit = mp_.arena_max;
-          else if (narenas > mp_.arena_test)
+          else if (atomic_load(narenas) > mp_.arena_test)
             {
               int n = __get_nprocs ();
 
@@ -914,7 +912,7 @@
             }
         }
     repeat:;
-      size_t n = narenas;
+      size_t n = atomic_load(narenas);
       /* NB: the following depends on the fact that (size_t)0 - 1 is a
          very large number and that the underflow is OK.  If arena_max
          is set the value of arena_test is irrelevant.  If arena_test
diff -N -r --unified glibc-2.25.orig/malloc/hooks.c glibc-2.25/malloc/hooks.c
--- glibc-2.25.orig/malloc/hooks.c	2017-09-11 17:15:01.361646548 +0000
+++ glibc-2.25/malloc/hooks.c	2017-09-13 19:55:55.894738998 +0000
@@ -77,10 +77,10 @@
       return;
     }
   using_malloc_checking = 1;
-  __malloc_hook = malloc_check;
-  __free_hook = free_check;
-  __realloc_hook = realloc_check;
-  __memalign_hook = memalign_check;
+  atomic_store(__malloc_hook, malloc_check);
+  atomic_store(__free_hook, free_check);
+  atomic_store(__realloc_hook, realloc_check);
+  atomic_store(__memalign_hook, memalign_check);
 }
 
 /* A simple, standard set of debugging hooks.  Overhead is `only' one
@@ -525,10 +525,10 @@
      cannot be more than one thread when we reach this point.  */
 
   /* Disable the malloc hooks (and malloc checking).  */
-  __malloc_hook = NULL;
-  __realloc_hook = NULL;
-  __free_hook = NULL;
-  __memalign_hook = NULL;
+  atomic_store(__malloc_hook, NULL);
+  atomic_store(__realloc_hook, NULL);
+  atomic_store(__free_hook, NULL);
+  atomic_store(__memalign_hook, NULL);
   using_malloc_checking = 0;
 
   /* Patch the dumped heap.  We no longer try to integrate into the
diff -N -r --unified glibc-2.25.orig/malloc/malloc.c glibc-2.25/malloc/malloc.c
--- glibc-2.25.orig/malloc/malloc.c	2017-09-11 17:15:01.384648194 +0000
+++ glibc-2.25/malloc/malloc.c	2017-09-13 19:55:55.910740181 +0000
@@ -1601,7 +1601,7 @@
 
 #define FASTCHUNKS_BIT        (1U)
 
-#define have_fastchunks(M)     (((M)->flags & FASTCHUNKS_BIT) == 0)
+#define have_fastchunks(M)     ((atomic_load((M)->flags) & FASTCHUNKS_BIT) == 0)
 #define clear_fastchunks(M)    catomic_or (&(M)->flags, FASTCHUNKS_BIT)
 #define set_fastchunks(M)      catomic_and (&(M)->flags, ~FASTCHUNKS_BIT)
 
@@ -2120,7 +2120,7 @@
 
   for (i = 0; i < NFASTBINS; ++i)
     {
-      p = fastbin (av, i);
+      p = atomic_load(fastbin (av, i));
 
       /* The following test can only be performed for the main arena.
          While mallopt calls malloc_consolidate to get rid of all fast
@@ -2272,7 +2272,7 @@
 
   if (av == NULL
       || ((unsigned long) (nb) >= (unsigned long) (mp_.mmap_threshold)
-	  && (mp_.n_mmaps < mp_.n_mmaps_max)))
+		  && (atomic_load(mp_.n_mmaps) < mp_.n_mmaps_max)))
     {
       char *mm;           /* return value from mmap call*/
 
@@ -3368,7 +3368,7 @@
     {
       idx = fastbin_index (nb);
       mfastbinptr *fb = &fastbin (av, idx);
-      mchunkptr pp = *fb;
+      mchunkptr pp = atomic_load(*fb);
       do
         {
           victim = pp;
@@ -3932,7 +3932,7 @@
     fb = &fastbin (av, idx);
 
     /* Atomically link P to its fastbin: P->FD = *FB; *FB = P;  */
-    mchunkptr old = *fb, old2;
+    mchunkptr old = atomic_load(*fb), old2;
     unsigned int old_idx = ~0u;
     do
       {
@@ -4648,7 +4648,7 @@
 
   for (i = 0; i < NFASTBINS; ++i)
     {
-      for (p = fastbin (av, i); p != 0; p = p->fd)
+      for (p = atomic_load(fastbin (av, i)); p != 0; p = p->fd)
         {
           ++nfastblocks;
           fastavail += chunksize (p);
@@ -4676,8 +4676,8 @@
   m->fsmblks += fastavail;
   if (av == &main_arena)
     {
-      m->hblks = mp_.n_mmaps;
-      m->hblkhd = mp_.mmapped_mem;
+      m->hblks = atomic_load(mp_.n_mmaps);
+      m->hblkhd = atomic_load(mp_.mmapped_mem);
       m->usmblks = 0;
       m->keepcost = chunksize (av->top);
     }
@@ -4717,7 +4717,7 @@
 {
   int i;
   mstate ar_ptr;
-  unsigned int in_use_b = mp_.mmapped_mem, system_b = in_use_b;
+  unsigned int in_use_b = atomic_load(mp_.mmapped_mem), system_b = in_use_b;
 
   if (__malloc_initialized < 0)
     ptmalloc_init ();
@@ -4748,9 +4748,9 @@
   fprintf (stderr, "Total (incl. mmap):\n");
   fprintf (stderr, "system bytes     = %10u\n", system_b);
   fprintf (stderr, "in use bytes     = %10u\n", in_use_b);
-  fprintf (stderr, "max mmap regions = %10u\n", (unsigned int) mp_.max_n_mmaps);
+  fprintf (stderr, "max mmap regions = %10u\n", (unsigned int) atomic_load(mp_.max_n_mmaps));
   fprintf (stderr, "max mmap bytes   = %10lu\n",
-           (unsigned long) mp_.max_mmapped_mem);
+           (unsigned long) atomic_load(mp_.max_mmapped_mem));
   ((_IO_FILE *) stderr)->_flags2 |= old_flags2;
   _IO_funlockfile (stderr);
 }
@@ -5156,7 +5156,7 @@
 
       for (size_t i = 0; i < NFASTBINS; ++i)
 	{
-	  mchunkptr p = fastbin (ar_ptr, i);
+	  mchunkptr p = atomic_load(fastbin (ar_ptr, i));
 	  if (p != NULL)
 	    {
 	      size_t nthissize = 0;
@@ -5278,7 +5278,7 @@
 	   "<aspace type=\"mprotect\" size=\"%zu\"/>\n"
 	   "</malloc>\n",
 	   total_nfastblocks, total_fastavail, total_nblocks, total_avail,
-	   mp_.n_mmaps, mp_.mmapped_mem,
+		   atomic_load(mp_.n_mmaps), atomic_load(mp_.mmapped_mem),
 	   total_system, total_max_system,
 	   total_aspace, total_aspace_mprotect);
 
diff -N -r --unified glibc-2.25.orig/malloc/mcheck.c glibc-2.25/malloc/mcheck.c
--- glibc-2.25.orig/malloc/mcheck.c	2017-09-11 17:15:01.386648337 +0000
+++ glibc-2.25/malloc/mcheck.c	2017-09-13 19:55:55.916740610 +0000
@@ -189,12 +189,12 @@
       flood (ptr, FREEFLOOD, hdr->size);
       ptr = hdr->block;
     }
-  __free_hook = old_free_hook;
+  atomic_store(__free_hook, old_free_hook);
   if (old_free_hook != NULL)
     (*old_free_hook)(ptr, caller);
   else
     free (ptr);
-  __free_hook = freehook;
+  atomic_store(__free_hook, freehook);
 }
 
 static __ptr_t
@@ -211,13 +211,13 @@
       return NULL;
     }
 
-  __malloc_hook = old_malloc_hook;
+  atomic_store(__malloc_hook, old_malloc_hook);
   if (old_malloc_hook != NULL)
     hdr = (struct hdr *) (*old_malloc_hook)(sizeof (struct hdr) + size + 1,
                                             caller);
   else
     hdr = (struct hdr *) malloc (sizeof (struct hdr) + size + 1);
-  __malloc_hook = mallochook;
+  atomic_store(__malloc_hook, mallochook);
   if (hdr == NULL)
     return NULL;
 
@@ -249,12 +249,12 @@
       return NULL;
     }
 
-  __memalign_hook = old_memalign_hook;
+  atomic_store(__memalign_hook, old_memalign_hook);
   if (old_memalign_hook != NULL)
     block = (*old_memalign_hook)(alignment, slop + size + 1, caller);
   else
     block = memalign (alignment, slop + size + 1);
-  __memalign_hook = memalignhook;
+  atomic_store(__memalign_hook, memalignhook);
   if (block == NULL)
     return NULL;
 
@@ -305,10 +305,10 @@
       osize = 0;
       hdr = NULL;
     }
-  __free_hook = old_free_hook;
-  __malloc_hook = old_malloc_hook;
-  __memalign_hook = old_memalign_hook;
-  __realloc_hook = old_realloc_hook;
+  atomic_store(__free_hook, old_free_hook);
+  atomic_store(__malloc_hook, old_malloc_hook);
+  atomic_store(__memalign_hook, old_memalign_hook);
+  atomic_store(__realloc_hook, old_realloc_hook);
   if (old_realloc_hook != NULL)
     hdr = (struct hdr *) (*old_realloc_hook)((__ptr_t) hdr,
                                              sizeof (struct hdr) + size + 1,
@@ -316,10 +316,10 @@
   else
     hdr = (struct hdr *) realloc ((__ptr_t) hdr,
                                   sizeof (struct hdr) + size + 1);
-  __free_hook = freehook;
-  __malloc_hook = mallochook;
-  __memalign_hook = memalignhook;
-  __realloc_hook = reallochook;
+  atomic_store(__free_hook, freehook);
+  atomic_store(__malloc_hook, mallochook);
+  atomic_store(__memalign_hook, memalignhook);
+  atomic_store(__realloc_hook, reallochook);
   if (hdr == NULL)
     return NULL;
 
@@ -383,15 +383,15 @@
       p = malloc_opt_barrier (p);
       free (p);
 
-      old_free_hook = __free_hook;
-      __free_hook = freehook;
-      old_malloc_hook = __malloc_hook;
-      __malloc_hook = mallochook;
-      old_memalign_hook = __memalign_hook;
-      __memalign_hook = memalignhook;
-      old_realloc_hook = __realloc_hook;
-      __realloc_hook = reallochook;
-      mcheck_used = 1;
+      old_free_hook = atomic_load(__free_hook);
+      atomic_store(__free_hook, freehook);
+      old_malloc_hook = atomic_load(__malloc_hook);
+      atomic_store(__malloc_hook, mallochook);
+      old_memalign_hook = atomic_load(__memalign_hook);
+      atomic_store(__memalign_hook, memalignhook);
+      old_realloc_hook = atomic_load(__realloc_hook);
+	  atomic_store(__realloc_hook, reallochook);      
+	  mcheck_used = 1;
     }
 
   return mcheck_used ? 0 : -1;
diff -N -r --unified glibc-2.25.orig/malloc/mtrace.c glibc-2.25/malloc/mtrace.c
--- glibc-2.25.orig/malloc/mtrace.c	2017-09-11 17:15:01.407649839 +0000
+++ glibc-2.25/malloc/mtrace.c	2017-09-13 19:55:55.921740967 +0000
@@ -137,12 +137,12 @@
       tr_break ();
       __libc_lock_lock (lock);
     }
-  __free_hook = tr_old_free_hook;
+  atomic_store(__free_hook, tr_old_free_hook);
   if (tr_old_free_hook != NULL)
     (*tr_old_free_hook)(ptr, caller);
   else
     free (ptr);
-  __free_hook = tr_freehook;
+  atomic_store(__free_hook, tr_freehook);
   __libc_lock_unlock (lock);
 }
 
@@ -154,12 +154,12 @@
   Dl_info mem;
   Dl_info *info = lock_and_info (caller, &mem);
 
-  __malloc_hook = tr_old_malloc_hook;
+  atomic_store(__malloc_hook, tr_old_malloc_hook);
   if (tr_old_malloc_hook != NULL)
     hdr = (__ptr_t) (*tr_old_malloc_hook)(size, caller);
   else
     hdr = (__ptr_t) malloc (size);
-  __malloc_hook = tr_mallochook;
+  atomic_store(__malloc_hook, tr_mallochook);
 
   tr_where (caller, info);
   /* We could be printing a NULL here; that's OK.  */
@@ -184,16 +184,16 @@
   Dl_info mem;
   Dl_info *info = lock_and_info (caller, &mem);
 
-  __free_hook = tr_old_free_hook;
-  __malloc_hook = tr_old_malloc_hook;
-  __realloc_hook = tr_old_realloc_hook;
+  atomic_store(__free_hook, tr_old_free_hook);
+  atomic_store(__malloc_hook, tr_old_malloc_hook);
+  atomic_store(__realloc_hook, tr_old_realloc_hook);
   if (tr_old_realloc_hook != NULL)
     hdr = (__ptr_t) (*tr_old_realloc_hook)(ptr, size, caller);
   else
     hdr = (__ptr_t) realloc (ptr, size);
-  __free_hook = tr_freehook;
-  __malloc_hook = tr_mallochook;
-  __realloc_hook = tr_reallochook;
+  atomic_store(__free_hook, tr_freehook);
+  atomic_store(__malloc_hook, tr_mallochook);
+  atomic_store(__realloc_hook, tr_reallochook);
 
   tr_where (caller, info);
   if (hdr == NULL)
@@ -229,14 +229,14 @@
   Dl_info mem;
   Dl_info *info = lock_and_info (caller, &mem);
 
-  __memalign_hook = tr_old_memalign_hook;
-  __malloc_hook = tr_old_malloc_hook;
+  atomic_store(__memalign_hook, tr_old_memalign_hook);
+  atomic_store(__malloc_hook, tr_old_malloc_hook);
   if (tr_old_memalign_hook != NULL)
     hdr = (__ptr_t) (*tr_old_memalign_hook)(alignment, size, caller);
   else
     hdr = (__ptr_t) memalign (alignment, size);
-  __memalign_hook = tr_memalignhook;
-  __malloc_hook = tr_mallochook;
+  atomic_store(__memalign_hook, tr_memalignhook);
+  atomic_store(__malloc_hook, tr_mallochook);
 
   tr_where (caller, info);
   /* We could be printing a NULL here; that's OK.  */
@@ -313,14 +313,14 @@
           malloc_trace_buffer = mtb;
           setvbuf (mallstream, malloc_trace_buffer, _IOFBF, TRACE_BUFFER_SIZE);
           fprintf (mallstream, "= Start\n");
-          tr_old_free_hook = __free_hook;
-          __free_hook = tr_freehook;
-          tr_old_malloc_hook = __malloc_hook;
-          __malloc_hook = tr_mallochook;
-          tr_old_realloc_hook = __realloc_hook;
-          __realloc_hook = tr_reallochook;
-          tr_old_memalign_hook = __memalign_hook;
-          __memalign_hook = tr_memalignhook;
+          tr_old_free_hook = atomic_load(__free_hook);
+          atomic_store(__free_hook, tr_freehook);
+          tr_old_malloc_hook = atomic_load(__malloc_hook);
+          atomic_store(__malloc_hook, tr_mallochook);
+          tr_old_realloc_hook = atomic_load(__realloc_hook);
+          atomic_store(__realloc_hook, tr_reallochook);
+          tr_old_memalign_hook = atomic_load(__memalign_hook);
+          atomic_store(__memalign_hook, tr_memalignhook);
 #ifdef _LIBC
           if (!added_atexit_handler)
             {
@@ -347,10 +347,10 @@
      file.  */
   FILE *f = mallstream;
   mallstream = NULL;
-  __free_hook = tr_old_free_hook;
-  __malloc_hook = tr_old_malloc_hook;
-  __realloc_hook = tr_old_realloc_hook;
-  __memalign_hook = tr_old_memalign_hook;
+  atomic_store(__free_hook, tr_old_free_hook);
+  atomic_store(__malloc_hook, tr_old_malloc_hook);
+  atomic_store(__realloc_hook, tr_old_realloc_hook);
+  atomic_store(__memalign_hook, tr_old_memalign_hook);
 
   fprintf (f, "= End\n");
   fclose (f);
diff -N -r --unified glibc-2.25.orig/nptl/allocatestack.c glibc-2.25/nptl/allocatestack.c
--- glibc-2.25.orig/nptl/allocatestack.c	2017-09-11 17:15:06.345003021 +0000
+++ glibc-2.25/nptl/allocatestack.c	2017-09-13 19:55:55.925741253 +0000
@@ -865,7 +865,7 @@
       if (curp != self)
 	{
 	  /* This marks the stack as free.  */
-	  curp->tid = 0;
+		atomic_store(curp->tid, 0);
 
 	  /* Account for the size of the stack.  */
 	  stack_cache_actsize += curp->stackblock_size;
diff -N -r --unified glibc-2.25.orig/nptl/cancellation.c glibc-2.25/nptl/cancellation.c
--- glibc-2.25.orig/nptl/cancellation.c	2017-09-11 17:15:06.346003093 +0000
+++ glibc-2.25/nptl/cancellation.c	2017-09-13 19:55:55.928741468 +0000
@@ -30,7 +30,7 @@
 __pthread_enable_asynccancel (void)
 {
   struct pthread *self = THREAD_SELF;
-  int oldval = THREAD_GETMEM (self, cancelhandling);
+  int oldval = THREAD_ATOMIC_GETMEM (self, cancelhandling);
 
   while (1)
     {
@@ -45,7 +45,7 @@
 	{
 	  if (CANCEL_ENABLED_AND_CANCELED_AND_ASYNCHRONOUS (newval))
 	    {
-	      THREAD_SETMEM (self, result, PTHREAD_CANCELED);
+	      THREAD_ATOMIC_SETMEM (self, result, PTHREAD_CANCELED);
 	      __do_cancel ();
 	    }
 
@@ -72,7 +72,7 @@
   struct pthread *self = THREAD_SELF;
   int newval;
 
-  int oldval = THREAD_GETMEM (self, cancelhandling);
+  int oldval = THREAD_ATOMIC_GETMEM (self, cancelhandling);
 
   while (1)
     {
@@ -96,6 +96,6 @@
     {
       futex_wait_simple ((unsigned int *) &self->cancelhandling, newval,
 			 FUTEX_PRIVATE);
-      newval = THREAD_GETMEM (self, cancelhandling);
+      newval = THREAD_ATOMIC_GETMEM (self, cancelhandling);
     }
 }
diff -N -r --unified glibc-2.25.orig/nptl/lll_timedwait_tid.c glibc-2.25/nptl/lll_timedwait_tid.c
--- glibc-2.25.orig/nptl/lll_timedwait_tid.c	2017-09-11 17:15:06.360004095 +0000
+++ glibc-2.25/nptl/lll_timedwait_tid.c	2017-09-13 19:55:55.931741682 +0000
@@ -37,7 +37,7 @@
     return EINVAL;
 
   /* Repeat until thread terminated.  */
-  while ((tid = *tidp) != 0)
+  while ((tid = (*tidp)) != 0 || mvee_should_sync_tid())
     {
       struct timeval tv;
       struct timespec rt;
@@ -62,8 +62,11 @@
          The kernel up to version 3.16.3 does not use the private futex
          operations for futex wake-up when the clone terminates.
       */
-      if (lll_futex_timed_wait (tidp, tid, &rt, LLL_SHARED) == -ETIMEDOUT)
+	  if (lll_futex_syscall (4, tidp, __lll_private_flag(mvee_should_sync_tid() ? MVEE_FUTEX_WAIT_TID : FUTEX_WAIT, LLL_SHARED),
+							 tid, &rt) == -ETIMEDOUT)
         return ETIMEDOUT;
+	  else if (*tidp == 0)
+		  break;
     }
 
   return 0;
diff -N -r --unified glibc-2.25.orig/nptl/lowlevellock.c glibc-2.25/nptl/lowlevellock.c
--- glibc-2.25.orig/nptl/lowlevellock.c	2017-09-11 17:15:06.360004095 +0000
+++ glibc-2.25/nptl/lowlevellock.c	2017-09-13 19:55:55.933741825 +0000
@@ -26,7 +26,7 @@
 void
 __lll_lock_wait_private (int *futex)
 {
-  if (*futex == 2)
+  if (atomic_load(*futex) == 2)
     lll_futex_wait (futex, 2, LLL_PRIVATE); /* Wait if *futex == 2.  */
 
   while (atomic_exchange_acq (futex, 2) != 0)
@@ -39,7 +39,7 @@
 void
 __lll_lock_wait (int *futex, int private)
 {
-  if (*futex == 2)
+  if (atomic_load(*futex) == 2)
     lll_futex_wait (futex, 2, private); /* Wait if *futex == 2.  */
 
   while (atomic_exchange_acq (futex, 2) != 0)
diff -N -r --unified glibc-2.25.orig/nptl/pthread_cond_common.c glibc-2.25/nptl/pthread_cond_common.c
--- glibc-2.25.orig/nptl/pthread_cond_common.c	2017-09-11 17:15:06.389006169 +0000
+++ glibc-2.25/nptl/pthread_cond_common.c	2017-09-13 19:55:55.936742039 +0000
@@ -54,7 +54,7 @@
 __condvar_add_g1_start_relaxed (pthread_cond_t *cond, unsigned int val)
 {
   atomic_store_relaxed (&cond->__data.__g1_start,
-      atomic_load_relaxed (&cond->__data.__g1_start) + val);
+      orig_atomic_load_relaxed (&cond->__data.__g1_start) + val);
 }
 
 #else
diff -N -r --unified glibc-2.25.orig/nptl/pthread_join.c glibc-2.25/nptl/pthread_join.c
--- glibc-2.25.orig/nptl/pthread_join.c	2017-09-11 17:15:06.410007671 +0000
+++ glibc-2.25/nptl/pthread_join.c	2017-09-13 19:55:55.938742182 +0000
@@ -100,7 +100,7 @@
   if (__glibc_likely (result == 0))
     {
       /* We mark the thread as terminated and as joined.  */
-      pd->tid = -1;
+		atomic_store(pd->tid, -1);
 
       /* Store the return value if the caller is interested.  */
       if (thread_return != NULL)
diff -N -r --unified glibc-2.25.orig/nptl/pthread_mutex_setprioceiling.c glibc-2.25/nptl/pthread_mutex_setprioceiling.c
--- glibc-2.25.orig/nptl/pthread_mutex_setprioceiling.c	2017-09-11 17:15:06.412007815 +0000
+++ glibc-2.25/nptl/pthread_mutex_setprioceiling.c	2017-09-13 19:55:55.941742397 +0000
@@ -109,9 +109,9 @@
 
   int newlock = 0;
   if (locked)
-    newlock = (mutex->__data.__lock & ~PTHREAD_MUTEX_PRIO_CEILING_MASK);
-  mutex->__data.__lock = newlock
-			 | (prioceiling << PTHREAD_MUTEX_PRIO_CEILING_SHIFT);
+	  newlock = (atomic_load(mutex->__data.__lock) & ~PTHREAD_MUTEX_PRIO_CEILING_MASK);
+  atomic_store(mutex->__data.__lock, newlock
+			   | (prioceiling << PTHREAD_MUTEX_PRIO_CEILING_SHIFT));
   atomic_full_barrier ();
 
   lll_futex_wake (&mutex->__data.__lock, INT_MAX,
diff -N -r --unified glibc-2.25.orig/nptl/pthread_spin_lock.c glibc-2.25/nptl/pthread_spin_lock.c
--- glibc-2.25.orig/nptl/pthread_spin_lock.c	2017-09-11 17:15:06.442009961 +0000
+++ glibc-2.25/nptl/pthread_spin_lock.c	2017-09-13 19:55:55.943742540 +0000
@@ -54,12 +54,12 @@
 	{
 	  int wait = SPIN_LOCK_READS_BETWEEN_CMPXCHG;
 
-	  while (*lock != 0 && wait > 0)
+	  while (atomic_load(*lock) != 0 && wait > 0)
 	    --wait;
 	}
       else
 	{
-	  while (*lock != 0)
+		while (atomic_load(*lock) != 0)
 	    ;
 	}
     }
diff -N -r --unified glibc-2.25.orig/nptl/pthread_spin_unlock.c glibc-2.25/nptl/pthread_spin_unlock.c
--- glibc-2.25.orig/nptl/pthread_spin_unlock.c	2017-09-11 17:15:06.442009961 +0000
+++ glibc-2.25/nptl/pthread_spin_unlock.c	2017-09-13 19:55:55.945742682 +0000
@@ -24,6 +24,6 @@
 pthread_spin_unlock (pthread_spinlock_t *lock)
 {
   atomic_full_barrier ();
-  *lock = 0;
+  atomic_store(*lock, 0);
   return 0;
 }
diff -N -r --unified glibc-2.25.orig/nptl/register-atfork.c glibc-2.25/nptl/register-atfork.c
--- glibc-2.25.orig/nptl/register-atfork.c	2017-09-11 17:15:06.443010032 +0000
+++ glibc-2.25/nptl/register-atfork.c	2017-09-13 19:55:55.948742897 +0000
@@ -51,18 +51,19 @@
     {
       /* Search for an empty entry.  */
       for (i = 0; i < NHANDLER; ++i)
-	if (runp->mem[i].refcntr == 0)
+        if (atomic_load(runp->mem[i].refcntr) == 0)
 	  goto found;
     }
-  while ((runp = runp->next) != NULL);
+  while ((runp = atomic_load(runp->next)) != NULL);
 
   /* We have to allocate a new entry.  */
   runp = (struct fork_handler_pool *) calloc (1, sizeof (*runp));
   if (runp != NULL)
     {
       /* Enqueue the new memory pool into the list.  */
-      runp->next = fork_handler_pool.next;
-      fork_handler_pool.next = runp;
+		void* tmp = atomic_load(fork_handler_pool.next);
+      atomic_store(runp->next, tmp);
+      atomic_store(fork_handler_pool.next, runp);
 
       /* We use the last entry on the page.  This means when we start
 	 searching from the front the next time we will find the first
@@ -71,7 +72,7 @@
 
     found:
       result = &runp->mem[i];
-      result->refcntr = 1;
+      atomic_store(result->refcntr, 1);
       result->need_signal = 0;
     }
 
@@ -112,7 +113,7 @@
 __linkin_atfork (struct fork_handler *newp)
 {
   do
-    newp->next = __fork_handlers;
+	  newp->next = atomic_load(__fork_handlers);
   while (catomic_compare_and_exchange_bool_acq (&__fork_handlers,
 						newp, newp->next) != 0);
 }
@@ -124,10 +125,10 @@
   lll_lock (__fork_lock, LLL_PRIVATE);
 
   /* No more fork handlers.  */
-  __fork_handlers = NULL;
+  atomic_store(__fork_handlers, NULL);
 
   /* Free eventually allocated memory blocks for the object pool.  */
-  struct fork_handler_pool *runp = fork_handler_pool.next;
+  struct fork_handler_pool *runp = atomic_load(fork_handler_pool.next);
 
   memset (&fork_handler_pool, '\0', sizeof (fork_handler_pool));
 
@@ -138,7 +139,7 @@
   while (runp != NULL)
     {
       struct fork_handler_pool *oldp = runp;
-      runp = runp->next;
+      runp = atomic_load(runp->next);
       free (oldp);
     }
 }
diff -N -r --unified glibc-2.25.orig/nptl/unregister-atfork.c glibc-2.25/nptl/unregister-atfork.c
--- glibc-2.25.orig/nptl/unregister-atfork.c	2017-09-11 17:15:06.618022551 +0000
+++ glibc-2.25/nptl/unregister-atfork.c	2017-09-13 19:55:55.951743111 +0000
@@ -33,7 +33,7 @@
      We do not worry about other threads adding entries for this DSO
      right this moment.  If this happens this is a race and we can do
      whatever we please.  The program will crash anyway seen.  */
-  struct fork_handler *runp = __fork_handlers;
+  struct fork_handler *runp = atomic_load(__fork_handlers);
   struct fork_handler *lastp = NULL;
 
   while (runp != NULL)
@@ -42,7 +42,7 @@
     else
       {
 	lastp = runp;
-	runp = runp->next;
+	runp = atomic_load(runp->next);
       }
 
   if (runp == NULL)
@@ -77,12 +77,15 @@
 							 runp->next, runp)
 		  != 0)
 		{
-		  runp = __fork_handlers;
+          runp = atomic_load(__fork_handlers);
 		  goto again;
 		}
 	    }
 	  else
-	    lastp->next = runp->next;
+	  {
+		  void* tmp = atomic_load(runp->next);
+        atomic_store(lastp->next, tmp);
+	  }
 
 	  /* We cannot overwrite the ->next element now.  Put the deleted
 	     entries in a separate list.  */
@@ -94,7 +97,7 @@
       else
 	lastp = runp;
 
-      runp = runp->next;
+      runp = atomic_load(runp->next);
     }
   while (runp != NULL);
 
@@ -113,9 +116,9 @@
 	 wait for the last user.  */
       atomic_decrement (&deleted->handler->refcntr);
       unsigned int val;
-      while ((val = deleted->handler->refcntr) != 0)
+      while ((val = atomic_load(deleted->handler->refcntr)) != 0)
 	futex_wait_simple (&deleted->handler->refcntr, val, FUTEX_PRIVATE);
 
-      deleted = deleted->next;
+      deleted = atomic_load(deleted->next);
     }
 }
diff -N -r --unified glibc-2.25.orig/resolv/res_libc.c glibc-2.25/resolv/res_libc.c
--- glibc-2.25.orig/resolv/res_libc.c	2017-09-11 17:15:07.654096665 +0000
+++ glibc-2.25/resolv/res_libc.c	2017-09-13 19:55:55.953743254 +0000
@@ -28,7 +28,7 @@
 
 extern unsigned long long int __res_initstamp attribute_hidden;
 /* We have atomic increment operations on 64-bit platforms.  */
-#if __WORDSIZE == 64
+#if 0
 # define atomicinclock(lock) (void) 0
 # define atomicincunlock(lock) (void) 0
 # define atomicinc(var) catomic_increment (&(var))
diff -N -r --unified glibc-2.25.orig/stdlib/cxa_atexit.c glibc-2.25/stdlib/cxa_atexit.c
--- glibc-2.25.orig/stdlib/cxa_atexit.c	2017-09-11 17:15:08.086127569 +0000
+++ glibc-2.25/stdlib/cxa_atexit.c	2017-09-13 19:55:55.956743469 +0000
@@ -44,7 +44,7 @@
   new->func.cxa.arg = arg;
   new->func.cxa.dso_handle = d;
   atomic_write_barrier ();
-  new->flavor = ef_cxa;
+  atomic_store(new->flavor, ef_cxa);
   return 0;
 }
 
@@ -81,7 +81,7 @@
   for (l = *listp; l != NULL; p = l, l = l->next)
     {
       for (i = l->idx; i > 0; --i)
-	if (l->fns[i - 1].flavor != ef_free)
+		  if (atomic_load(l->fns[i - 1].flavor) != ef_free)
 	  break;
 
       if (i > 0)
@@ -123,7 +123,7 @@
   /* Mark entry as used, but we don't know the flavor now.  */
   if (r != NULL)
     {
-      r->flavor = ef_us;
+		atomic_store(r->flavor, ef_us);
       ++__new_exitfn_called;
     }
 
diff -N -r --unified glibc-2.25.orig/stdlib/cxa_finalize.c glibc-2.25/stdlib/cxa_finalize.c
--- glibc-2.25.orig/stdlib/cxa_finalize.c	2017-09-11 17:15:08.086127569 +0000
+++ glibc-2.25/stdlib/cxa_finalize.c	2017-09-13 19:55:55.958743611 +0000
@@ -70,7 +70,7 @@
 
       for (f = &funcs->fns[funcs->idx - 1]; f >= &funcs->fns[0]; --f)
 	if (d == NULL || d == f->func.cxa.dso_handle)
-	  f->flavor = ef_free;
+		atomic_store(f->flavor, ef_free);
     }
 
   /* Remove the registered fork handlers.  We do not have to
diff -N -r --unified glibc-2.25.orig/stdlib/exit.c glibc-2.25/stdlib/exit.c
--- glibc-2.25.orig/stdlib/exit.c	2017-09-11 17:15:08.098128427 +0000
+++ glibc-2.25/stdlib/exit.c	2017-09-13 19:55:55.960743754 +0000
@@ -52,7 +52,7 @@
 	{
 	  const struct exit_function *const f =
 	    &cur->fns[--cur->idx];
-	  switch (f->flavor)
+	  switch (atomic_load(f->flavor))
 	    {
 	      void (*atfct) (void);
 	      void (*onfct) (int status, void *arg);
diff -N -r --unified glibc-2.25.orig/stdlib/on_exit.c glibc-2.25/stdlib/on_exit.c
--- glibc-2.25.orig/stdlib/on_exit.c	2017-09-11 17:15:08.151132219 +0000
+++ glibc-2.25/stdlib/on_exit.c	2017-09-13 19:55:55.962743897 +0000
@@ -35,7 +35,7 @@
   new->func.on.fn = func;
   new->func.on.arg = arg;
   atomic_write_barrier ();
-  new->flavor = ef_on;
+  atomic_store(new->flavor, ef_on);
   return 0;
 }
 weak_alias (__on_exit, on_exit)
diff -N -r --unified glibc-2.25.orig/sysdeps/arm/atomic-machine.h glibc-2.25/sysdeps/arm/atomic-machine.h
--- glibc-2.25.orig/sysdeps/arm/atomic-machine.h	2017-09-11 17:15:08.941188734 +0000
+++ glibc-2.25/sysdeps/arm/atomic-machine.h	2017-09-13 19:58:08.735967877 +0000
@@ -55,68 +55,68 @@
    a pattern to do this efficiently.  */
 #ifdef __GCC_HAVE_SYNC_COMPARE_AND_SWAP_4
 
-# define atomic_exchange_acq(mem, value)                                \
-  __atomic_val_bysize (__arch_exchange, int, mem, value, __ATOMIC_ACQUIRE)
+# define orig_atomic_exchange_acq(mem, value)                                \
+  __atomic_val_bysize (orig___arch_exchange, int, mem, value, __ATOMIC_ACQUIRE)
 
-# define atomic_exchange_rel(mem, value)                                \
-  __atomic_val_bysize (__arch_exchange, int, mem, value, __ATOMIC_RELEASE)
+# define orig_atomic_exchange_rel(mem, value)                                \
+  __atomic_val_bysize (orig___arch_exchange, int, mem, value, __ATOMIC_RELEASE)
 
 /* Atomic exchange (without compare).  */
 
-# define __arch_exchange_8_int(mem, newval, model)      \
+# define orig___arch_exchange_8_int(mem, newval, model)      \
   (__arm_link_error (), (typeof (*mem)) 0)
 
-# define __arch_exchange_16_int(mem, newval, model)     \
+# define orig___arch_exchange_16_int(mem, newval, model)     \
   (__arm_link_error (), (typeof (*mem)) 0)
 
-# define __arch_exchange_32_int(mem, newval, model)     \
+# define orig___arch_exchange_32_int(mem, newval, model)     \
   __atomic_exchange_n (mem, newval, model)
 
-# define __arch_exchange_64_int(mem, newval, model)     \
+# define orig___arch_exchange_64_int(mem, newval, model)     \
   (__arm_link_error (), (typeof (*mem)) 0)
 
 /* Compare and exchange with "acquire" semantics, ie barrier after.  */
 
-# define atomic_compare_and_exchange_bool_acq(mem, new, old)    \
-  __atomic_bool_bysize (__arch_compare_and_exchange_bool, int,  \
+# define orig_atomic_compare_and_exchange_bool_acq(mem, new, old)    \
+  __atomic_bool_bysize (orig___arch_compare_and_exchange_bool, int,  \
                         mem, new, old, __ATOMIC_ACQUIRE)
 
-# define atomic_compare_and_exchange_val_acq(mem, new, old)     \
-  __atomic_val_bysize (__arch_compare_and_exchange_val, int,    \
+# define orig_atomic_compare_and_exchange_val_acq(mem, new, old)     \
+  __atomic_val_bysize (orig___arch_compare_and_exchange_val, int,    \
                        mem, new, old, __ATOMIC_ACQUIRE)
 
 /* Compare and exchange with "release" semantics, ie barrier before.  */
 
-# define atomic_compare_and_exchange_val_rel(mem, new, old)      \
-  __atomic_val_bysize (__arch_compare_and_exchange_val, int,    \
+# define orig_atomic_compare_and_exchange_val_rel(mem, new, old)      \
+  __atomic_val_bysize (orig___arch_compare_and_exchange_val, int,    \
                        mem, new, old, __ATOMIC_RELEASE)
 
 /* Compare and exchange.
    For all "bool" routines, we return FALSE if exchange succesful.  */
 
-# define __arch_compare_and_exchange_bool_8_int(mem, newval, oldval, model) \
+# define orig___arch_compare_and_exchange_bool_8_int(mem, newval, oldval, model) \
   ({__arm_link_error (); 0; })
 
-# define __arch_compare_and_exchange_bool_16_int(mem, newval, oldval, model) \
+# define orig___arch_compare_and_exchange_bool_16_int(mem, newval, oldval, model) \
   ({__arm_link_error (); 0; })
 
-# define __arch_compare_and_exchange_bool_32_int(mem, newval, oldval, model) \
+# define orig___arch_compare_and_exchange_bool_32_int(mem, newval, oldval, model) \
   ({                                                                    \
     typeof (*mem) __oldval = (oldval);                                  \
     !__atomic_compare_exchange_n (mem, (void *) &__oldval, newval, 0,   \
                                   model, __ATOMIC_RELAXED);             \
   })
 
-# define __arch_compare_and_exchange_bool_64_int(mem, newval, oldval, model) \
+# define orig___arch_compare_and_exchange_bool_64_int(mem, newval, oldval, model) \
   ({__arm_link_error (); 0; })
 
-# define __arch_compare_and_exchange_val_8_int(mem, newval, oldval, model) \
+# define orig___arch_compare_and_exchange_val_8_int(mem, newval, oldval, model) \
   ({__arm_link_error (); oldval; })
 
-# define __arch_compare_and_exchange_val_16_int(mem, newval, oldval, model) \
+# define orig___arch_compare_and_exchange_val_16_int(mem, newval, oldval, model) \
   ({__arm_link_error (); oldval; })
 
-# define __arch_compare_and_exchange_val_32_int(mem, newval, oldval, model) \
+# define orig___arch_compare_and_exchange_val_32_int(mem, newval, oldval, model) \
   ({                                                                    \
     typeof (*mem) __oldval = (oldval);                                  \
     __atomic_compare_exchange_n (mem, (void *) &__oldval, newval, 0,    \
@@ -124,11 +124,11 @@
     __oldval;                                                           \
   })
 
-# define __arch_compare_and_exchange_val_64_int(mem, newval, oldval, model) \
+# define orig___arch_compare_and_exchange_val_64_int(mem, newval, oldval, model) \
   ({__arm_link_error (); oldval; })
 
 #else
-# define __arch_compare_and_exchange_val_32_acq(mem, newval, oldval) \
+# define orig___arch_compare_and_exchange_val_32_acq(mem, newval, oldval) \
   __arm_assisted_compare_and_exchange_val_32_acq ((mem), (newval), (oldval))
 #endif
 
@@ -152,3 +152,867 @@
 # define __arm_assisted_compare_and_exchange_val_32_acq(mem, newval, oldval) \
   ({ __arm_link_error (); oldval; })
 #endif
+
+/*--------------------------------------------------------------------------------
+                                  MVEE PATCHES
+--------------------------------------------------------------------------------*/
+#define USE_MVEE_LIBC
+
+#ifdef USE_MVEE_LIBC
+// NOTE: This is different from the base value for x86 because the ARM syscall
+// instruction can only encode small immediates as syscall numbers
+#define MVEE_FAKE_SYSCALL_BASE          0x6FF 
+#define MVEE_GET_MASTERTHREAD_ID        MVEE_FAKE_SYSCALL_BASE + 3
+#define MVEE_GET_SHARED_BUFFER          MVEE_FAKE_SYSCALL_BASE + 4
+#define MVEE_FLUSH_SHARED_BUFFER        MVEE_FAKE_SYSCALL_BASE + 5
+#define MVEE_SET_INFINITE_LOOP_PTR      MVEE_FAKE_SYSCALL_BASE + 6
+#define MVEE_TOGGLESYNC                 MVEE_FAKE_SYSCALL_BASE + 7
+#define MVEE_SET_SHARED_BUFFER_POS_PTR  MVEE_FAKE_SYSCALL_BASE + 8
+#define MVEE_RUNS_UNDER_MVEE_CONTROL    MVEE_FAKE_SYSCALL_BASE + 9
+#define MVEE_GET_THREAD_NUM             MVEE_FAKE_SYSCALL_BASE + 10
+#define MVEE_SET_SYNC_PRIMITIVES_PTR    MVEE_FAKE_SYSCALL_BASE + 12
+#define MVEE_ALL_HEAPS_ALIGNED          MVEE_FAKE_SYSCALL_BASE + 13
+#define MVEE_GET_VIRTUALIZED_ARGV0      MVEE_FAKE_SYSCALL_BASE + 17
+#define MVEE_LIBC_LOCK_BUFFER           3
+#define MVEE_LIBC_MALLOC_DEBUG_BUFFER   11
+#define MVEE_LIBC_ATOMIC_BUFFER         13
+#define MVEE_FUTEX_WAIT_TID             30
+
+enum mvee_alloc_types
+{
+	LIBC_MALLOC,
+	LIBC_FREE,
+	LIBC_REALLOC,
+	LIBC_MEMALIGN,
+	LIBC_CALLOC,
+	MALLOC_TRIM,
+	HEAP_TRIM,
+	MALLOC_CONSOLIDATE,
+	ARENA_GET2,
+	_INT_MALLOC,
+	_INT_FREE,
+	_INT_REALLOC
+};
+
+//
+// Atomic operations list. Keep this in sync with MVEE/Inc/MVEE_shm.h
+//
+enum mvee_atomics
+{
+    // LOAD OPERATIONS FIRST!!! DO NOT CHANGE THIS CONVENTION
+    ATOMIC_FORCED_READ,
+    ATOMIC_LOAD,
+    // THE FOLLOWING IS NOT AN ACTUAL ATOMIC OPERATION, IT JUST DENOTES THE END OF THE LOAD-ONLY ATOMICS!!!
+    ATOMIC_LOAD_MAX,
+    // STORES AFTER LOADS
+    CATOMIC_AND,
+    CATOMIC_OR,
+    CATOMIC_EXCHANGE_AND_ADD,
+    CATOMIC_ADD,
+    CATOMIC_INCREMENT,
+    CATOMIC_DECREMENT,
+    CATOMIC_MAX,
+    ATOMIC_COMPARE_AND_EXCHANGE_VAL,
+    ATOMIC_COMPARE_AND_EXCHANGE_BOOL,
+    ATOMIC_EXCHANGE,
+    ATOMIC_EXCHANGE_AND_ADD,
+    ATOMIC_INCREMENT_AND_TEST,
+    ATOMIC_DECREMENT_AND_TEST,
+	ATOMIC_ADD_NEGATIVE,
+    ATOMIC_ADD_ZERO,
+    ATOMIC_ADD,
+	ATOMIC_OR,
+	ATOMIC_OR_VAL,
+    ATOMIC_INCREMENT,
+    ATOMIC_DECREMENT,
+    ATOMIC_BIT_TEST_SET,
+    ATOMIC_BIT_SET,
+    ATOMIC_AND,
+	ATOMIC_AND_VAL,
+    ATOMIC_STORE,
+	ATOMIC_MIN,
+    ATOMIC_MAX,
+    ATOMIC_DECREMENT_IF_POSITIVE,
+	ATOMIC_FETCH_ADD,
+	ATOMIC_FETCH_AND,
+	ATOMIC_FETCH_OR,
+	ATOMIC_FETCH_XOR,
+    __THREAD_ATOMIC_CMPXCHG_VAL,
+    __THREAD_ATOMIC_AND,
+    __THREAD_ATOMIC_BIT_SET,
+    ___UNKNOWN_LOCK_TYPE___,
+    __MVEE_ATOMICS_MAX__
+};
+
+#define MVEE_ROUND_UP(x, multiple)				\
+	((x + (multiple - 1)) & ~(multiple -1))
+#define MVEE_MIN(a, b) ((a > b) ? (b) : (a))
+
+# ifdef MVEE_USE_TOTALPARTIAL_AGENT
+#  warning "The total and partial order agents have not been tested on ARM and will probably break!"
+#  include "mvee-totalpartial-agent.h"
+# else
+#  include "mvee-woc-agent.h"
+# endif
+
+#endif
+
+// We don't use our sync agent in the dynamic loader so just use the original atomics everywhere
+#if IS_IN (rtld) || !defined(USE_MVEE_LIBC)
+
+//
+// generic atomics (include/atomic.h and sysdeps/arch/atomic-machine.h)
+//
+#define __arch_c_compare_and_exchange_val_8_acq(mem, newval, oldval) orig___arch_c_compare_and_exchange_val_8_acq(mem, newval, oldval)
+#define __arch_c_compare_and_exchange_val_16_acq(mem, newval, oldval) orig___arch_c_compare_and_exchange_val_16_acq(mem, newval, oldval)
+#define __arch_c_compare_and_exchange_val_32_acq(mem, newval, oldval) orig___arch_c_compare_and_exchange_val_32_acq(mem, newval, oldval)
+#define __arch_c_compare_and_exchange_val_64_acq(mem, newval, oldval) orig___arch_c_compare_and_exchange_val_64_acq(mem, newval, oldval)
+#define atomic_compare_and_exchange_val_acq(mem, newval, oldval) orig_atomic_compare_and_exchange_val_acq(mem, newval, oldval)
+#define atomic_compare_and_exchange_val_rel(mem, newval, oldval) orig_atomic_compare_and_exchange_val_rel(mem, newval, oldval)
+#define atomic_compare_and_exchange_bool_acq(mem, newval, oldval) orig_atomic_compare_and_exchange_bool_acq(mem, newval, oldval)
+#define atomic_compare_and_exchange_bool_rel(mem, newval, oldval) orig_atomic_compare_and_exchange_bool_rel(mem, newval, oldval)
+#define atomic_exchange_acq(mem, newvalue) orig_atomic_exchange_acq(mem, newvalue)
+#define atomic_exchange_rel(mem, newvalue) orig_atomic_exchange_rel(mem, newvalue)
+#define atomic_exchange_and_add(mem, value) orig_atomic_exchange_and_add(mem, value)
+#define atomic_exchange_and_add_acq(mem, value) orig_atomic_exchange_and_add_acq(mem, value)
+#define atomic_exchange_and_add_rel(mem, value) orig_atomic_exchange_and_add_rel(mem, value)
+#define atomic_add(mem, value) orig_atomic_add(mem, value)
+#define atomic_increment(mem) orig_atomic_increment(mem)
+#define atomic_increment_and_test(mem) orig_atomic_increment_and_test(mem)
+#define atomic_increment_val(mem) orig_atomic_increment_val(mem)
+#define atomic_decrement(mem) orig_atomic_decrement(mem)
+#define atomic_decrement_and_test(mem) orig_atomic_decrement_and_test(mem)
+#define atomic_decrement_val(mem) orig_atomic_decrement_val(mem)
+#define atomic_add_negative(mem, value) orig_atomic_add_negative(mem, value)
+#define atomic_add_zero(mem, value) orig_atomic_add_zero(mem, value)
+#define atomic_bit_set(mem, bit) orig_atomic_bit_set(mem, bit)
+#define atomic_bit_test_set(mem, bit) orig_atomic_bit_test_set(mem, bit)
+#define atomic_and(mem, mask) orig_atomic_and(mem, mask)
+#define atomic_or(mem, mask) orig_atomic_or(mem, mask)
+#define atomic_max(mem, value) orig_atomic_max(mem, value)
+#define atomic_min(mem, value) orig_atomic_min(mem, value)
+#define atomic_decrement_if_positive(mem) orig_atomic_decrement_if_positive(mem)
+#define atomic_and_val(mem, mask) orig_atomic_and_val(mem, mask)
+#define atomic_or_val(mem, mask) orig_atomic_or_val(mem, mask)
+#define atomic_forced_read(x) orig_atomic_forced_read(x)
+#define catomic_compare_and_exchange_val_acq(mem, newval, oldval) orig_catomic_compare_and_exchange_val_acq(mem, newval, oldval)
+#define catomic_compare_and_exchange_val_rel(mem, newval, oldval) orig_catomic_compare_and_exchange_val_rel(mem, newval, oldval)
+#define catomic_compare_and_exchange_bool_acq(mem, newval, oldval) orig_catomic_compare_and_exchange_bool_acq(mem, newval, oldval)
+#define catomic_compare_and_exchange_bool_rel(mem, newval, oldval) orig_catomic_compare_and_exchange_bool_rel(mem, newval, oldval)
+#define catomic_exchange_and_add(mem, value) orig_catomic_exchange_and_add(mem, value)
+#define catomic_add(mem, value) orig_catomic_add(mem, value)
+#define catomic_increment(mem) orig_catomic_increment(mem)
+#define catomic_increment_val(mem) orig_catomic_increment_val(mem)
+#define catomic_decrement(mem) orig_catomic_decrement(mem)
+#define catomic_decrement_val(mem) orig_catomic_decrement_val(mem)
+#define catomic_and(mem, mask) orig_catomic_and(mem, mask)
+#define catomic_or(mem, mask) orig_catomic_or(mem, mask)
+#define catomic_max(mem, value) orig_catomic_max(mem, value)
+
+//
+// C11-style atomics (include/atomic.h)
+//
+#define atomic_load_relaxed(mem) orig_atomic_load_relaxed(mem)
+#define atomic_load_acquire(mem) orig_atomic_load_acquire(mem)
+#define atomic_store_relaxed(mem, val) orig_atomic_store_relaxed(mem, val)
+#define atomic_store_release(mem, val) orig_atomic_store_release(mem, val)
+#define atomic_compare_exchange_weak_relaxed(mem, expected, desired) orig_atomic_compare_exchange_weak_relaxed(mem, expected, desired)
+#define atomic_compare_exchange_weak_acquire(mem, expected, desired) orig_atomic_compare_exchange_weak_acquire(mem, expected, desired) 
+#define atomic_compare_exchange_weak_release(mem, expected, desired) orig_atomic_compare_exchange_weak_release(mem, expected, desired)
+#define atomic_exchange_relaxed(mem, desired) orig_atomic_exchange_relaxed(mem, desired)
+#define atomic_exchange_acquire(mem, desired) orig_atomic_exchange_acquire(mem, desired)
+#define atomic_exchange_release(mem, desired) orig_atomic_exchange_release(mem, desired)
+#define atomic_fetch_add_relaxed(mem, operand) orig_atomic_fetch_add_relaxed(mem, operand)
+#define atomic_fetch_add_acquire(mem, operand) orig_atomic_fetch_add_acquire(mem, operand)
+#define atomic_fetch_add_release(mem, operand) orig_atomic_fetch_add_release(mem, operand)
+#define atomic_fetch_add_acq_rel(mem, operand) orig_atomic_fetch_add_acq_rel(mem, operand)
+#define atomic_fetch_and_relaxed(mem, operand) orig_atomic_fetch_and_relaxed(mem, operand)
+#define atomic_fetch_and_acquire(mem, operand) orig_atomic_fetch_and_acquire(mem, operand)
+#define atomic_fetch_and_release(mem, operand) orig_atomic_fetch_and_release(mem, operand)
+#define atomic_fetch_or_relaxed(mem, operand) orig_atomic_fetch_or_relaxed(mem, operand) 
+#define atomic_fetch_or_acquire(mem, operand) orig_atomic_fetch_or_acquire(mem, operand) 
+#define atomic_fetch_or_release(mem, operand) orig_atomic_fetch_or_release(mem, operand) 
+#define atomic_fetch_xor_release(mem, operand) orig_atomic_fetch_xor_release(mem, operand) 
+
+//
+// TLS atomics (tls.h)
+//
+// stijn: ARM doesn't have these
+/*
+#define THREAD_ATOMIC_CMPXCHG_VAL(descr, member, newval, oldval) orig_THREAD_ATOMIC_CMPXCHG_VAL(descr, member, newval, oldval)
+#define THREAD_ATOMIC_AND(descr, member, val) orig_THREAD_ATOMIC_AND(descr, member, val)
+#define THREAD_ATOMIC_BIT_SET(descr, member, bit) orig_THREAD_ATOMIC_BIT_SET(descr, member, bit)
+*/
+
+//
+// MVEE additions
+//
+#define atomic_load(var) ({ var; })
+#define atomic_store(var, value) ({ var = value; })
+#define THREAD_ATOMIC_GETMEM(descr, member) THREAD_GETMEM(descr, member)
+#define THREAD_ATOMIC_SETMEM(descr, member, val) THREAD_SETMEM(descr, member, val)
+
+
+#else // !IS_IN_rtld
+
+//
+// architecture-specific atomics (atomic-machine.h)
+//
+#define __arch_c_compare_and_exchange_val_8_acq(mem, newval, oldval)	\
+	({																	\
+		typeof(*mem) ____result;										\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		____result = orig___arch_c_compare_and_exchange_val_8_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define __arch_c_compare_and_exchange_val_16_acq(mem, newval, oldval)	\
+	({																	\
+		typeof(*mem) ____result;										\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		____result = orig___arch_c_compare_and_exchange_val_16_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define __arch_c_compare_and_exchange_val_32_acq(mem, newval, oldval)	\
+	({																	\
+		typeof(*mem) ____result;										\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		____result = orig___arch_c_compare_and_exchange_val_32_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define __arch_c_compare_and_exchange_val_64_acq(mem, newval, oldval)	\
+	({																	\
+		typeof(*mem) ____result;										\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		____result = orig___arch_c_compare_and_exchange_val_64_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define atomic_compare_and_exchange_val_acq(mem, newval, oldval)		\
+	({																	\
+		typeof(*mem) ____result;										\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		____result = orig_atomic_compare_and_exchange_val_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define atomic_compare_and_exchange_val_rel(mem, newval, oldval)		\
+	({																	\
+		typeof(*mem) ____result;										\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		____result = orig_atomic_compare_and_exchange_val_rel(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define atomic_compare_and_exchange_bool_acq(mem, newval, oldval)		\
+	({																	\
+		bool ____result;												\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		____result = orig_atomic_compare_and_exchange_bool_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define atomic_compare_and_exchange_bool_rel(mem, newval, oldval)		\
+	({																	\
+		bool ____result;												\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		____result = orig_atomic_compare_and_exchange_bool_rel(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define atomic_exchange_acq(mem, newvalue)						\
+	({															\
+		typeof(*mem) ____result;								\
+		MVEE_PREOP(ATOMIC_EXCHANGE, mem, 1);					\
+		____result = orig_atomic_exchange_acq(mem, newvalue);	\
+		MVEE_POSTOP();											\
+		____result;												\
+	})
+
+#define atomic_exchange_rel(mem, newvalue)						\
+	({															\
+		typeof(*mem) ____result;								\
+		MVEE_PREOP(ATOMIC_EXCHANGE, mem, 1);					\
+		____result = orig_atomic_exchange_rel(mem, newvalue);	\
+		MVEE_POSTOP();											\
+		____result;												\
+	})
+
+#define atomic_exchange_and_add(mem, value)						\
+	({															\
+		typeof(*mem) ____result;								\
+		MVEE_PREOP(ATOMIC_EXCHANGE_AND_ADD, mem, 1);			\
+		____result = orig_atomic_exchange_and_add(mem, value);	\
+		MVEE_POSTOP();											\
+		____result;												\
+	})
+
+#define atomic_exchange_and_add_acq(mem, value)					\
+	({															\
+		typeof(*mem) ____result;								\
+		MVEE_PREOP(ATOMIC_EXCHANGE_AND_ADD, mem, 1);			\
+		____result = orig_atomic_exchange_and_add_acq(mem, value);	\
+		MVEE_POSTOP();											\
+		____result;												\
+	})
+
+#define atomic_exchange_and_add_rel(mem, value)					\
+	({															\
+		typeof(*mem) ____result;								\
+		MVEE_PREOP(ATOMIC_EXCHANGE_AND_ADD, mem, 1);			\
+		____result = orig_atomic_exchange_and_add_rel(mem, value);	\
+		MVEE_POSTOP();											\
+		____result;												\
+	})
+
+#define atomic_add(mem, value)					\
+	({											\
+		MVEE_PREOP(ATOMIC_ADD, mem, 1);			\
+		orig_atomic_add(mem, value);			\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_increment(mem)					\
+	({											\
+		MVEE_PREOP(ATOMIC_INCREMENT, mem, 1);	\
+		orig_atomic_increment(mem);				\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_increment_and_test(mem)						\
+	({														\
+		unsigned char ____result;							\
+		MVEE_PREOP(ATOMIC_INCREMENT_AND_TEST, mem, 1);		\
+		____result = orig_atomic_increment_and_test(mem);	\
+		MVEE_POSTOP();										\
+		____result;											\
+	})
+
+#define atomic_increment_val(mem)				\
+	({											\
+		typeof(*mem) ____result;				\
+		MVEE_PREOP(ATOMIC_INCREMENT, mem, 1);	\
+		____result = orig_atomic_increment_val(mem);	\
+		MVEE_POSTOP();							\
+		____result;								\
+	})
+
+#define atomic_decrement(mem)					\
+	({											\
+		MVEE_PREOP(ATOMIC_DECREMENT, mem, 1);	\
+		orig_atomic_decrement(mem);				\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_decrement_and_test(mem)						\
+	({														\
+		unsigned char ____result;							\
+		MVEE_PREOP(ATOMIC_DECREMENT_AND_TEST, mem, 1);		\
+		____result = orig_atomic_decrement_and_test(mem);	\
+		MVEE_POSTOP();										\
+		____result;											\
+	})
+
+#define atomic_decrement_val(mem)				\
+	({											\
+		typeof(*mem) ____result;				\
+		MVEE_PREOP(ATOMIC_DECREMENT, mem, 1);	\
+		____result = orig_atomic_decrement_val(mem);	\
+		MVEE_POSTOP();							\
+		____result;								\
+	})
+
+#define atomic_add_negative(mem, value)						\
+	({														\
+		unsigned char ____result;							\
+		MVEE_PREOP(ATOMIC_ADD, mem, 1);						\
+		____result = orig_atomic_add_negative(mem, value);	\
+		MVEE_POSTOP();										\
+		____result;											\
+	})
+
+#define atomic_add_zero(mem, value)						\
+	({													\
+		unsigned char ____result;						\
+		MVEE_PREOP(ATOMIC_ADD_ZERO, mem, 1);			\
+		____result = orig_atomic_add_zero(mem, value);	\
+		MVEE_POSTOP();									\
+		____result;										\
+	})
+
+#define atomic_bit_set(mem, bit)				\
+	({											\
+		MVEE_PREOP(ATOMIC_BIT_SET, mem, 1);		\
+		orig_atomic_bit_set(mem, bit);			\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_bit_test_set(mem, bit)						\
+	({														\
+		unsigned char ____result;							\
+		MVEE_PREOP(ATOMIC_BIT_TEST_SET, mem, 1);			\
+		____result = orig_atomic_bit_test_set(mem, bit);	\
+		MVEE_POSTOP();										\
+		____result;											\
+	})
+
+#define atomic_and(mem, mask)					\
+	({											\
+		MVEE_PREOP(ATOMIC_AND, mem, 1);			\
+		orig_atomic_and(mem, mask);				\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_or(mem, mask)					\
+	({											\
+		MVEE_PREOP(ATOMIC_OR, mem, 1);			\
+		orig_atomic_or(mem, mask);				\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_max(mem, value)					\
+	({											\
+		MVEE_PREOP(ATOMIC_MAX, mem, 1);			\
+		orig_atomic_max(mem, value);			\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_min(mem, value)					\
+	({											\
+		MVEE_PREOP(ATOMIC_MIN, mem, 1);			\
+		orig_atomic_max(mem, value);			\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_decrement_if_positive(mem)					\
+	({														\
+		__typeof(*mem) __result;							\
+		MVEE_PREOP(ATOMIC_DECREMENT_IF_POSITIVE, mem, 1);	\
+		__result = orig_atomic_decrement_if_positive(mem);	\
+		MVEE_POSTOP();										\
+		__result;											\
+	})
+
+#define atomic_and_val(mem, mask)							\
+	({														\
+		__typeof(*mem) __result;							\
+		MVEE_PREOP(ATOMIC_AND_VAL, mem, 1);					\
+		__result = orig_atomic_and_val(mem);				\
+		MVEE_POSTOP();										\
+		__result;											\
+	})
+
+#define atomic_or_val(mem, mask)							\
+	({														\
+		__typeof(*mem) __result;							\
+		MVEE_PREOP(ATOMIC_OR_VAL, mem, 1);					\
+		__result = orig_atomic_or_val(mem);					\
+		MVEE_POSTOP();										\
+		__result;											\
+	})
+
+#define atomic_forced_read(x)						\
+	({												\
+		typeof(x) ____result;						\
+		MVEE_PREOP(ATOMIC_FORCED_READ, &x, 0);		\
+		____result = orig_atomic_forced_read(x);	\
+		MVEE_POSTOP();								\
+		____result;									\
+	})
+
+#define catomic_compare_and_exchange_val_acq(mem, newval, oldval)		\
+	({																	\
+		typeof(*mem) ____result;										\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		____result = orig_catomic_compare_and_exchange_val_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define catomic_compare_and_exchange_val_rel(mem, newval, oldval)		\
+	({																	\
+		typeof(*mem) ____result;										\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		____result = orig_catomic_compare_and_exchange_val_rel(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define catomic_compare_and_exchange_bool_acq(mem, newval, oldval)		\
+	({																	\
+		bool ____result;												\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		____result = orig_catomic_compare_and_exchange_bool_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define catomic_compare_and_exchange_bool_rel(mem, newval, oldval)		\
+	({																	\
+		bool ____result;												\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		____result = orig_catomic_compare_and_exchange_bool_rel(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define catomic_exchange_and_add(mem, value)					\
+	({															\
+		typeof(*mem) ____result;								\
+		MVEE_PREOP(CATOMIC_EXCHANGE_AND_ADD, mem, 1);			\
+		____result = orig_catomic_exchange_and_add(mem, value);	\
+		MVEE_POSTOP();											\
+		____result;												\
+	})
+
+#define catomic_add(mem, value)					\
+	({											\
+		MVEE_PREOP(CATOMIC_ADD, mem, 1);		\
+		orig_catomic_add(mem, value);			\
+		MVEE_POSTOP();							\
+	})
+
+#define catomic_increment(mem)					\
+	({											\
+		MVEE_PREOP(CATOMIC_INCREMENT, mem, 1);	\
+		orig_catomic_increment(mem);			\
+		MVEE_POSTOP();							\
+	})
+
+#define catomic_increment_val(mem)						\
+	({													\
+		typeof(*mem) ____result;						\
+		MVEE_PREOP(CATOMIC_INCREMENT, mem, 1);			\
+		____result = orig_catomic_increment_val(mem);	\
+		MVEE_POSTOP();									\
+		____result;										\
+	})
+
+#define catomic_decrement(mem)					\
+	({											\
+		MVEE_PREOP(CATOMIC_DECREMENT, mem, 1);	\
+		orig_catomic_decrement(mem);			\
+		MVEE_POSTOP();							\
+	})
+
+#define catomic_decrement_val(mem)						\
+	({													\
+		typeof(*mem) ____result;						\
+		MVEE_PREOP(CATOMIC_DECREMENT, mem, 1);			\
+		____result = orig_catomic_decrement_val(mem);	\
+		MVEE_POSTOP();									\
+		____result;										\
+	})
+
+
+#define catomic_and(mem, mask)					\
+	({											\
+		MVEE_PREOP(CATOMIC_AND, mem, 1);		\
+		orig_catomic_and(mem, mask);			\
+		MVEE_POSTOP();							\
+	})
+
+#define catomic_or(mem, mask)					\
+	({											\
+		MVEE_PREOP(CATOMIC_OR, mem, 1);			\
+		orig_catomic_or(mem, mask);				\
+		MVEE_POSTOP();							\
+	})
+
+#define catomic_max(mem, value)					\
+	({											\
+		MVEE_PREOP(CATOMIC_MAX, mem, 1);		\
+		orig_catomic_max(mem, value);			\
+		MVEE_POSTOP();							\
+	})
+
+
+//
+// generic C11-style atomics (include/atomic.h)
+//
+#define atomic_load_relaxed(mem)					\
+	({												\
+		__typeof(*mem) ____result;					\
+		MVEE_PREOP(ATOMIC_LOAD, mem, 0);			\
+		____result = orig_atomic_load_relaxed(mem);	\
+		MVEE_POSTOP();								\
+		____result;									\
+	})
+
+#define atomic_load_acquire(mem)					\
+	({												\
+		__typeof(*mem) ____result;					\
+		MVEE_PREOP(ATOMIC_LOAD, mem, 0);			\
+		____result = orig_atomic_load_acquire(mem);	\
+		MVEE_POSTOP();								\
+		____result;									\
+	})
+
+#define atomic_store_relaxed(mem, val)			\
+	(void)({									\
+		MVEE_PREOP(ATOMIC_STORE, mem, 1);		\
+		orig_atomic_store_relaxed(mem, val);	\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_store_release(mem, val)			\
+	(void)({									\
+		MVEE_PREOP(ATOMIC_STORE, mem, 1);		\
+		orig_atomic_store_release(mem, val);	\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_compare_exchange_weak_relaxed(mem, expected, desired)	\
+	({																	\
+		bool __result;													\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		__result = orig_atomic_compare_exchange_weak_relaxed(mem, expected, desired); \
+		MVEE_POSTOP();													\
+		__result;														\
+	})
+
+#define atomic_compare_exchange_weak_acquire(mem, expected, desired)	\
+	({																	\
+		bool __result;													\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		__result = orig_atomic_compare_exchange_weak_acquire(mem, expected, desired); \
+		MVEE_POSTOP();													\
+		__result;														\
+	})
+
+#define atomic_compare_exchange_weak_release(mem, expected, desired)	\
+	({																	\
+		bool __result;													\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		__result = orig_atomic_compare_exchange_weak_release(mem, expected, desired); \
+		MVEE_POSTOP();													\
+		__result;														\
+	})
+
+#define atomic_exchange_relaxed(mem, desired)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_EXCHANGE, mem, 1);						\
+		____result = orig_atomic_exchange_relaxed(mem, desired);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_exchange_acquire(mem, desired)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_EXCHANGE, mem, 1);						\
+		____result = orig_atomic_exchange_acquire(mem, desired);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_exchange_release(mem, desired)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_EXCHANGE, mem, 1);						\
+		____result = orig_atomic_exchange_release(mem, desired);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_add_relaxed(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_ADD, mem, 1);						\
+		____result = orig_atomic_fetch_add_relaxed(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_add_acquire(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_ADD, mem, 1);						\
+		____result = orig_atomic_fetch_add_acquire(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_add_release(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_ADD, mem, 1);						\
+		____result = orig_atomic_fetch_add_release(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_add_acq_rel(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_ADD, mem, 1);						\
+		____result = orig_atomic_fetch_add_acq_rel(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_and_relaxed(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_AND, mem, 1);						\
+		____result = orig_atomic_fetch_and_relaxed(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_and_acquire(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_AND, mem, 1);						\
+		____result = orig_atomic_fetch_and_acquire(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_and_release(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_AND, mem, 1);						\
+		____result = orig_atomic_fetch_and_release(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+
+#define atomic_fetch_or_relaxed(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_OR, mem, 1);						\
+		____result = orig_atomic_fetch_or_relaxed(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_or_acquire(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_OR, mem, 1);						\
+		____result = orig_atomic_fetch_or_acquire(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_or_release(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_OR, mem, 1);						\
+		____result = orig_atomic_fetch_or_release(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_xor_release(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_XOR, mem, 1);						\
+		____result = orig_atomic_fetch_xor_release(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+//
+// TLS atomics (tls.h)
+//
+// stijn: ARM doesn't have these
+/*
+#define THREAD_ATOMIC_CMPXCHG_VAL(descr, member, newval, oldval)		\
+	({																	\
+		__typeof(descr->member) ____result;								\
+		MVEE_PREOP(__THREAD_ATOMIC_CMPXCHG_VAL, &descr->member, 1);		\
+		____result = orig_THREAD_ATOMIC_CMPXCHG_VAL(descr, member, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+
+#define THREAD_ATOMIC_AND(descr, member, val)					\
+	(void)({													\
+			MVEE_PREOP(__THREAD_ATOMIC_AND, &descr->member, 1);	\
+			orig_THREAD_ATOMIC_AND(descr, member, val);			\
+			MVEE_POSTOP();										\
+		})
+
+
+#define THREAD_ATOMIC_BIT_SET(descr, member, bit)					\
+	(void)({														\
+			MVEE_PREOP(__THREAD_ATOMIC_BIT_SET, &descr->member, 1);	\
+			orig_THREAD_ATOMIC_BIT_SET(descr, member, bit);			\
+			MVEE_POSTOP();											\
+		})
+*/
+
+//
+// MVEE additions
+//
+#define atomic_load(var)						\
+	({											\
+		__typeof(var+0) ____result;				\
+		MVEE_PREOP(ATOMIC_LOAD, (void*)&var, 0);	\
+		____result = var;						\
+		MVEE_POSTOP();							\
+		____result;								\
+	})
+
+#define atomic_store(var, val)					\
+	(void)({									\
+		MVEE_PREOP(ATOMIC_STORE, &var, 1);		\
+		var = val;								\
+		MVEE_POSTOP();							\
+	})
+
+#define THREAD_ATOMIC_GETMEM(descr, member)			\
+	({												\
+		__typeof(descr->member) ____result;			\
+		MVEE_PREOP(ATOMIC_LOAD, &descr->member, 1);	\
+		____result = THREAD_GETMEM(descr, member);	\
+		MVEE_POSTOP();								\
+		____result;									\
+	})
+
+#define THREAD_ATOMIC_SETMEM(descr, member, val)			\
+	(void)({												\
+			MVEE_PREOP(ATOMIC_STORE, &descr->member, 1);	\
+			THREAD_SETMEM(descr, member, val);				\
+			MVEE_POSTOP();									\
+		})
+
+//
+// sys_futex with FUTEX_WAKE_OP usually overwrites the value of the futex.
+// We have to make sure that we include the futex write in our sync buf ordering
+//
+#define lll_futex_wake_unlock(futexp, nr_wake, nr_wake2, futexp2, private) \
+	({																	\
+		INTERNAL_SYSCALL_DECL (__err);									\
+		long int __ret;													\
+		MVEE_PREOP(___UNKNOWN_LOCK_TYPE___, futexp2, 1);				\
+		__ret = INTERNAL_SYSCALL (futex, __err, 6, (futexp),			\
+								  __lll_private_flag (FUTEX_WAKE_OP, private), \
+								  (nr_wake), (nr_wake2), (futexp2),		\
+								  FUTEX_OP_CLEAR_WAKE_IF_GT_ONE);		\
+		if (mvee_should_futex_unlock())									\
+		{																\
+			*futexp2 = 0;												\
+		}																\
+		MVEE_POSTOP();													\
+		INTERNAL_SYSCALL_ERROR_P (__ret, __err);						\
+	})
+
+#define arch_cpu_relax() __asm__ __volatile__("mov\tr0,r0\t@ nop\n\t");
+
+#endif // !IS_IN (rtld)
diff -N -r --unified glibc-2.25.orig/sysdeps/arm/mvee-totalpartial-agent.h glibc-2.25/sysdeps/arm/mvee-totalpartial-agent.h
--- glibc-2.25.orig/sysdeps/arm/mvee-totalpartial-agent.h	1970-01-01 00:00:00.000000000 +0000
+++ glibc-2.25/sysdeps/arm/mvee-totalpartial-agent.h	2017-09-13 19:55:55.983745398 +0000
@@ -0,0 +1,134 @@
+//
+// MVEE_PARTIAL_ORDER_REPLICATION: when defined, slaves will use
+// queue projection to replay synchronization operations in
+// partial order rather than total order. In other words,
+// the slaves will only respect the order in which the master
+// has performed its synchronization operations on a per-word
+// basis
+//
+#define MVEE_PARTIAL_ORDER_REPLICATION
+//
+// MVEE_EXTENDED_QUEUE: when defined, the locking operation and
+// mutex pointer are also logged in the queue.
+//
+#define MVEE_EXTENDED_QUEUE
+//
+// MVEE_LOG_EIPS: when defined, libc logs return addresses for
+// all locking operations into a seperate queue
+//
+// WARNING: enabling EIP logging _CAN_ trigger crashes! We're
+// using __builtin_return_address(2) to fetch the eip of the 
+// caller of the locking function. Unfortunately, libc uses inline
+// __libc_lock_* operations every now and then. When it does, 
+// the __builtin_... call will return the wrong caller and in some
+// cases (e.g. in do_system) it might try to fetch the eip beyond
+// the end of the stack!
+//
+#define MVEE_LOG_EIPS
+#define MVEE_STACK_DEPTH 5
+//
+// MVEE_CHECK_LOCK_TYPE: if this is defined, the slave will check
+// whether or not it's replaying a lock of the same type
+// (only works with the extended queue)
+//
+#define MVEE_CHECK_LOCK_TYPE
+//
+// MVEE_DEBUG_MALLOC: if this is defined, the slaves will check whether
+// their malloc behavior is synced with the master
+//
+// #define MVEE_DEBUG_MALLOC
+//
+// MVEE_MALLOC_IGNORE_ASLR: if this is defined, the malloc debugger will
+// only compare allocation types, messages and chunk sizes
+// arena and chunk pointers are ignored.
+// #define MVEE_MALLOC_IGNORE_ASLR
+
+#define DEFINE_MVEE_QUEUE(name, has_eip_queue)				\
+  static unsigned long             mvee_##name##_buffer_data_start  = 0; \
+  static volatile unsigned int*    mvee_##name##_buffer_pos         = 0; \
+  static volatile unsigned int*    mvee_##name##_buffer_lock        = 0; \
+  static volatile unsigned int*    mvee_##name##_buffer_flush_cnt   = 0; \
+  static volatile unsigned char*   mvee_##name##_buffer_flushing    = 0; \
+  static unsigned long             mvee_##name##_buffer_slots       = 0; \
+  static void*                     mvee_##name##_eip_buffer         = NULL; \
+  static unsigned char             mvee_##name##_buffer_log_eips    = has_eip_queue;
+
+// this is extremely wasteful but required to prevent false sharing in the producer...
+#ifdef MVEE_PARTIAL_ORDER_REPLICATION
+# ifdef MVEE_EXTENDED_QUEUE
+      #define MVEE_LOCK_QUEUE_SLOT_SIZE (sizeof(long) + sizeof(short) * (mvee_num_childs + 1))
+# else
+      #define MVEE_LOCK_QUEUE_SLOT_SIZE (sizeof(long) + sizeof(short) + (mvee_num_childs - 1))
+# endif
+#else
+# ifdef MVEE_EXTENDED_QUEUE
+      #define MVEE_LOCK_QUEUE_SLOT_SIZE (3*sizeof(long))
+# else
+      #define MVEE_LOCK_QUEUE_SLOT_SIZE sizeof(short)
+# endif
+#endif
+#define mvee_lock_buffer_slot_size 64
+#define mvee_malloc_buffer_slot_size 64
+
+// In the new queue layout, we want each replica's lock, position, 
+// flush_cnt and flushing word on one and the same cache line
+// Therefore, we round up the buffer ptr to a multiple of 64 for the master replica.
+// Each subsequent replica has its variables aligned on the next cache line boundary
+
+#define INIT_MVEE_QUEUE(name, slot_size, queue_ident)			\
+  if (!mvee_##name##_buffer_data_start)					\
+    {									\
+      long tmp_id = syscall(MVEE_GET_SHARED_BUFFER, 0, queue_ident, &mvee_##name##_buffer_slots, MVEE_LOCK_QUEUE_SLOT_SIZE); \
+      mvee_##name##_buffer_slots      = (mvee_##name##_buffer_slots - mvee_num_childs * 64) / mvee_##name##_buffer_slot_size - 2; \
+      void* tmp_buffer                = (void*)syscall(__NR_shmat, tmp_id, NULL, 0); \
+      mvee_##name##_buffer_lock       = (volatile unsigned int*)  (MVEE_ROUND_UP((unsigned long)tmp_buffer, 64) + mvee_child_num * 64); \
+      mvee_##name##_buffer_pos        = (volatile unsigned int*)  (MVEE_ROUND_UP((unsigned long)tmp_buffer, 64) + mvee_child_num * 64 + sizeof(int)); \
+      mvee_##name##_buffer_flush_cnt  = (volatile unsigned int*)  (MVEE_ROUND_UP((unsigned long)tmp_buffer, 64) + mvee_child_num * 64 + sizeof(int) * 2); \
+      mvee_##name##_buffer_flushing   = (volatile unsigned char*) (MVEE_ROUND_UP((unsigned long)tmp_buffer, 64) + mvee_child_num * 64 + sizeof(int) * 3); \
+     *mvee_##name##_buffer_lock       = 1; \
+      mvee_##name##_buffer_data_start = MVEE_ROUND_UP((unsigned long)tmp_buffer, 64) + mvee_num_childs * 64; \
+      if (mvee_##name##_buffer_log_eips)				\
+	{								\
+	  long eip_buffer_id = syscall(MVEE_GET_SHARED_BUFFER, 1,	\
+				      queue_ident, NULL, mvee_num_childs * sizeof(long) * MVEE_STACK_DEPTH, MVEE_STACK_DEPTH); \
+	  mvee_##name##_eip_buffer = (void*)syscall(__NR_shmat, eip_buffer_id, NULL, 0); \
+	}								\
+    }									
+
+#define MVEE_LOG_QUEUE_DATA(name, pos, offset, data)			\
+  *(typeof(data)*)(mvee_##name##_buffer_data_start + mvee_##name##_buffer_slot_size * (pos) + offset) = data;
+
+#define MVEE_READ_QUEUE_DATA(name, pos, offset, result)			\
+  result = *(typeof(result)*)(mvee_##name##_buffer_data_start + mvee_##name##_buffer_slot_size * pos + offset);
+
+#define MVEE_LOG_STACK(name, start_depth, pos)				\
+  mvee_log_stack(mvee_##name##_eip_buffer, sizeof(long) * mvee_num_childs * MVEE_STACK_DEPTH, pos, start_depth);
+
+#ifdef MVEE_DEBUG_MALLOC
+extern void mvee_malloc_hook(int alloc_type, int msg, long chunksize, void* ar_ptr, void* chunk_ptr);
+#define MVEE_MALLOC_HOOK(type, msg, sz, ar_ptr, chunk_ptr)		\
+  mvee_malloc_hook(type, msg, sz, ar_ptr, chunk_ptr)
+#else
+#define MVEE_MALLOC_HOOK(type, msg, sz, ar_ptr, chunk_ptr)
+#endif // !MVEE_DEBUG_MALLOC
+
+extern void mvee_invalidate_buffer      (void);
+extern void mvee_atomic_postop_internal (unsigned char preop_result);
+extern int  mvee_should_sync_tid        (void);
+extern int  mvee_all_heaps_aligned      (char* heap, unsigned long alloc_size); 
+extern void mvee_malloc_hook(int alloc_type, int msg, long chunksize, void* ar_ptr, void* chunk_ptr);
+extern void mvee_write_malloc_info(int alloc_type, int msg, long chunksize, void* ar_ptr, void* chunk_ptr);
+extern void mvee_verify_malloc_info(int alloc_type, int msg, long chunksize, void* ar_ptr, void* chunk_ptr);
+
+#define MVEE_POSTOP() \
+  mvee_atomic_postop_internal(__tmp_mvee_preop);
+
+#ifdef MVEE_EXTENDED_QUEUE
+ extern unsigned char     mvee_atomic_preop_internal             (unsigned char is_store, void* word_ptr, unsigned short op_type);
+# define MVEE_PREOP(op_type, mem, is_store)					\
+	register unsigned char __tmp_mvee_preop = mvee_atomic_preop_internal(is_store, (void*)mem, op_type);
+#else
+ extern unsigned char     mvee_atomic_preop_internal            (unsigned char is_store, void* word_ptr);
+# define MVEE_PREOP(op_type, mem, is_store) \
+	register unsigned char __tmp_mvee_preop = mvee_atomic_preop_internal(is_store, (void*)mem);
+#endif // !MVEE_EXTENDED_QUEUE
diff -N -r --unified glibc-2.25.orig/sysdeps/arm/mvee-woc-agent.h glibc-2.25/sysdeps/arm/mvee-woc-agent.h
--- glibc-2.25.orig/sysdeps/arm/mvee-woc-agent.h	1970-01-01 00:00:00.000000000 +0000
+++ glibc-2.25/sysdeps/arm/mvee-woc-agent.h	2017-09-13 19:55:55.985745541 +0000
@@ -0,0 +1,16 @@
+#define MVEE_MAX_COUNTERS 65536
+
+#define MVEE_MALLOC_HOOK(type, msg, sz, ar_ptr, chunk_ptr)
+
+extern void          mvee_atomic_postop_internal (unsigned char preop_result);
+extern unsigned char mvee_atomic_preop_internal  (volatile void* word_ptr);
+extern int           mvee_should_sync_tid        (void);
+extern int           mvee_all_heaps_aligned      (char* heap, unsigned long alloc_size); 
+extern void          mvee_invalidate_buffer      (void);
+extern unsigned char mvee_should_futex_unlock    (void);
+
+#define MVEE_POSTOP()								\
+	mvee_atomic_postop_internal(__tmp_mvee_preop);
+
+#define MVEE_PREOP(op_type, mem, is_store)								\
+	register unsigned char  __tmp_mvee_preop = mvee_atomic_preop_internal(mem);
diff -N -r --unified glibc-2.25.orig/sysdeps/nptl/fork.c glibc-2.25/sysdeps/nptl/fork.c
--- glibc-2.25.orig/sysdeps/nptl/fork.c	2017-09-11 17:15:12.163419229 +0000
+++ glibc-2.25/sysdeps/nptl/fork.c	2017-09-13 19:55:55.988745755 +0000
@@ -63,12 +63,12 @@
   /* Run all the registered preparation handlers.  In reverse order.
      While doing this we build up a list of all the entries.  */
   struct fork_handler *runp;
-  while ((runp = __fork_handlers) != NULL)
+  while ((runp = atomic_load(__fork_handlers)) != NULL)
     {
       /* Make sure we read from the current RUNP pointer.  */
       atomic_full_barrier ();
 
-      unsigned int oldval = runp->refcntr;
+      unsigned int oldval = atomic_load(runp->refcntr);
 
       if (oldval == 0)
 	/* This means some other thread removed the list just after
@@ -103,7 +103,7 @@
 	  allp = newp;
 
 	  /* Advance to the next handler.  */
-	  runp = runp->next;
+	  runp = atomic_load(runp->next);
 	  if (runp == NULL)
 	    break;
 
@@ -214,7 +214,7 @@
 	     may have been bumped up by other threads doing a fork.
 	     We reset it to 1, to avoid waiting for non-existing
 	     thread(s) to release the count.  */
-	  allp->handler->refcntr = 1;
+	  atomic_store(allp->handler->refcntr, 1);
 
 	  /* XXX We could at this point look through the object pool
 	     and mark all objects not on the __fork_handlers list as
@@ -222,7 +222,7 @@
 	     while another thread called dlclose() and that call had
 	     to create a new list.  */
 
-	  allp = allp->next;
+	  allp = atomic_load(allp->next);
 	}
 
       /* Initialize the fork lock.  */
@@ -252,7 +252,7 @@
 	      && allp->handler->need_signal)
 	    futex_wake (&allp->handler->refcntr, 1, FUTEX_PRIVATE);
 
-	  allp = allp->next;
+	  allp = atomic_load(allp->next);
 	}
     }
 
diff -N -r --unified glibc-2.25.orig/sysdeps/nptl/lowlevellock.h glibc-2.25/sysdeps/nptl/lowlevellock.h
--- glibc-2.25.orig/sysdeps/nptl/lowlevellock.h	2017-09-11 17:15:12.166419443 +0000
+++ glibc-2.25/sysdeps/nptl/lowlevellock.h	2017-09-13 19:55:55.991745970 +0000
@@ -184,8 +184,10 @@
 #define lll_wait_tid(tid) \
   do {					\
     __typeof (tid) __tid;		\
-    while ((__tid = (tid)) != 0)	\
-      lll_futex_wait (&(tid), __tid, LLL_SHARED);\
+    while ((__tid = (tid)) != 0 || mvee_should_sync_tid()) {	\
+		lll_futex_syscall(4, &(tid), __lll_private_flag(mvee_should_sync_tid() ? MVEE_FUTEX_WAIT_TID : FUTEX_WAIT, LLL_SHARED), __tid, NULL); \
+				   if (tid == 0) break; \
+	}\
   } while (0)
 
 extern int __lll_timedwait_tid (int *, const struct timespec *)
@@ -196,7 +198,7 @@
 #define lll_timedwait_tid(tid, abstime) \
   ({							\
     int __res = 0;					\
-    if ((tid) != 0)					\
+    if ((tid) != 0 || mvee_should_sync_tid())			\
       __res = __lll_timedwait_tid (&(tid), (abstime));	\
     __res;						\
   })
diff -N -r --unified glibc-2.25.orig/sysdeps/unix/sysv/linux/check_pf.c glibc-2.25/sysdeps/unix/sysv/linux/check_pf.c
--- glibc-2.25.orig/sysdeps/unix/sysv/linux/check_pf.c	2017-09-11 17:15:13.642525033 +0000
+++ glibc-2.25/sysdeps/unix/sysv/linux/check_pf.c	2017-09-13 19:55:55.994746184 +0000
@@ -269,7 +269,7 @@
   if (seen_ipv6 && result != NULL)
     {
       result->timestamp = get_nl_timestamp ();
-      result->usecnt = 2;
+      atomic_store(result->usecnt, 2);
       result->seen_ipv4 = seen_ipv4;
       result->seen_ipv6 = true;
       result->in6ailen = result_len;
@@ -348,7 +348,7 @@
       *in6ailen = data->in6ailen;
       *in6ai = data->in6ai;
 
-      if (olddata != NULL && olddata->usecnt > 0
+      if (olddata != NULL && atomic_load(olddata->usecnt) > 0
 	  && atomic_add_zero (&olddata->usecnt, -1))
 	free (olddata);
 
@@ -381,7 +381,7 @@
 	{
 	  __libc_lock_lock (lock);
 
-	  if (data->usecnt == 0)
+	  if (atomic_load(data->usecnt) == 0)
 	    /* Still unused.  */
 	    free (data);
 
diff -N -r --unified glibc-2.25.orig/sysdeps/unix/sysv/linux/lowlevellock-futex.h glibc-2.25/sysdeps/unix/sysv/linux/lowlevellock-futex.h
--- glibc-2.25.orig/sysdeps/unix/sysv/linux/lowlevellock-futex.h	2017-09-11 17:15:13.937546137 +0000
+++ glibc-2.25/sysdeps/unix/sysv/linux/lowlevellock-futex.h	2017-09-13 19:55:55.997746398 +0000
@@ -115,7 +115,7 @@
 		     nr_wake, nr_move, mutex, val)
 
 /* Returns non-zero if error happened, zero if success.  */
-#define lll_futex_wake_unlock(futexp, nr_wake, nr_wake2, futexp2, private) \
+#define orig_lll_futex_wake_unlock(futexp, nr_wake, nr_wake2, futexp2, private) \
   lll_futex_syscall (6, futexp,                                         \
 		     __lll_private_flag (FUTEX_WAKE_OP, private),       \
 		     nr_wake, nr_wake2, futexp2,                        \
diff -N -r --unified glibc-2.25.orig/sysdeps/unix/sysv/linux/x86/elision-lock.c glibc-2.25/sysdeps/unix/sysv/linux/x86/elision-lock.c
--- glibc-2.25.orig/sysdeps/unix/sysv/linux/x86/elision-lock.c	2017-09-11 17:15:15.722673832 +0000
+++ glibc-2.25/sysdeps/unix/sysv/linux/x86/elision-lock.c	2017-09-13 19:55:55.999746541 +0000
@@ -99,7 +99,7 @@
       /* Use a normal lock until the threshold counter runs out.
 	 Lost updates possible.  */
       atomic_store_relaxed (adapt_count,
-	  atomic_load_relaxed (adapt_count) - 1);
+	  orig_atomic_load_relaxed (adapt_count) - 1);
     }
 
   /* Use a normal lock here.  */
diff -N -r --unified glibc-2.25.orig/sysdeps/unix/sysv/linux/x86/elision-trylock.c glibc-2.25/sysdeps/unix/sysv/linux/x86/elision-trylock.c
--- glibc-2.25.orig/sysdeps/unix/sysv/linux/x86/elision-trylock.c	2017-09-11 17:15:15.723673904 +0000
+++ glibc-2.25/sysdeps/unix/sysv/linux/x86/elision-trylock.c	2017-09-13 19:55:56.001746684 +0000
@@ -68,7 +68,7 @@
     {
       /* Lost updates are possible but harmless (see above).  */
       atomic_store_relaxed (adapt_count,
-	  atomic_load_relaxed (adapt_count) - 1);
+	  orig_atomic_load_relaxed (adapt_count) - 1);
     }
 
   return lll_trylock (*futex);
diff -N -r --unified glibc-2.25.orig/sysdeps/unix/sysv/linux/x86/elision-unlock.c glibc-2.25/sysdeps/unix/sysv/linux/x86/elision-unlock.c
--- glibc-2.25.orig/sysdeps/unix/sysv/linux/x86/elision-unlock.c	2017-09-11 17:15:15.724673975 +0000
+++ glibc-2.25/sysdeps/unix/sysv/linux/x86/elision-unlock.c	2017-09-13 19:55:56.003746827 +0000
@@ -25,7 +25,7 @@
 {
   /* When the lock was free we're in a transaction.
      When you crash here you unlocked a free lock.  */
-  if (*lock == 0)
+	if (atomic_load(*lock) == 0)
     _xend();
   else
     lll_unlock ((*lock), private);
diff -N -r --unified glibc-2.25.orig/sysdeps/unix/sysv/linux/x86_64/cancellation.S glibc-2.25/sysdeps/unix/sysv/linux/x86_64/cancellation.S
--- glibc-2.25.orig/sysdeps/unix/sysv/linux/x86_64/cancellation.S	2017-09-11 17:15:15.746675549 +0000
+++ glibc-2.25/sysdeps/unix/sysv/linux/x86_64/cancellation.S	1970-01-01 00:00:00.000000000 +0000
@@ -1,115 +0,0 @@
-/* Copyright (C) 2009-2017 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-   Contributed by Ulrich Drepper <drepper@redhat.com>, 2009.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, see
-   <http://www.gnu.org/licenses/>.  */
-
-#include <sysdep.h>
-#include <tcb-offsets.h>
-#include <kernel-features.h>
-#include "lowlevellock.h"
-
-#define PTHREAD_UNWIND JUMPTARGET(__pthread_unwind)
-#if IS_IN (libpthread)
-# if defined SHARED && !defined NO_HIDDEN
-#  undef PTHREAD_UNWIND
-#  define PTHREAD_UNWIND __GI___pthread_unwind
-# endif
-#else
-# ifndef SHARED
-	.weak __pthread_unwind
-# endif
-#endif
-
-
-#ifdef __ASSUME_PRIVATE_FUTEX
-# define LOAD_PRIVATE_FUTEX_WAIT(reg) \
-	movl	$(FUTEX_WAIT | FUTEX_PRIVATE_FLAG), reg
-#else
-# if FUTEX_WAIT == 0
-#  define LOAD_PRIVATE_FUTEX_WAIT(reg) \
-	movl	%fs:PRIVATE_FUTEX, reg
-# else
-#  define LOAD_PRIVATE_FUTEX_WAIT(reg) \
-	movl	%fs:PRIVATE_FUTEX, reg ; \
-	orl	$FUTEX_WAIT, reg
-# endif
-#endif
-
-/* It is crucial that the functions in this file don't modify registers
-   other than %rax and %r11.  The syscall wrapper code depends on this
-   because it doesn't explicitly save the other registers which hold
-   relevant values.  */
-	.text
-
-	.hidden __pthread_enable_asynccancel
-ENTRY(__pthread_enable_asynccancel)
-	movl	%fs:CANCELHANDLING, %eax
-2:	movl	%eax, %r11d
-	orl	$TCB_CANCELTYPE_BITMASK, %r11d
-	cmpl	%eax, %r11d
-	je	1f
-
-	lock
-	cmpxchgl %r11d, %fs:CANCELHANDLING
-	jnz	2b
-
-	andl	$(TCB_CANCELSTATE_BITMASK|TCB_CANCELTYPE_BITMASK|TCB_CANCELED_BITMASK|TCB_EXITING_BITMASK|TCB_CANCEL_RESTMASK|TCB_TERMINATED_BITMASK), %r11d
-	cmpl	$(TCB_CANCELTYPE_BITMASK|TCB_CANCELED_BITMASK), %r11d
-	je	3f
-
-1:	ret
-
-3:	subq	$8, %rsp
-	cfi_adjust_cfa_offset(8)
-	LP_OP(mov) $TCB_PTHREAD_CANCELED, %fs:RESULT
-	lock
-	orl	$TCB_EXITING_BITMASK, %fs:CANCELHANDLING
-	mov	%fs:CLEANUP_JMP_BUF, %RDI_LP
-	call	PTHREAD_UNWIND
-	hlt
-END(__pthread_enable_asynccancel)
-
-
-	.hidden __pthread_disable_asynccancel
-ENTRY(__pthread_disable_asynccancel)
-	testl	$TCB_CANCELTYPE_BITMASK, %edi
-	jnz	1f
-
-	movl	%fs:CANCELHANDLING, %eax
-2:	movl	%eax, %r11d
-	andl	$~TCB_CANCELTYPE_BITMASK, %r11d
-	lock
-	cmpxchgl %r11d, %fs:CANCELHANDLING
-	jnz	2b
-
-	movl	%r11d, %eax
-3:	andl	$(TCB_CANCELING_BITMASK|TCB_CANCELED_BITMASK), %eax
-	cmpl	$TCB_CANCELING_BITMASK, %eax
-	je	4f
-1:	ret
-
-	/* Performance doesn't matter in this loop.  We will
-	   delay until the thread is canceled.  And we will unlikely
-	   enter the loop twice.  */
-4:	mov	%fs:0, %RDI_LP
-	movl	$__NR_futex, %eax
-	xorq	%r10, %r10
-	addq	$CANCELHANDLING, %rdi
-	LOAD_PRIVATE_FUTEX_WAIT (%esi)
-	syscall
-	movl	%fs:CANCELHANDLING, %eax
-	jmp	3b
-END(__pthread_disable_asynccancel)
diff -N -r --unified glibc-2.25.orig/sysdeps/unix/sysv/linux/x86_64/libc-cancellation.S glibc-2.25/sysdeps/unix/sysv/linux/x86_64/libc-cancellation.S
--- glibc-2.25.orig/sysdeps/unix/sysv/linux/x86_64/libc-cancellation.S	2017-09-11 17:15:15.753676050 +0000
+++ glibc-2.25/sysdeps/unix/sysv/linux/x86_64/libc-cancellation.S	1970-01-01 00:00:00.000000000 +0000
@@ -1,21 +0,0 @@
-/* Copyright (C) 2009-2017 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-   Contributed by Ulrich Drepper <drepper@redhat.com>, 2009.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, see
-   <http://www.gnu.org/licenses/>.  */
-
-#define __pthread_enable_asynccancel __libc_enable_asynccancel
-#define __pthread_disable_asynccancel __libc_disable_asynccancel
-#include "cancellation.S"
diff -N -r --unified glibc-2.25.orig/sysdeps/unix/sysv/linux/x86_64/libc-lowlevellock.S glibc-2.25/sysdeps/unix/sysv/linux/x86_64/libc-lowlevellock.S
--- glibc-2.25.orig/sysdeps/unix/sysv/linux/x86_64/libc-lowlevellock.S	2017-09-11 17:15:15.753676050 +0000
+++ glibc-2.25/sysdeps/unix/sysv/linux/x86_64/libc-lowlevellock.S	1970-01-01 00:00:00.000000000 +0000
@@ -1,19 +0,0 @@
-/* Copyright (C) 2002-2017 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, see
-   <http://www.gnu.org/licenses/>.  */
-
-#include "lowlevellock.S"
diff -N -r --unified glibc-2.25.orig/sysdeps/unix/sysv/linux/x86_64/librt-cancellation.S glibc-2.25/sysdeps/unix/sysv/linux/x86_64/librt-cancellation.S
--- glibc-2.25.orig/sysdeps/unix/sysv/linux/x86_64/librt-cancellation.S	2017-09-11 17:15:15.754676121 +0000
+++ glibc-2.25/sysdeps/unix/sysv/linux/x86_64/librt-cancellation.S	1970-01-01 00:00:00.000000000 +0000
@@ -1,21 +0,0 @@
-/* Copyright (C) 2009-2017 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-   Contributed by Ulrich Drepper <drepper@redhat.com>, 2009.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, see
-   <http://www.gnu.org/licenses/>.  */
-
-#define __pthread_enable_asynccancel __librt_enable_asynccancel
-#define __pthread_disable_asynccancel __librt_disable_asynccancel
-#include "cancellation.S"
diff -N -r --unified glibc-2.25.orig/sysdeps/unix/sysv/linux/x86_64/lll_timedlock_wait.c glibc-2.25/sysdeps/unix/sysv/linux/x86_64/lll_timedlock_wait.c
--- glibc-2.25.orig/sysdeps/unix/sysv/linux/x86_64/lll_timedlock_wait.c	2017-09-11 17:15:15.755676193 +0000
+++ glibc-2.25/sysdeps/unix/sysv/linux/x86_64/lll_timedlock_wait.c	2017-09-13 19:55:56.013747542 +0000
@@ -1 +1 @@
-/* __lll_timedlock_wait is in lowlevellock.S.  */
+#include "../../../../../nptl/lll_timedlock_wait.c"
diff -N -r --unified glibc-2.25.orig/sysdeps/unix/sysv/linux/x86_64/lll_timedwait_tid.c glibc-2.25/sysdeps/unix/sysv/linux/x86_64/lll_timedwait_tid.c
--- glibc-2.25.orig/sysdeps/unix/sysv/linux/x86_64/lll_timedwait_tid.c	2017-09-11 17:15:15.755676193 +0000
+++ glibc-2.25/sysdeps/unix/sysv/linux/x86_64/lll_timedwait_tid.c	2017-09-13 19:55:56.015747685 +0000
@@ -1 +1 @@
-/* __lll_timedwait_tid is in lowlevellock.S.  */
+#include "../../../../../nptl/lll_timedwait_tid.c"
diff -N -r --unified glibc-2.25.orig/sysdeps/unix/sysv/linux/x86_64/lowlevellock.h glibc-2.25/sysdeps/unix/sysv/linux/x86_64/lowlevellock.h
--- glibc-2.25.orig/sysdeps/unix/sysv/linux/x86_64/lowlevellock.h	2017-09-11 17:15:15.841682345 +0000
+++ glibc-2.25/sysdeps/unix/sysv/linux/x86_64/lowlevellock.h	2017-09-13 19:55:56.025748399 +0000
@@ -1,6 +1,6 @@
-/* Copyright (C) 2002-2017 Free Software Foundation, Inc.
+/* Low-level lock implementation.  Generic futex-based version.
+   Copyright (C) 2005-2017 Free Software Foundation, Inc.
    This file is part of the GNU C Library.
-   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
 
    The GNU C Library is free software; you can redistribute it and/or
    modify it under the terms of the GNU Lesser General Public
@@ -9,221 +9,176 @@
 
    The GNU C Library is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.	 See the GNU
    Lesser General Public License for more details.
 
    You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, see
+   License along with the GNU C Library.  If not, see
    <http://www.gnu.org/licenses/>.  */
 
 #ifndef _LOWLEVELLOCK_H
 #define _LOWLEVELLOCK_H	1
 
-#include <stap-probe.h>
+#include <atomic.h>
+#include <lowlevellock-futex.h>
+
+#define SPIN_LOCK_READS_BETWEEN_CMPXCHG 1000
+
+/* Low-level locks use a combination of atomic operations (to acquire and
+   release lock ownership) and futex operations (to block until the state
+   of a lock changes).  A lock can be in one of three states:
+   0:  not acquired,
+   1:  acquired with no waiters; no other threads are blocked or about to block
+       for changes to the lock state,
+   >1: acquired, possibly with waiters; there may be other threads blocked or
+       about to block for changes to the lock state.
+
+   We expect that the common case is an uncontended lock, so we just need
+   to transition the lock between states 0 and 1; releasing the lock does
+   not need to wake any other blocked threads.  If the lock is contended
+   and a thread decides to block using a futex operation, then this thread
+   needs to first change the state to >1; if this state is observed during
+   lock release, the releasing thread will wake one of the potentially
+   blocked threads.
+
+   Much of this code takes a 'private' parameter.  This may be:
+   LLL_PRIVATE: lock only shared within a process
+   LLL_SHARED:  lock may be shared across processes.
+
+   Condition variables contain an optimization for broadcasts that requeues
+   waiting threads on a lock's futex.  Therefore, there is a special
+   variant of the locks (whose name contains "cond") that makes sure to
+   always set the lock state to >1 and not just 1.
+
+   Robust locks set the lock to the id of the owner.  This allows detection
+   of the case where the owner exits without releasing the lock.  Flags are
+   OR'd with the owner id to record additional information about lock state.
+   Therefore the states of robust locks are:
+    0: not acquired
+   id: acquired (by user identified by id & FUTEX_TID_MASK)
+
+   The following flags may be set in the robust lock value:
+   FUTEX_WAITERS     - possibly has waiters
+   FUTEX_OWNER_DIED  - owning user has exited without releasing the futex.  */
+
+
+/* If LOCK is 0 (not acquired), set to 1 (acquired with no waiters) and return
+   0.  Otherwise leave lock unchanged and return non-zero to indicate that the
+   lock was not acquired.  */
+#define lll_trylock(lock)	\
+  atomic_compare_and_exchange_bool_acq (&(lock), 1, 0)
+
+/* If LOCK is 0 (not acquired), set to 2 (acquired, possibly with waiters) and
+   return 0.  Otherwise leave lock unchanged and return non-zero to indicate
+   that the lock was not acquired.  */
+#define lll_cond_trylock(lock)	\
+  atomic_compare_and_exchange_bool_acq (&(lock), 2, 0)
 
 #ifndef __ASSEMBLER__
-# include <time.h>
-# include <sys/param.h>
-# include <bits/pthreadtypes.h>
-# include <kernel-features.h>
-# include <tcb-offsets.h>
-
-# ifndef LOCK_INSTR
-#  ifdef UP
-#   define LOCK_INSTR	/* nothing */
-#  else
-#   define LOCK_INSTR "lock;"
-#  endif
-# endif
-#else
-# ifndef LOCK
-#  ifdef UP
-#   define LOCK
-#  else
-#   define LOCK lock
-#  endif
-# endif
+extern void __lll_lock_wait_private (int *futex) attribute_hidden;
+extern void __lll_lock_wait (int *futex, int private) attribute_hidden;
 #endif
 
-#include <lowlevellock-futex.h>
-
-/* XXX Remove when no assembler code uses futexes anymore.  */
-#define SYS_futex		__NR_futex
+/* This is an expression rather than a statement even though its value is
+   void, so that it can be used in a comma expression or as an expression
+   that's cast to void.  */
+/* The inner conditional compiles to a call to __lll_lock_wait_private if
+   private is known at compile time to be LLL_PRIVATE, and to a call to
+   __lll_lock_wait otherwise.  */
+/* If FUTEX is 0 (not acquired), set to 1 (acquired with no waiters) and
+   return.  Otherwise, ensure that it is >1 (acquired, possibly with waiters)
+   and then block until we acquire the lock, at which point FUTEX will still be
+   >1.  The lock is always acquired on return.  */
+#define __lll_lock(futex, private)                                      \
+  ((void)                                                               \
+   ({                                                                   \
+     int *__futex = (futex);                                            \
+     if (__glibc_unlikely                                               \
+         (atomic_compare_and_exchange_bool_acq (__futex, 1, 0)))        \
+       {                                                                \
+         if (__builtin_constant_p (private) && (private) == LLL_PRIVATE) \
+           __lll_lock_wait_private (__futex);                           \
+         else                                                           \
+           __lll_lock_wait (__futex, private);                          \
+       }                                                                \
+   }))
+#define lll_lock(futex, private)	\
+  __lll_lock (&(futex), private)
+
+
+/* This is an expression rather than a statement even though its value is
+   void, so that it can be used in a comma expression or as an expression
+   that's cast to void.  */
+/* Unconditionally set FUTEX to 2 (acquired, possibly with waiters).  If FUTEX
+   was 0 (not acquired) then return.  Otherwise, block until the lock is
+   acquired, at which point FUTEX is 2 (acquired, possibly with waiters).  The
+   lock is always acquired on return.  */
+#define __lll_cond_lock(futex, private)                                 \
+  ((void)                                                               \
+   ({                                                                   \
+     int *__futex = (futex);                                            \
+     if (__glibc_unlikely (atomic_exchange_acq (__futex, 2) != 0))      \
+       __lll_lock_wait (__futex, private);                              \
+   }))
+#define lll_cond_lock(futex, private) __lll_cond_lock (&(futex), private)
 
 #ifndef __ASSEMBLER__
+extern int __lll_timedlock_wait (int *futex, const struct timespec *,
+				 int private) attribute_hidden;
+#endif
 
-/* Initializer for lock.  */
-#define LLL_LOCK_INITIALIZER		(0)
-#define LLL_LOCK_INITIALIZER_LOCKED	(1)
-#define LLL_LOCK_INITIALIZER_WAITERS	(2)
 
+/* As __lll_lock, but with a timeout.  If the timeout occurs then return
+   ETIMEDOUT.  If ABSTIME is invalid, return EINVAL.  */
+#define __lll_timedlock(futex, abstime, private)                \
+  ({                                                            \
+    int *__futex = (futex);                                     \
+    int __val = 0;                                              \
+                                                                \
+    if (__glibc_unlikely                                        \
+        (atomic_compare_and_exchange_bool_acq (__futex, 1, 0))) \
+      __val = __lll_timedlock_wait (__futex, abstime, private); \
+    __val;                                                      \
+  })
+#define lll_timedlock(futex, abstime, private)  \
+  __lll_timedlock (&(futex), abstime, private)
+
+
+/* This is an expression rather than a statement even though its value is
+   void, so that it can be used in a comma expression or as an expression
+   that's cast to void.  */
+/* Unconditionally set FUTEX to 0 (not acquired), releasing the lock.  If FUTEX
+   was >1 (acquired, possibly with waiters), then wake any waiters.  The waiter
+   that acquires the lock will set FUTEX to >1.
+   Evaluate PRIVATE before releasing the lock so that we do not violate the
+   mutex destruction requirements.  Specifically, we need to ensure that
+   another thread can destroy the mutex (and reuse its memory) once it
+   acquires the lock and when there will be no further lock acquisitions;
+   thus, we must not access the lock after releasing it, or those accesses
+   could be concurrent with mutex destruction or reuse of the memory.  */
+#define __lll_unlock(futex, private)                    \
+  ((void)                                               \
+   ({                                                   \
+     int *__futex = (futex);                            \
+     int __private = (private);                         \
+     int __oldval = atomic_exchange_rel (__futex, 0);   \
+     if (__glibc_unlikely (__oldval > 1))               \
+       lll_futex_wake (__futex, 1, __private);          \
+   }))
+#define lll_unlock(futex, private)	\
+  __lll_unlock (&(futex), private)
 
-/* NB: in the lll_trylock macro we simply return the value in %eax
-   after the cmpxchg instruction.  In case the operation succeded this
-   value is zero.  In case the operation failed, the cmpxchg instruction
-   has loaded the current value of the memory work which is guaranteed
-   to be nonzero.  */
-#if !IS_IN (libc) || defined UP
-# define __lll_trylock_asm LOCK_INSTR "cmpxchgl %2, %1"
-#else
-# define __lll_trylock_asm "cmpl $0, __libc_multiple_threads(%%rip)\n\t"      \
-			   "je 0f\n\t"					      \
-			   "lock; cmpxchgl %2, %1\n\t"			      \
-			   "jmp 1f\n\t"					      \
-			   "0:\tcmpxchgl %2, %1\n\t"			      \
-			   "1:"
-#endif
 
-#define lll_trylock(futex) \
-  ({ int ret;								      \
-     __asm __volatile (__lll_trylock_asm				      \
-		       : "=a" (ret), "=m" (futex)			      \
-		       : "r" (LLL_LOCK_INITIALIZER_LOCKED), "m" (futex),      \
-			 "0" (LLL_LOCK_INITIALIZER)			      \
-		       : "memory");					      \
-     ret; })
-
-#define lll_cond_trylock(futex) \
-  ({ int ret;								      \
-     __asm __volatile (LOCK_INSTR "cmpxchgl %2, %1"			      \
-		       : "=a" (ret), "=m" (futex)			      \
-		       : "r" (LLL_LOCK_INITIALIZER_WAITERS),		      \
-			 "m" (futex), "0" (LLL_LOCK_INITIALIZER)	      \
-		       : "memory");					      \
-     ret; })
-
-#if !IS_IN (libc) || defined UP
-# define __lll_lock_asm_start LOCK_INSTR "cmpxchgl %4, %2\n\t"		      \
-			      "jz 24f\n\t"
-#else
-# define __lll_lock_asm_start "cmpl $0, __libc_multiple_threads(%%rip)\n\t"   \
-			      "je 0f\n\t"				      \
-			      "lock; cmpxchgl %4, %2\n\t"		      \
-			      "jnz 1f\n\t"				      \
-			      "jmp 24f\n"				      \
-			      "0:\tcmpxchgl %4, %2\n\t"			      \
-			      "jz 24f\n\t"
-#endif
+#define lll_islocked(futex) \
+  ((futex) != LLL_LOCK_INITIALIZER)
 
-#define lll_lock(futex, private) \
-  (void)								      \
-    ({ int ignore1, ignore2, ignore3;					      \
-       if (__builtin_constant_p (private) && (private) == LLL_PRIVATE)	      \
-	 __asm __volatile (__lll_lock_asm_start				      \
-			   "1:\tlea %2, %%" RDI_LP "\n"			      \
-			   "2:\tsub $128, %%" RSP_LP "\n"		      \
-			   ".cfi_adjust_cfa_offset 128\n"		      \
-			   "3:\tcallq __lll_lock_wait_private\n"	      \
-			   "4:\tadd $128, %%" RSP_LP "\n"		      \
-			   ".cfi_adjust_cfa_offset -128\n"		      \
-			   "24:"					      \
-			   : "=S" (ignore1), "=&D" (ignore2), "=m" (futex),   \
-			     "=a" (ignore3)				      \
-			   : "0" (1), "m" (futex), "3" (0)		      \
-			   : "cx", "r11", "cc", "memory");		      \
-       else								      \
-	 __asm __volatile (__lll_lock_asm_start				      \
-			   "1:\tlea %2, %%" RDI_LP "\n"			      \
-			   "2:\tsub $128, %%" RSP_LP "\n"		      \
-			   ".cfi_adjust_cfa_offset 128\n"		      \
-			   "3:\tcallq __lll_lock_wait\n"		      \
-			   "4:\tadd $128, %%" RSP_LP "\n"		      \
-			   ".cfi_adjust_cfa_offset -128\n"		      \
-			   "24:"					      \
-			   : "=S" (ignore1), "=D" (ignore2), "=m" (futex),    \
-			     "=a" (ignore3)				      \
-			   : "1" (1), "m" (futex), "3" (0), "0" (private)     \
-			   : "cx", "r11", "cc", "memory");		      \
-    })									      \
-
-#define lll_cond_lock(futex, private) \
-  (void)								      \
-    ({ int ignore1, ignore2, ignore3;					      \
-       __asm __volatile (LOCK_INSTR "cmpxchgl %4, %2\n\t"		      \
-			 "jz 24f\n"					      \
-			 "1:\tlea %2, %%" RDI_LP "\n"			      \
-			 "2:\tsub $128, %%" RSP_LP "\n"			      \
-			 ".cfi_adjust_cfa_offset 128\n"			      \
-			 "3:\tcallq __lll_lock_wait\n"			      \
-			 "4:\tadd $128, %%" RSP_LP "\n"			      \
-			 ".cfi_adjust_cfa_offset -128\n"		      \
-			 "24:"						      \
-			 : "=S" (ignore1), "=D" (ignore2), "=m" (futex),      \
-			   "=a" (ignore3)				      \
-			 : "1" (2), "m" (futex), "3" (0), "0" (private)	      \
-			 : "cx", "r11", "cc", "memory");		      \
-    })
-
-#define lll_timedlock(futex, timeout, private) \
-  ({ int result, ignore1, ignore2, ignore3;				      \
-     __asm __volatile (LOCK_INSTR "cmpxchgl %1, %4\n\t"			      \
-		       "jz 24f\n"					      \
-		       "1:\tlea %4, %%" RDI_LP "\n"			      \
-		       "0:\tmov %8, %%" RDX_LP "\n"			      \
-		       "2:\tsub $128, %%" RSP_LP "\n"			      \
-		       ".cfi_adjust_cfa_offset 128\n"			      \
-		       "3:\tcallq __lll_timedlock_wait\n"		      \
-		       "4:\tadd $128, %%" RSP_LP "\n"			      \
-		       ".cfi_adjust_cfa_offset -128\n"			      \
-		       "24:"						      \
-		       : "=a" (result), "=D" (ignore1), "=S" (ignore2),	      \
-			 "=&d" (ignore3), "=m" (futex)			      \
-		       : "0" (0), "1" (1), "m" (futex), "m" (timeout),	      \
-			 "2" (private)					      \
-		       : "memory", "cx", "cc", "r10", "r11");		      \
-     result; })
-
-extern int __lll_timedlock_elision (int *futex, short *adapt_count,
-					 const struct timespec *timeout,
-					 int private) attribute_hidden;
-
-#define lll_timedlock_elision(futex, adapt_count, timeout, private)	\
-  __lll_timedlock_elision(&(futex), &(adapt_count), timeout, private)
-
-#if !IS_IN (libc) || defined UP
-# define __lll_unlock_asm_start LOCK_INSTR "decl %0\n\t"		      \
-				"je 24f\n\t"
-#else
-# define __lll_unlock_asm_start "cmpl $0, __libc_multiple_threads(%%rip)\n\t" \
-				"je 0f\n\t"				      \
-				"lock; decl %0\n\t"			      \
-				"jne 1f\n\t"				      \
-				"jmp 24f\n\t"				      \
-				"0:\tdecl %0\n\t"			      \
-				"je 24f\n\t"
-#endif
 
-#define lll_unlock(futex, private) \
-  (void)								      \
-    ({ int ignore;							      \
-       if (__builtin_constant_p (private) && (private) == LLL_PRIVATE)	      \
-	 __asm __volatile (__lll_unlock_asm_start			      \
-			   "1:\tlea %0, %%" RDI_LP "\n"			      \
-			   "2:\tsub $128, %%" RSP_LP "\n"		      \
-			   ".cfi_adjust_cfa_offset 128\n"		      \
-			   "3:\tcallq __lll_unlock_wake_private\n"	      \
-			   "4:\tadd $128, %%" RSP_LP "\n"		      \
-			   ".cfi_adjust_cfa_offset -128\n"		      \
-			   "24:"					      \
-			   : "=m" (futex), "=&D" (ignore)		      \
-			   : "m" (futex)				      \
-			   : "ax", "cx", "r11", "cc", "memory");	      \
-       else								      \
-	 __asm __volatile (__lll_unlock_asm_start			      \
-			   "1:\tlea %0, %%" RDI_LP "\n"			      \
-			   "2:\tsub $128, %%" RSP_LP "\n"		      \
-			   ".cfi_adjust_cfa_offset 128\n"		      \
-			   "3:\tcallq __lll_unlock_wake\n"		      \
-			   "4:\tadd $128, %%" RSP_LP "\n"		      \
-			   ".cfi_adjust_cfa_offset -128\n"		      \
-			   "24:"					      \
-			   : "=m" (futex), "=&D" (ignore)		      \
-			   : "m" (futex), "S" (private)			      \
-			   : "ax", "cx", "r11", "cc", "memory");	      \
-    })
+/* Our internal lock implementation is identical to the binary-compatible
+   mutex implementation. */
 
-#define lll_islocked(futex) \
-  (futex != LLL_LOCK_INITIALIZER)
+/* Initializers for lock.  */
+#define LLL_LOCK_INITIALIZER		(0)
+#define LLL_LOCK_INITIALIZER_LOCKED	(1)
 
 
 /* The kernel notifies a process which uses CLONE_CHILD_CLEARTID via futex
@@ -234,45 +189,26 @@
 #define lll_wait_tid(tid) \
   do {					\
     __typeof (tid) __tid;		\
-    while ((__tid = (tid)) != 0)	\
-      lll_futex_wait (&(tid), __tid, LLL_SHARED);\
+    while ((__tid = (tid)) != 0 || mvee_should_sync_tid())	{			\
+		lll_futex_syscall (4, &(tid), __lll_private_flag(mvee_should_sync_tid() ? MVEE_FUTEX_WAIT_TID : FUTEX_WAIT, LLL_SHARED), __tid, NULL); \
+				   if ((tid) == 0) break; \
+	}\
   } while (0)
 
+#ifndef __ASSEMBLER__
 extern int __lll_timedwait_tid (int *, const struct timespec *)
      attribute_hidden;
+#endif
 
 /* As lll_wait_tid, but with a timeout.  If the timeout occurs then return
-   ETIMEDOUT.  If ABSTIME is invalid, return EINVAL.
-   XXX Note that this differs from the generic version in that we do the
-   error checking here and not in __lll_timedwait_tid.  */
+   ETIMEDOUT.  If ABSTIME is invalid, return EINVAL.  */
 #define lll_timedwait_tid(tid, abstime) \
-  ({									      \
-    int __result = 0;							      \
-    if ((tid) != 0)							      \
-      {									      \
-	if ((abstime)->tv_nsec < 0 || (abstime)->tv_nsec >= 1000000000)	      \
-	  __result = EINVAL;						      \
-	else								      \
-	  __result = __lll_timedwait_tid (&(tid), (abstime));		      \
-      }									      \
-    __result; })
-
-extern int __lll_lock_elision (int *futex, short *adapt_count, int private)
-  attribute_hidden;
-
-extern int __lll_unlock_elision (int *lock, int private)
-  attribute_hidden;
-
-extern int __lll_trylock_elision (int *lock, short *adapt_count)
-  attribute_hidden;
-
-#define lll_lock_elision(futex, adapt_count, private) \
-  __lll_lock_elision (&(futex), &(adapt_count), private)
-#define lll_unlock_elision(futex, adapt_count, private) \
-  __lll_unlock_elision (&(futex), private)
-#define lll_trylock_elision(futex, adapt_count) \
-  __lll_trylock_elision (&(futex), &(adapt_count))
+  ({							\
+    int __res = 0;					\
+    if ((tid) != 0 || mvee_should_sync_tid())				\
+      __res = __lll_timedwait_tid (&(tid), (abstime));	\
+    __res;						\
+  })
 
-#endif  /* !__ASSEMBLER__ */
 
 #endif	/* lowlevellock.h */
diff -N -r --unified glibc-2.25.orig/sysdeps/unix/sysv/linux/x86_64/lowlevellock.S glibc-2.25/sysdeps/unix/sysv/linux/x86_64/lowlevellock.S
--- glibc-2.25.orig/sysdeps/unix/sysv/linux/x86_64/lowlevellock.S	2017-09-11 17:15:15.756676264 +0000
+++ glibc-2.25/sysdeps/unix/sysv/linux/x86_64/lowlevellock.S	1970-01-01 00:00:00.000000000 +0000
@@ -1,463 +0,0 @@
-/* Copyright (C) 2002-2017 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, see
-   <http://www.gnu.org/licenses/>.  */
-
-#include <sysdep.h>
-#include <pthread-errnos.h>
-#include <kernel-features.h>
-#include <lowlevellock.h>
-
-#include <stap-probe.h>
-
-	.text
-
-#ifdef __ASSUME_PRIVATE_FUTEX
-# define LOAD_PRIVATE_FUTEX_WAIT(reg) \
-	movl	$(FUTEX_WAIT | FUTEX_PRIVATE_FLAG), reg
-# define LOAD_PRIVATE_FUTEX_WAKE(reg) \
-	movl	$(FUTEX_WAKE | FUTEX_PRIVATE_FLAG), reg
-# define LOAD_FUTEX_WAIT(reg) \
-	xorl	$(FUTEX_WAIT | FUTEX_PRIVATE_FLAG), reg
-# define LOAD_FUTEX_WAIT_ABS(reg) \
-	xorl	$(FUTEX_WAIT_BITSET | FUTEX_PRIVATE_FLAG | FUTEX_CLOCK_REALTIME), reg
-# define LOAD_FUTEX_WAKE(reg) \
-	xorl	$(FUTEX_WAKE | FUTEX_PRIVATE_FLAG), reg
-#else
-# if FUTEX_WAIT == 0
-#  define LOAD_PRIVATE_FUTEX_WAIT(reg) \
-	movl    %fs:PRIVATE_FUTEX, reg
-# else
-#  define LOAD_PRIVATE_FUTEX_WAIT(reg) \
-	movl	%fs:PRIVATE_FUTEX, reg ; \
-	orl	$FUTEX_WAIT, reg
-# endif
-# define LOAD_PRIVATE_FUTEX_WAKE(reg) \
-	movl    %fs:PRIVATE_FUTEX, reg ; \
-	orl     $FUTEX_WAKE, reg
-# if FUTEX_WAIT == 0
-#  define LOAD_FUTEX_WAIT(reg) \
-	xorl	$FUTEX_PRIVATE_FLAG, reg ; \
-	andl	%fs:PRIVATE_FUTEX, reg
-# else
-#  define LOAD_FUTEX_WAIT(reg) \
-	xorl	$FUTEX_PRIVATE_FLAG, reg ; \
-	andl	%fs:PRIVATE_FUTEX, reg ; \
-	orl	$FUTEX_WAIT, reg
-# endif
-# define LOAD_FUTEX_WAIT_ABS(reg) \
-	xorl	$FUTEX_PRIVATE_FLAG, reg ; \
-	andl	%fs:PRIVATE_FUTEX, reg ; \
-	orl	$FUTEX_WAIT_BITSET | FUTEX_CLOCK_REALTIME, reg
-# define LOAD_FUTEX_WAKE(reg) \
-	xorl	$FUTEX_PRIVATE_FLAG, reg ; \
-	andl	%fs:PRIVATE_FUTEX, reg ; \
-	orl	$FUTEX_WAKE, reg
-#endif
-
-
-	.globl	__lll_lock_wait_private
-	.type	__lll_lock_wait_private,@function
-	.hidden	__lll_lock_wait_private
-	.align	16
-__lll_lock_wait_private:
-	cfi_startproc
-	pushq	%r10
-	cfi_adjust_cfa_offset(8)
-	pushq	%rdx
-	cfi_adjust_cfa_offset(8)
-	cfi_offset(%r10, -16)
-	cfi_offset(%rdx, -24)
-	xorq	%r10, %r10	/* No timeout.  */
-	movl	$2, %edx
-	LOAD_PRIVATE_FUTEX_WAIT (%esi)
-
-	cmpl	%edx, %eax	/* NB:	 %edx == 2 */
-	jne	2f
-
-1:	LIBC_PROBE (lll_lock_wait_private, 1, %rdi)
-	movl	$SYS_futex, %eax
-	syscall
-
-2:	movl	%edx, %eax
-	xchgl	%eax, (%rdi)	/* NB:	 lock is implied */
-
-	testl	%eax, %eax
-	jnz	1b
-
-	popq	%rdx
-	cfi_adjust_cfa_offset(-8)
-	cfi_restore(%rdx)
-	popq	%r10
-	cfi_adjust_cfa_offset(-8)
-	cfi_restore(%r10)
-	retq
-	cfi_endproc
-	.size	__lll_lock_wait_private,.-__lll_lock_wait_private
-
-#if !IS_IN (libc)
-	.globl	__lll_lock_wait
-	.type	__lll_lock_wait,@function
-	.hidden	__lll_lock_wait
-	.align	16
-__lll_lock_wait:
-	cfi_startproc
-	pushq	%r10
-	cfi_adjust_cfa_offset(8)
-	pushq	%rdx
-	cfi_adjust_cfa_offset(8)
-	cfi_offset(%r10, -16)
-	cfi_offset(%rdx, -24)
-	xorq	%r10, %r10	/* No timeout.  */
-	movl	$2, %edx
-	LOAD_FUTEX_WAIT (%esi)
-
-	cmpl	%edx, %eax	/* NB:	 %edx == 2 */
-	jne	2f
-
-1:	LIBC_PROBE (lll_lock_wait, 2, %rdi, %rsi)
-	movl	$SYS_futex, %eax
-	syscall
-
-2:	movl	%edx, %eax
-	xchgl	%eax, (%rdi)	/* NB:	 lock is implied */
-
-	testl	%eax, %eax
-	jnz	1b
-
-	popq	%rdx
-	cfi_adjust_cfa_offset(-8)
-	cfi_restore(%rdx)
-	popq	%r10
-	cfi_adjust_cfa_offset(-8)
-	cfi_restore(%r10)
-	retq
-	cfi_endproc
-	.size	__lll_lock_wait,.-__lll_lock_wait
-
-	/*      %rdi: futex
-		%rsi: flags
-		%rdx: timeout
-		%eax: futex value
-	*/
-	.globl	__lll_timedlock_wait
-	.type	__lll_timedlock_wait,@function
-	.hidden	__lll_timedlock_wait
-	.align	16
-__lll_timedlock_wait:
-	cfi_startproc
-# ifndef __ASSUME_FUTEX_CLOCK_REALTIME
-#  ifdef PIC
-	cmpl	$0, __have_futex_clock_realtime(%rip)
-#  else
-	cmpl	$0, __have_futex_clock_realtime
-#  endif
-	je	.Lreltmo
-# endif
-
-	cmpq	$0, (%rdx)
-	js	5f
-
-	pushq	%r9
-	cfi_adjust_cfa_offset(8)
-	cfi_rel_offset(%r9, 0)
-
-	movq	%rdx, %r10
-	movl	$0xffffffff, %r9d
-	LOAD_FUTEX_WAIT_ABS (%esi)
-
-	movl	$2, %edx
-	cmpl	%edx, %eax
-	jne	2f
-
-1:	movl	$SYS_futex, %eax
-	movl	$2, %edx
-	syscall
-
-2:	xchgl	%edx, (%rdi)	/* NB:   lock is implied */
-
-	testl	%edx, %edx
-	jz	3f
-
-	cmpl	$-ETIMEDOUT, %eax
-	je	4f
-	cmpl	$-EINVAL, %eax
-	jne	1b
-4:	movl	%eax, %edx
-	negl	%edx
-
-3:	movl	%edx, %eax
-	popq	%r9
-	cfi_adjust_cfa_offset(-8)
-	cfi_restore(%r9)
-	retq
-
-5:	movl	$ETIMEDOUT, %eax
-	retq
-
-# ifndef __ASSUME_FUTEX_CLOCK_REALTIME
-.Lreltmo:
-	/* Check for a valid timeout value.  */
-	cmpq	$1000000000, 8(%rdx)
-	jae	3f
-
-	pushq	%r8
-	cfi_adjust_cfa_offset(8)
-	pushq	%r9
-	cfi_adjust_cfa_offset(8)
-	pushq	%r12
-	cfi_adjust_cfa_offset(8)
-	pushq	%r13
-	cfi_adjust_cfa_offset(8)
-	pushq	%r14
-	cfi_adjust_cfa_offset(8)
-	cfi_offset(%r8, -16)
-	cfi_offset(%r9, -24)
-	cfi_offset(%r12, -32)
-	cfi_offset(%r13, -40)
-	cfi_offset(%r14, -48)
-	pushq	%rsi
-	cfi_adjust_cfa_offset(8)
-
-	/* Stack frame for the timespec and timeval structs.  */
-	subq	$24, %rsp
-	cfi_adjust_cfa_offset(24)
-
-	movq	%rdi, %r12
-	movq	%rdx, %r13
-
-	movl	$2, %edx
-	xchgl	%edx, (%r12)
-
-	testl	%edx, %edx
-	je	6f
-
-1:
-	/* Get current time.  */
-	movq	%rsp, %rdi
-	xorl	%esi, %esi
-	/* This call works because we directly jump to a system call entry
-	   which preserves all the registers.  */
-	call	JUMPTARGET(__gettimeofday)
-
-	/* Compute relative timeout.  */
-	movq	8(%rsp), %rax
-	movl	$1000, %edi
-	mul	%rdi		/* Milli seconds to nano seconds.  */
-	movq	(%r13), %rdi
-	movq	8(%r13), %rsi
-	subq	(%rsp), %rdi
-	subq	%rax, %rsi
-	jns	4f
-	addq	$1000000000, %rsi
-	decq	%rdi
-4:	testq	%rdi, %rdi
-	js	2f		/* Time is already up.  */
-
-	/* Store relative timeout.  */
-	movq	%rdi, (%rsp)
-	movq	%rsi, 8(%rsp)
-
-	/* Futex call.  */
-	movl	$2, %edx
-	movl	$1, %eax
-	movq	%rsp, %r10
-	movl	24(%rsp), %esi
-	LOAD_FUTEX_WAIT (%esi)
-	movq	%r12, %rdi
-	movl	$SYS_futex, %eax
-	syscall
-
-	/* NB: %edx == 2 */
-	xchgl	%edx, (%r12)
-
-	testl	%edx, %edx
-	je	6f
-
-	cmpl	$-ETIMEDOUT, %eax
-	jne	1b
-2:	movl	$ETIMEDOUT, %edx
-
-6:	addq	$32, %rsp
-	cfi_adjust_cfa_offset(-32)
-	popq	%r14
-	cfi_adjust_cfa_offset(-8)
-	cfi_restore(%r14)
-	popq	%r13
-	cfi_adjust_cfa_offset(-8)
-	cfi_restore(%r13)
-	popq	%r12
-	cfi_adjust_cfa_offset(-8)
-	cfi_restore(%r12)
-	popq	%r9
-	cfi_adjust_cfa_offset(-8)
-	cfi_restore(%r9)
-	popq	%r8
-	cfi_adjust_cfa_offset(-8)
-	cfi_restore(%r8)
-	movl	%edx, %eax
-	retq
-
-3:	movl	$EINVAL, %eax
-	retq
-# endif
-	cfi_endproc
-	.size	__lll_timedlock_wait,.-__lll_timedlock_wait
-#endif
-
-
-	.globl	__lll_unlock_wake_private
-	.type	__lll_unlock_wake_private,@function
-	.hidden	__lll_unlock_wake_private
-	.align	16
-__lll_unlock_wake_private:
-	cfi_startproc
-	pushq	%rsi
-	cfi_adjust_cfa_offset(8)
-	pushq	%rdx
-	cfi_adjust_cfa_offset(8)
-	cfi_offset(%rsi, -16)
-	cfi_offset(%rdx, -24)
-
-	movl	$0, (%rdi)
-	LOAD_PRIVATE_FUTEX_WAKE (%esi)
-	movl	$1, %edx	/* Wake one thread.  */
-	movl	$SYS_futex, %eax
-	syscall
-
-	popq	%rdx
-	cfi_adjust_cfa_offset(-8)
-	cfi_restore(%rdx)
-	popq	%rsi
-	cfi_adjust_cfa_offset(-8)
-	cfi_restore(%rsi)
-	retq
-	cfi_endproc
-	.size	__lll_unlock_wake_private,.-__lll_unlock_wake_private
-
-#if !IS_IN (libc)
-	.globl	__lll_unlock_wake
-	.type	__lll_unlock_wake,@function
-	.hidden	__lll_unlock_wake
-	.align	16
-__lll_unlock_wake:
-	cfi_startproc
-	pushq	%rsi
-	cfi_adjust_cfa_offset(8)
-	pushq	%rdx
-	cfi_adjust_cfa_offset(8)
-	cfi_offset(%rsi, -16)
-	cfi_offset(%rdx, -24)
-
-	movl	$0, (%rdi)
-	LOAD_FUTEX_WAKE (%esi)
-	movl	$1, %edx	/* Wake one thread.  */
-	movl	$SYS_futex, %eax
-	syscall
-
-	popq	%rdx
-	cfi_adjust_cfa_offset(-8)
-	cfi_restore(%rdx)
-	popq	%rsi
-	cfi_adjust_cfa_offset(-8)
-	cfi_restore(%rsi)
-	retq
-	cfi_endproc
-	.size	__lll_unlock_wake,.-__lll_unlock_wake
-
-	.globl	__lll_timedwait_tid
-	.type	__lll_timedwait_tid,@function
-	.hidden	__lll_timedwait_tid
-	.align	16
-__lll_timedwait_tid:
-	cfi_startproc
-	pushq	%r12
-	cfi_adjust_cfa_offset(8)
-	pushq	%r13
-	cfi_adjust_cfa_offset(8)
-	cfi_offset(%r12, -16)
-	cfi_offset(%r13, -24)
-
-	movq	%rdi, %r12
-	movq	%rsi, %r13
-
-	/* Align stack to 16 bytes when calling __gettimeofday.  */
-	subq	$24, %rsp
-	cfi_adjust_cfa_offset(24)
-
-	/* Get current time.  */
-2:	movq	%rsp, %rdi
-	xorl	%esi, %esi
-	/* This call works because we directly jump to a system call entry
-	   which preserves all the registers.  */
-	call	JUMPTARGET(__gettimeofday)
-
-	/* Compute relative timeout.  */
-	movq	8(%rsp), %rax
-	movl	$1000, %edi
-	mul	%rdi		/* Milli seconds to nano seconds.  */
-	movq	(%r13), %rdi
-	movq	8(%r13), %rsi
-	subq	(%rsp), %rdi
-	subq	%rax, %rsi
-	jns	5f
-	addq	$1000000000, %rsi
-	decq	%rdi
-5:	testq	%rdi, %rdi
-	js	6f		/* Time is already up.  */
-
-	movq	%rdi, (%rsp)	/* Store relative timeout.  */
-	movq	%rsi, 8(%rsp)
-
-	movl	(%r12), %edx
-	testl	%edx, %edx
-	jz	4f
-
-	movq	%rsp, %r10
-	/* XXX The kernel so far uses global futex for the wakeup at
-	   all times.  */
-#if FUTEX_WAIT == 0
-	xorl	%esi, %esi
-#else
-	movl	$FUTEX_WAIT, %esi
-#endif
-	movq	%r12, %rdi
-	movl	$SYS_futex, %eax
-	syscall
-
-	cmpl	$0, (%rdi)
-	jne	1f
-4:	xorl	%eax, %eax
-
-8:	addq	$24, %rsp
-	cfi_adjust_cfa_offset(-24)
-	popq	%r13
-	cfi_adjust_cfa_offset(-8)
-	cfi_restore(%r13)
-	popq	%r12
-	cfi_adjust_cfa_offset(-8)
-	cfi_restore(%r12)
-	retq
-
-	cfi_adjust_cfa_offset(32)
-1:	cmpq	$-ETIMEDOUT, %rax
-	jne	2b
-
-6:	movl	$ETIMEDOUT, %eax
-	jmp	8b
-	cfi_endproc
-	.size	__lll_timedwait_tid,.-__lll_timedwait_tid
-#endif
diff -N -r --unified glibc-2.25.orig/sysdeps/unix/sysv/linux/x86_64/sysdep-cancel.h glibc-2.25/sysdeps/unix/sysv/linux/x86_64/sysdep-cancel.h
--- glibc-2.25.orig/sysdeps/unix/sysv/linux/x86_64/sysdep-cancel.h	2017-09-11 17:15:15.859683633 +0000
+++ glibc-2.25/sysdeps/unix/sysv/linux/x86_64/sysdep-cancel.h	2017-09-13 19:55:56.034749042 +0000
@@ -28,6 +28,18 @@
    functions are special.  They don't modify registers other than %rax
    and %r11 if they return.  Therefore we don't have to preserve other
    registers around these calls.  */
+
+/* This is not true for the GHUMVEE version of glibc. We must explicitly
+   preserve registers because we use the C versions of the functions that enable
+   and disable thread cancellation.  We only have to preserve registers that are
+   caller-saved according to the ABI.  These caller-saved registers are:
+
+   rax, rcx, rdx, rsi, rdi, r8, r9, r10, r11
+
+   The code below already assumes that rax and r11 get clobbered so we only 
+   preserve the remaining regs.
+*/
+
 # undef PSEUDO
 # define PSEUDO(name, syscall_name, args)				      \
   .text;								      \
@@ -45,14 +57,42 @@
   L(pseudo_cancel):							      \
     /* We always have to align the stack before calling a function.  */	      \
     subq $8, %rsp; cfi_adjust_cfa_offset (8);				      \
+    pushq %rcx; cfi_adjust_cfa_offset (8);		  \
+    pushq %rdx; cfi_adjust_cfa_offset (8);	\
+    pushq %rsi; cfi_adjust_cfa_offset (8);	\
+    pushq %rdi; cfi_adjust_cfa_offset (8);	\
+    pushq %r8; cfi_adjust_cfa_offset (8);	\
+    pushq %r9; cfi_adjust_cfa_offset (8);	\
+    pushq %r10; cfi_adjust_cfa_offset (8);	\
     CENABLE								      \
+    popq %r10; cfi_adjust_cfa_offset (-8);	\
+    popq %r9; cfi_adjust_cfa_offset (-8);	\
+    popq %r8; cfi_adjust_cfa_offset (-8);	\
+    popq %rdi; cfi_adjust_cfa_offset (-8);	\
+    popq %rsi; cfi_adjust_cfa_offset (-8);	\
+    popq %rdx; cfi_adjust_cfa_offset (-8);	\
+    popq %rcx; cfi_adjust_cfa_offset (-8);	\
     /* The return value from CENABLE is argument for CDISABLE.  */	      \
     movq %rax, (%rsp);							      \
     DO_CALL (syscall_name, args);					      \
     movq (%rsp), %rdi;							      \
     /* Save %rax since it's the error code from the syscall.  */	      \
     movq %rax, %rdx;							      \
+    pushq %rcx; cfi_adjust_cfa_offset (8);		  \
+    pushq %rdx; cfi_adjust_cfa_offset (8);	\
+    pushq %rsi; cfi_adjust_cfa_offset (8);	\
+    pushq %rdi; cfi_adjust_cfa_offset (8);	\
+    pushq %r8; cfi_adjust_cfa_offset (8);	\
+    pushq %r9; cfi_adjust_cfa_offset (8);	\
+    pushq %r10; cfi_adjust_cfa_offset (8);	\
     CDISABLE								      \
+    popq %r10; cfi_adjust_cfa_offset (-8);	\
+    popq %r9; cfi_adjust_cfa_offset (-8);	\
+    popq %r8; cfi_adjust_cfa_offset (-8);	\
+    popq %rdi; cfi_adjust_cfa_offset (-8);	\
+    popq %rsi; cfi_adjust_cfa_offset (-8);	\
+    popq %rdx; cfi_adjust_cfa_offset (-8);	\
+    popq %rcx; cfi_adjust_cfa_offset (-8);	\
     movq %rdx, %rax;							      \
     addq $8,%rsp; cfi_adjust_cfa_offset (-8);				      \
     cmpq $-4095, %rax;							      \
diff -N -r --unified glibc-2.25.orig/sysdeps/x86_64/atomic-machine.h glibc-2.25/sysdeps/x86_64/atomic-machine.h
--- glibc-2.25.orig/sysdeps/x86_64/atomic-machine.h	2017-09-11 17:15:16.105701231 +0000
+++ glibc-2.25/sysdeps/x86_64/atomic-machine.h	2017-09-13 19:55:56.054750472 +0000
@@ -58,13 +58,13 @@
 #define __HAVE_64B_ATOMICS 1
 #define USE_ATOMIC_COMPILER_BUILTINS 1
 
-#define atomic_compare_and_exchange_val_acq(mem, newval, oldval) \
+#define orig_atomic_compare_and_exchange_val_acq(mem, newval, oldval) \
   __sync_val_compare_and_swap (mem, oldval, newval)
-#define atomic_compare_and_exchange_bool_acq(mem, newval, oldval) \
+#define orig_atomic_compare_and_exchange_bool_acq(mem, newval, oldval) \
   (! __sync_bool_compare_and_swap (mem, oldval, newval))
 
 
-#define __arch_c_compare_and_exchange_val_8_acq(mem, newval, oldval) \
+#define orig___arch_c_compare_and_exchange_val_8_acq(mem, newval, oldval) \
   ({ __typeof (*mem) ret;						      \
     __asm __volatile ("cmpl $0, %%fs:%P5\n\t"				      \
 		      "je 0f\n\t"					      \
@@ -75,7 +75,7 @@
 			 "i" (offsetof (tcbhead_t, multiple_threads)));	      \
      ret; })
 
-#define __arch_c_compare_and_exchange_val_16_acq(mem, newval, oldval) \
+#define orig___arch_c_compare_and_exchange_val_16_acq(mem, newval, oldval) \
   ({ __typeof (*mem) ret;						      \
     __asm __volatile ("cmpl $0, %%fs:%P5\n\t"				      \
 		      "je 0f\n\t"					      \
@@ -86,7 +86,7 @@
 			 "i" (offsetof (tcbhead_t, multiple_threads)));	      \
      ret; })
 
-#define __arch_c_compare_and_exchange_val_32_acq(mem, newval, oldval) \
+#define orig___arch_c_compare_and_exchange_val_32_acq(mem, newval, oldval) \
   ({ __typeof (*mem) ret;						      \
     __asm __volatile ("cmpl $0, %%fs:%P5\n\t"				      \
 		      "je 0f\n\t"					      \
@@ -97,7 +97,7 @@
 			 "i" (offsetof (tcbhead_t, multiple_threads)));	      \
      ret; })
 
-#define __arch_c_compare_and_exchange_val_64_acq(mem, newval, oldval) \
+#define orig___arch_c_compare_and_exchange_val_64_acq(mem, newval, oldval) \
   ({ __typeof (*mem) ret;						      \
      __asm __volatile ("cmpl $0, %%fs:%P5\n\t"				      \
 		       "je 0f\n\t"					      \
@@ -112,7 +112,7 @@
 
 
 /* Note that we need no lock prefix.  */
-#define atomic_exchange_acq(mem, newvalue) \
+#define orig_atomic_exchange_acq(mem, newvalue) \
   ({ __typeof (*mem) result;						      \
      if (sizeof (*mem) == 1)						      \
        __asm __volatile ("xchgb %b0, %1"				      \
@@ -134,7 +134,7 @@
      result; })
 
 
-#define __arch_exchange_and_add_body(lock, mem, value)			      \
+#define orig___arch_exchange_and_add_body(lock, mem, value)			      \
   ({ __typeof (*mem) result;						      \
      if (sizeof (*mem) == 1)						      \
        __asm __volatile (lock "xaddb %b0, %1"				      \
@@ -159,17 +159,17 @@
 			   "i" (offsetof (tcbhead_t, multiple_threads)));     \
      result; })
 
-#define atomic_exchange_and_add(mem, value) \
+#define orig_atomic_exchange_and_add(mem, value) \
   __sync_fetch_and_add (mem, value)
 
 #define __arch_exchange_and_add_cprefix \
   "cmpl $0, %%fs:%P4\n\tje 0f\n\tlock\n0:\t"
 
-#define catomic_exchange_and_add(mem, value) \
-  __arch_exchange_and_add_body (__arch_exchange_and_add_cprefix, mem, value)
+#define orig_catomic_exchange_and_add(mem, value) \
+  orig___arch_exchange_and_add_body (__arch_exchange_and_add_cprefix, mem, value)
 
 
-#define __arch_add_body(lock, pfx, mem, value)				      \
+#define orig___arch_add_body(lock, pfx, mem, value)				      \
   do {									      \
     if (__builtin_constant_p (value) && (value) == 1)			      \
       pfx##_increment (mem);						      \
@@ -198,17 +198,17 @@
 			  "i" (offsetof (tcbhead_t, multiple_threads)));      \
   } while (0)
 
-#define atomic_add(mem, value) \
-  __arch_add_body (LOCK_PREFIX, atomic, mem, value)
+#define orig_atomic_add(mem, value) \
+  orig___arch_add_body (LOCK_PREFIX, orig_atomic, mem, value)
 
 #define __arch_add_cprefix \
   "cmpl $0, %%fs:%P3\n\tje 0f\n\tlock\n0:\t"
 
-#define catomic_add(mem, value) \
-  __arch_add_body (__arch_add_cprefix, catomic, mem, value)
+#define orig_catomic_add(mem, value) \
+  orig___arch_add_body (__arch_add_cprefix, orig_catomic, mem, value)
 
 
-#define atomic_add_negative(mem, value) \
+#define orig_atomic_add_negative(mem, value) \
   ({ unsigned char __result;						      \
      if (sizeof (*mem) == 1)						      \
        __asm __volatile (LOCK_PREFIX "addb %b2, %0; sets %1"		      \
@@ -230,7 +230,7 @@
      __result; })
 
 
-#define atomic_add_zero(mem, value) \
+#define orig_atomic_add_zero(mem, value) \
   ({ unsigned char __result;						      \
      if (sizeof (*mem) == 1)						      \
        __asm __volatile (LOCK_PREFIX "addb %b2, %0; setz %1"		      \
@@ -252,7 +252,7 @@
      __result; })
 
 
-#define __arch_increment_body(lock, mem) \
+#define orig___arch_increment_body(lock, mem) \
   do {									      \
     if (sizeof (*mem) == 1)						      \
       __asm __volatile (lock "incb %b0"					      \
@@ -276,16 +276,16 @@
 			  "i" (offsetof (tcbhead_t, multiple_threads)));      \
   } while (0)
 
-#define atomic_increment(mem) __arch_increment_body (LOCK_PREFIX, mem)
+#define orig_atomic_increment(mem) orig___arch_increment_body (LOCK_PREFIX, mem)
 
 #define __arch_increment_cprefix \
   "cmpl $0, %%fs:%P2\n\tje 0f\n\tlock\n0:\t"
 
-#define catomic_increment(mem) \
-  __arch_increment_body (__arch_increment_cprefix, mem)
+#define orig_catomic_increment(mem) \
+  orig___arch_increment_body (__arch_increment_cprefix, mem)
 
 
-#define atomic_increment_and_test(mem) \
+#define orig_atomic_increment_and_test(mem) \
   ({ unsigned char __result;						      \
      if (sizeof (*mem) == 1)						      \
        __asm __volatile (LOCK_PREFIX "incb %b0; sete %1"		      \
@@ -306,7 +306,7 @@
      __result; })
 
 
-#define __arch_decrement_body(lock, mem) \
+#define orig___arch_decrement_body(lock, mem) \
   do {									      \
     if (sizeof (*mem) == 1)						      \
       __asm __volatile (lock "decb %b0"					      \
@@ -330,16 +330,16 @@
 			  "i" (offsetof (tcbhead_t, multiple_threads)));      \
   } while (0)
 
-#define atomic_decrement(mem) __arch_decrement_body (LOCK_PREFIX, mem)
+#define orig_atomic_decrement(mem) orig___arch_decrement_body (LOCK_PREFIX, mem)
 
 #define __arch_decrement_cprefix \
   "cmpl $0, %%fs:%P2\n\tje 0f\n\tlock\n0:\t"
 
-#define catomic_decrement(mem) \
-  __arch_decrement_body (__arch_decrement_cprefix, mem)
+#define orig_catomic_decrement(mem) \
+  orig___arch_decrement_body (__arch_decrement_cprefix, mem)
 
 
-#define atomic_decrement_and_test(mem) \
+#define orig_atomic_decrement_and_test(mem) \
   ({ unsigned char __result;						      \
      if (sizeof (*mem) == 1)						      \
        __asm __volatile (LOCK_PREFIX "decb %b0; sete %1"		      \
@@ -360,7 +360,7 @@
      __result; })
 
 
-#define atomic_bit_set(mem, bit) \
+#define orig_atomic_bit_set(mem, bit) \
   do {									      \
     if (sizeof (*mem) == 1)						      \
       __asm __volatile (LOCK_PREFIX "orb %b2, %0"			      \
@@ -385,7 +385,7 @@
   } while (0)
 
 
-#define atomic_bit_test_set(mem, bit) \
+#define orig_atomic_bit_test_set(mem, bit) \
   ({ unsigned char __result;						      \
      if (sizeof (*mem) == 1)						      \
        __asm __volatile (LOCK_PREFIX "btsb %3, %1; setc %0"		      \
@@ -409,7 +409,7 @@
 #define atomic_spin_nop() asm ("rep; nop")
 
 
-#define __arch_and_body(lock, mem, mask) \
+#define orig___arch_and_body(lock, mem, mask) \
   do {									      \
     if (sizeof (*mem) == 1)						      \
       __asm __volatile (lock "andb %b1, %0"				      \
@@ -436,12 +436,12 @@
 #define __arch_cprefix \
   "cmpl $0, %%fs:%P3\n\tje 0f\n\tlock\n0:\t"
 
-#define atomic_and(mem, mask) __arch_and_body (LOCK_PREFIX, mem, mask)
+#define orig_atomic_and(mem, mask) orig___arch_and_body (LOCK_PREFIX, mem, mask)
 
-#define catomic_and(mem, mask) __arch_and_body (__arch_cprefix, mem, mask)
+#define orig_catomic_and(mem, mask) orig___arch_and_body (__arch_cprefix, mem, mask)
 
 
-#define __arch_or_body(lock, mem, mask)					      \
+#define orig___arch_or_body(lock, mem, mask)					      \
   do {									      \
     if (sizeof (*mem) == 1)						      \
       __asm __volatile (lock "orb %b1, %0"				      \
@@ -465,9 +465,9 @@
 			  "i" (offsetof (tcbhead_t, multiple_threads)));      \
   } while (0)
 
-#define atomic_or(mem, mask) __arch_or_body (LOCK_PREFIX, mem, mask)
+#define orig_atomic_or(mem, mask) orig___arch_or_body (LOCK_PREFIX, mem, mask)
 
-#define catomic_or(mem, mask) __arch_or_body (__arch_cprefix, mem, mask)
+#define orig_catomic_or(mem, mask) orig___arch_or_body (__arch_cprefix, mem, mask)
 
 /* We don't use mfence because it is supposedly slower due to having to
    provide stronger guarantees (e.g., regarding self-modifying code).  */
@@ -475,3 +475,904 @@
     __asm __volatile (LOCK_PREFIX "orl $0, (%%rsp)" ::: "memory")
 #define atomic_read_barrier() __asm ("" ::: "memory")
 #define atomic_write_barrier() __asm ("" ::: "memory")
+
+/*--------------------------------------------------------------------------------
+                                  MVEE PATCHES
+--------------------------------------------------------------------------------*/
+#define USE_MVEE_LIBC
+
+#ifdef USE_MVEE_LIBC
+#define MVEE_FAKE_SYSCALL_BASE          0x6FFFFFFF
+#define MVEE_GET_MASTERTHREAD_ID        MVEE_FAKE_SYSCALL_BASE + 3
+#define MVEE_GET_SHARED_BUFFER          MVEE_FAKE_SYSCALL_BASE + 4
+#define MVEE_FLUSH_SHARED_BUFFER        MVEE_FAKE_SYSCALL_BASE + 5
+#define MVEE_SET_INFINITE_LOOP_PTR      MVEE_FAKE_SYSCALL_BASE + 6
+#define MVEE_TOGGLESYNC                 MVEE_FAKE_SYSCALL_BASE + 7
+#define MVEE_SET_SHARED_BUFFER_POS_PTR  MVEE_FAKE_SYSCALL_BASE + 8
+#define MVEE_RUNS_UNDER_MVEE_CONTROL    MVEE_FAKE_SYSCALL_BASE + 9
+#define MVEE_GET_THREAD_NUM             MVEE_FAKE_SYSCALL_BASE + 10
+#define MVEE_SET_SYNC_PRIMITIVES_PTR    MVEE_FAKE_SYSCALL_BASE + 12
+#define MVEE_ALL_HEAPS_ALIGNED          MVEE_FAKE_SYSCALL_BASE + 13
+#define MVEE_GET_VIRTUALIZED_ARGV0      MVEE_FAKE_SYSCALL_BASE + 17
+#define MVEE_LIBC_LOCK_BUFFER           3
+#define MVEE_LIBC_MALLOC_DEBUG_BUFFER   11
+#define MVEE_LIBC_ATOMIC_BUFFER         13
+#define MVEE_LIBC_LOCK_BUFFER_PARTIAL   16
+#define MVEE_FUTEX_WAIT_TID             30
+
+enum mvee_alloc_types
+  {
+  LIBC_MALLOC,
+  LIBC_FREE,
+  LIBC_REALLOC,
+  LIBC_MEMALIGN,
+  LIBC_CALLOC,
+  MALLOC_TRIM,
+  HEAP_TRIM,
+  MALLOC_CONSOLIDATE,
+  ARENA_GET2,
+  _INT_MALLOC,
+  _INT_FREE,
+  _INT_REALLOC
+  };
+
+//
+// Atomic operations list. Keep this in sync with MVEE/Inc/MVEE_shm.h
+//
+enum mvee_base_atomics
+{
+    // LOAD OPERATIONS FIRST!!! DO NOT CHANGE THIS CONVENTION
+    ATOMIC_FORCED_READ,
+    ATOMIC_LOAD,
+    // THE FOLLOWING IS NOT AN ACTUAL ATOMIC OPERATION, IT JUST DENOTES THE END OF THE LOAD-ONLY ATOMICS!!!
+    ATOMIC_LOAD_MAX,
+    // STORES AFTER LOADS
+    CATOMIC_AND,
+    CATOMIC_OR,
+    CATOMIC_EXCHANGE_AND_ADD,
+    CATOMIC_ADD,
+    CATOMIC_INCREMENT,
+    CATOMIC_DECREMENT,
+    CATOMIC_MAX,
+    ATOMIC_COMPARE_AND_EXCHANGE_VAL,
+    ATOMIC_COMPARE_AND_EXCHANGE_BOOL,
+    ATOMIC_EXCHANGE,
+    ATOMIC_EXCHANGE_AND_ADD,
+    ATOMIC_INCREMENT_AND_TEST,
+    ATOMIC_DECREMENT_AND_TEST,
+	ATOMIC_ADD_NEGATIVE,
+    ATOMIC_ADD_ZERO,
+    ATOMIC_ADD,
+	ATOMIC_OR,
+	ATOMIC_OR_VAL,
+    ATOMIC_INCREMENT,
+    ATOMIC_DECREMENT,
+    ATOMIC_BIT_TEST_SET,
+    ATOMIC_BIT_SET,
+    ATOMIC_AND,
+	ATOMIC_AND_VAL,
+    ATOMIC_STORE,
+	ATOMIC_MIN,
+    ATOMIC_MAX,
+    ATOMIC_DECREMENT_IF_POSITIVE,
+	ATOMIC_FETCH_ADD,
+	ATOMIC_FETCH_AND,
+	ATOMIC_FETCH_OR,
+	ATOMIC_FETCH_XOR,
+    __THREAD_ATOMIC_CMPXCHG_VAL,
+    __THREAD_ATOMIC_AND,
+    __THREAD_ATOMIC_BIT_SET,
+    ___UNKNOWN_LOCK_TYPE___,
+    __MVEE_BASE_ATOMICS_MAX__
+};
+
+enum mvee_extended_atomics {
+  mvee_atomic_load_n,
+  mvee_atomic_load,
+  mvee_atomic_store_n,
+  mvee_atomic_store,
+  mvee_atomic_exchange_n,
+  mvee_atomic_exchange,
+  mvee_atomic_compare_exchange_n,
+  mvee_atomic_compare_exchange,
+  mvee_atomic_add_fetch,
+  mvee_atomic_sub_fetch,
+  mvee_atomic_and_fetch,
+  mvee_atomic_xor_fetch,
+  mvee_atomic_or_fetch,
+  mvee_atomic_nand_fetch,
+  mvee_atomic_fetch_add,
+  mvee_atomic_fetch_sub,
+  mvee_atomic_fetch_and,
+  mvee_atomic_fetch_xor,
+  mvee_atomic_fetch_or,
+  mvee_atomic_fetch_nand,
+  mvee_atomic_test_and_set,
+  mvee_atomic_clear,
+  mvee_atomic_always_lock_free,
+  mvee_atomic_is_lock_free,
+  mvee_sync_fetch_and_add,
+  mvee_sync_fetch_and_sub,
+  mvee_sync_fetch_and_or,
+  mvee_sync_fetch_and_and,
+  mvee_sync_fetch_and_xor,
+  mvee_sync_fetch_and_nand,
+  mvee_sync_add_and_fetch,
+  mvee_sync_sub_and_fetch,
+  mvee_sync_or_and_fetch,
+  mvee_sync_and_and_fetch,
+  mvee_sync_xor_and_fetch,
+  mvee_sync_nand_and_fetch,
+  mvee_sync_bool_compare_and_swap,
+  mvee_sync_val_compare_and_swap,
+  mvee_sync_lock_test_and_set,
+  mvee_sync_lock_release,
+  mvee_atomic_ops_max
+};
+
+#define MVEE_ROUND_UP(x, multiple)		\
+  ((x + (multiple - 1)) & ~(multiple -1))
+
+#define MVEE_MIN(a, b) ((a > b) ? (b) : (a))
+
+# ifdef MVEE_USE_TOTALPARTIAL_AGENT
+#  include "mvee-totalpartial-agent.h"
+# else
+#  include "mvee-woc-agent.h"
+# endif
+
+#endif
+
+// We don't use our sync agent in the dynamic loader so just use the original atomics everywhere
+#if IS_IN (rtld) || !defined USE_MVEE_LIBC
+
+//
+// generic atomics (include/atomic.h and sysdeps/arch/atomic-machine.h)
+//
+#define __arch_c_compare_and_exchange_val_8_acq(mem, newval, oldval) orig___arch_c_compare_and_exchange_val_8_acq(mem, newval, oldval)
+#define __arch_c_compare_and_exchange_val_16_acq(mem, newval, oldval) orig___arch_c_compare_and_exchange_val_16_acq(mem, newval, oldval)
+#define __arch_c_compare_and_exchange_val_32_acq(mem, newval, oldval) orig___arch_c_compare_and_exchange_val_32_acq(mem, newval, oldval)
+#define __arch_c_compare_and_exchange_val_64_acq(mem, newval, oldval) orig___arch_c_compare_and_exchange_val_64_acq(mem, newval, oldval)
+#define atomic_compare_and_exchange_val_acq(mem, newval, oldval) orig_atomic_compare_and_exchange_val_acq(mem, newval, oldval)
+#define atomic_compare_and_exchange_val_rel(mem, newval, oldval) orig_atomic_compare_and_exchange_val_rel(mem, newval, oldval)
+#define atomic_compare_and_exchange_bool_acq(mem, newval, oldval) orig_atomic_compare_and_exchange_bool_acq(mem, newval, oldval)
+#define atomic_compare_and_exchange_bool_rel(mem, newval, oldval) orig_atomic_compare_and_exchange_bool_rel(mem, newval, oldval)
+#define atomic_exchange_acq(mem, newvalue) orig_atomic_exchange_acq(mem, newvalue)
+#define atomic_exchange_rel(mem, newvalue) orig_atomic_exchange_rel(mem, newvalue)
+#define atomic_exchange_and_add(mem, value) orig_atomic_exchange_and_add(mem, value)
+#define atomic_exchange_and_add_acq(mem, value) orig_atomic_exchange_and_add_acq(mem, value)
+#define atomic_exchange_and_add_rel(mem, value) orig_atomic_exchange_and_add_rel(mem, value)
+#define atomic_add(mem, value) orig_atomic_add(mem, value)
+#define atomic_increment(mem) orig_atomic_increment(mem)
+#define atomic_increment_and_test(mem) orig_atomic_increment_and_test(mem)
+#define atomic_increment_val(mem) orig_atomic_increment_val(mem)
+#define atomic_decrement(mem) orig_atomic_decrement(mem)
+#define atomic_decrement_and_test(mem) orig_atomic_decrement_and_test(mem)
+#define atomic_decrement_val(mem) orig_atomic_decrement_val(mem)
+#define atomic_add_negative(mem, value) orig_atomic_add_negative(mem, value)
+#define atomic_add_zero(mem, value) orig_atomic_add_zero(mem, value)
+#define atomic_bit_set(mem, bit) orig_atomic_bit_set(mem, bit)
+#define atomic_bit_test_set(mem, bit) orig_atomic_bit_test_set(mem, bit)
+#define atomic_and(mem, mask) orig_atomic_and(mem, mask)
+#define atomic_or(mem, mask) orig_atomic_or(mem, mask)
+#define atomic_max(mem, value) orig_atomic_max(mem, value)
+#define atomic_min(mem, value) orig_atomic_min(mem, value)
+#define atomic_decrement_if_positive(mem) orig_atomic_decrement_if_positive(mem)
+#define atomic_and_val(mem, mask) orig_atomic_and_val(mem, mask)
+#define atomic_or_val(mem, mask) orig_atomic_or_val(mem, mask)
+#define atomic_forced_read(x) orig_atomic_forced_read(x)
+#define catomic_compare_and_exchange_val_acq(mem, newval, oldval) orig_catomic_compare_and_exchange_val_acq(mem, newval, oldval)
+#define catomic_compare_and_exchange_val_rel(mem, newval, oldval) orig_catomic_compare_and_exchange_val_rel(mem, newval, oldval)
+#define catomic_compare_and_exchange_bool_acq(mem, newval, oldval) orig_catomic_compare_and_exchange_bool_acq(mem, newval, oldval)
+#define catomic_compare_and_exchange_bool_rel(mem, newval, oldval) orig_catomic_compare_and_exchange_bool_rel(mem, newval, oldval)
+#define catomic_exchange_and_add(mem, value) orig_catomic_exchange_and_add(mem, value)
+#define catomic_add(mem, value) orig_catomic_add(mem, value)
+#define catomic_increment(mem) orig_catomic_increment(mem)
+#define catomic_increment_val(mem) orig_catomic_increment_val(mem)
+#define catomic_decrement(mem) orig_catomic_decrement(mem)
+#define catomic_decrement_val(mem) orig_catomic_decrement_val(mem)
+#define catomic_and(mem, mask) orig_catomic_and(mem, mask)
+#define catomic_or(mem, mask) orig_catomic_or(mem, mask)
+#define catomic_max(mem, value) orig_catomic_max(mem, value)
+
+//
+// C11-style atomics (include/atomic.h)
+//
+#define atomic_load_relaxed(mem) orig_atomic_load_relaxed(mem)
+#define atomic_load_acquire(mem) orig_atomic_load_acquire(mem)
+#define atomic_store_relaxed(mem, val) orig_atomic_store_relaxed(mem, val)
+#define atomic_store_release(mem, val) orig_atomic_store_release(mem, val)
+#define atomic_compare_exchange_weak_relaxed(mem, expected, desired) orig_atomic_compare_exchange_weak_relaxed(mem, expected, desired)
+#define atomic_compare_exchange_weak_acquire(mem, expected, desired) orig_atomic_compare_exchange_weak_acquire(mem, expected, desired) 
+#define atomic_compare_exchange_weak_release(mem, expected, desired) orig_atomic_compare_exchange_weak_release(mem, expected, desired)
+#define atomic_exchange_relaxed(mem, desired) orig_atomic_exchange_relaxed(mem, desired)
+#define atomic_exchange_acquire(mem, desired) orig_atomic_exchange_acquire(mem, desired)
+#define atomic_exchange_release(mem, desired) orig_atomic_exchange_release(mem, desired)
+#define atomic_fetch_add_relaxed(mem, operand) orig_atomic_fetch_add_relaxed(mem, operand)
+#define atomic_fetch_add_acquire(mem, operand) orig_atomic_fetch_add_acquire(mem, operand)
+#define atomic_fetch_add_release(mem, operand) orig_atomic_fetch_add_release(mem, operand)
+#define atomic_fetch_add_acq_rel(mem, operand) orig_atomic_fetch_add_acq_rel(mem, operand)
+#define atomic_fetch_and_relaxed(mem, operand) orig_atomic_fetch_and_relaxed(mem, operand)
+#define atomic_fetch_and_acquire(mem, operand) orig_atomic_fetch_and_acquire(mem, operand)
+#define atomic_fetch_and_release(mem, operand) orig_atomic_fetch_and_release(mem, operand)
+#define atomic_fetch_or_relaxed(mem, operand) orig_atomic_fetch_or_relaxed(mem, operand) 
+#define atomic_fetch_or_acquire(mem, operand) orig_atomic_fetch_or_acquire(mem, operand) 
+#define atomic_fetch_or_release(mem, operand) orig_atomic_fetch_or_release(mem, operand) 
+#define atomic_fetch_xor_release(mem, operand) orig_atomic_fetch_xor_release(mem, operand) 
+
+//
+// TLS atomics (tls.h)
+//
+#define THREAD_ATOMIC_CMPXCHG_VAL(descr, member, newval, oldval) orig_THREAD_ATOMIC_CMPXCHG_VAL(descr, member, newval, oldval)
+#define THREAD_ATOMIC_AND(descr, member, val) orig_THREAD_ATOMIC_AND(descr, member, val)
+#define THREAD_ATOMIC_BIT_SET(descr, member, bit) orig_THREAD_ATOMIC_BIT_SET(descr, member, bit)
+
+//
+// MVEE additions
+//
+#define atomic_load(var) ({ var; })
+#define atomic_store(var, value) ({ var = value; })
+#define THREAD_ATOMIC_GETMEM(descr, member) THREAD_GETMEM(descr, member)
+#define THREAD_ATOMIC_SETMEM(descr, member, val) THREAD_SETMEM(descr, member, val)
+
+
+#else // !IS_IN_rtld
+
+//
+// architecture-specific atomics (atomic-machine.h)
+//
+#define __arch_c_compare_and_exchange_val_8_acq(mem, newval, oldval)	\
+	({																	\
+		typeof(*mem) ____result;										\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		____result = orig___arch_c_compare_and_exchange_val_8_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define __arch_c_compare_and_exchange_val_16_acq(mem, newval, oldval)	\
+	({																	\
+		typeof(*mem) ____result;										\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		____result = orig___arch_c_compare_and_exchange_val_16_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define __arch_c_compare_and_exchange_val_32_acq(mem, newval, oldval)	\
+	({																	\
+		typeof(*mem) ____result;										\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		____result = orig___arch_c_compare_and_exchange_val_32_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define __arch_c_compare_and_exchange_val_64_acq(mem, newval, oldval)	\
+	({																	\
+		typeof(*mem) ____result;										\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		____result = orig___arch_c_compare_and_exchange_val_64_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define atomic_compare_and_exchange_val_acq(mem, newval, oldval)		\
+	({																	\
+		typeof(*mem) ____result;										\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		____result = orig_atomic_compare_and_exchange_val_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define atomic_compare_and_exchange_val_rel(mem, newval, oldval)		\
+	({																	\
+		typeof(*mem) ____result;										\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		____result = orig_atomic_compare_and_exchange_val_rel(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define atomic_compare_and_exchange_bool_acq(mem, newval, oldval)		\
+	({																	\
+		bool ____result;												\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		____result = orig_atomic_compare_and_exchange_bool_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define atomic_compare_and_exchange_bool_rel(mem, newval, oldval)		\
+	({																	\
+		bool ____result;												\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		____result = orig_atomic_compare_and_exchange_bool_rel(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define atomic_exchange_acq(mem, newvalue)						\
+	({															\
+		typeof(*mem) ____result;								\
+		MVEE_PREOP(ATOMIC_EXCHANGE, mem, 1);					\
+		____result = orig_atomic_exchange_acq(mem, newvalue);	\
+		MVEE_POSTOP();											\
+		____result;												\
+	})
+
+#define atomic_exchange_rel(mem, newvalue)						\
+	({															\
+		typeof(*mem) ____result;								\
+		MVEE_PREOP(ATOMIC_EXCHANGE, mem, 1);					\
+		____result = orig_atomic_exchange_rel(mem, newvalue);	\
+		MVEE_POSTOP();											\
+		____result;												\
+	})
+
+#define atomic_exchange_and_add(mem, value)						\
+	({															\
+		typeof(*mem) ____result;								\
+		MVEE_PREOP(ATOMIC_EXCHANGE_AND_ADD, mem, 1);			\
+		____result = orig_atomic_exchange_and_add(mem, value);	\
+		MVEE_POSTOP();											\
+		____result;												\
+	})
+
+#define atomic_exchange_and_add_acq(mem, value)					\
+	({															\
+		typeof(*mem) ____result;								\
+		MVEE_PREOP(ATOMIC_EXCHANGE_AND_ADD, mem, 1);			\
+		____result = orig_atomic_exchange_and_add_acq(mem, value);	\
+		MVEE_POSTOP();											\
+		____result;												\
+	})
+
+#define atomic_exchange_and_add_rel(mem, value)					\
+	({															\
+		typeof(*mem) ____result;								\
+		MVEE_PREOP(ATOMIC_EXCHANGE_AND_ADD, mem, 1);			\
+		____result = orig_atomic_exchange_and_add_rel(mem, value);	\
+		MVEE_POSTOP();											\
+		____result;												\
+	})
+
+#define atomic_add(mem, value)					\
+	({											\
+		MVEE_PREOP(ATOMIC_ADD, mem, 1);			\
+		orig_atomic_add(mem, value);			\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_increment(mem)					\
+	({											\
+		MVEE_PREOP(ATOMIC_INCREMENT, mem, 1);	\
+		orig_atomic_increment(mem);				\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_increment_and_test(mem)						\
+	({														\
+		unsigned char ____result;							\
+		MVEE_PREOP(ATOMIC_INCREMENT_AND_TEST, mem, 1);		\
+		____result = orig_atomic_increment_and_test(mem);	\
+		MVEE_POSTOP();										\
+		____result;											\
+	})
+
+#define atomic_increment_val(mem)				\
+	({											\
+		typeof(*mem) ____result;				\
+		MVEE_PREOP(ATOMIC_INCREMENT, mem, 1);	\
+		____result = orig_atomic_increment_val(mem);	\
+		MVEE_POSTOP();							\
+		____result;								\
+	})
+
+#define atomic_decrement(mem)					\
+	({											\
+		MVEE_PREOP(ATOMIC_DECREMENT, mem, 1);	\
+		orig_atomic_decrement(mem);				\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_decrement_and_test(mem)						\
+	({														\
+		unsigned char ____result;							\
+		MVEE_PREOP(ATOMIC_DECREMENT_AND_TEST, mem, 1);		\
+		____result = orig_atomic_decrement_and_test(mem);	\
+		MVEE_POSTOP();										\
+		____result;											\
+	})
+
+#define atomic_decrement_val(mem)				\
+	({											\
+		typeof(*mem) ____result;				\
+		MVEE_PREOP(ATOMIC_DECREMENT, mem, 1);	\
+		____result = orig_atomic_decrement_val(mem);	\
+		MVEE_POSTOP();							\
+		____result;								\
+	})
+
+#define atomic_add_negative(mem, value)						\
+	({														\
+		unsigned char ____result;							\
+		MVEE_PREOP(ATOMIC_ADD, mem, 1);						\
+		____result = orig_atomic_add_negative(mem, value);	\
+		MVEE_POSTOP();										\
+		____result;											\
+	})
+
+#define atomic_add_zero(mem, value)						\
+	({													\
+		unsigned char ____result;						\
+		MVEE_PREOP(ATOMIC_ADD_ZERO, mem, 1);			\
+		____result = orig_atomic_add_zero(mem, value);	\
+		MVEE_POSTOP();									\
+		____result;										\
+	})
+
+#define atomic_bit_set(mem, bit)				\
+	({											\
+		MVEE_PREOP(ATOMIC_BIT_SET, mem, 1);		\
+		orig_atomic_bit_set(mem, bit);			\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_bit_test_set(mem, bit)						\
+	({														\
+		unsigned char ____result;							\
+		MVEE_PREOP(ATOMIC_BIT_TEST_SET, mem, 1);			\
+		____result = orig_atomic_bit_test_set(mem, bit);	\
+		MVEE_POSTOP();										\
+		____result;											\
+	})
+
+#define atomic_and(mem, mask)					\
+	({											\
+		MVEE_PREOP(ATOMIC_AND, mem, 1);			\
+		orig_atomic_and(mem, mask);				\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_or(mem, mask)					\
+	({											\
+		MVEE_PREOP(ATOMIC_OR, mem, 1);			\
+		orig_atomic_or(mem, mask);				\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_max(mem, value)					\
+	({											\
+		MVEE_PREOP(ATOMIC_MAX, mem, 1);			\
+		orig_atomic_max(mem, value);			\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_min(mem, value)					\
+	({											\
+		MVEE_PREOP(ATOMIC_MIN, mem, 1);			\
+		orig_atomic_max(mem, value);			\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_decrement_if_positive(mem)					\
+	({														\
+		__typeof(*mem) __result;							\
+		MVEE_PREOP(ATOMIC_DECREMENT_IF_POSITIVE, mem, 1);	\
+		__result = orig_atomic_decrement_if_positive(mem);	\
+		MVEE_POSTOP();										\
+		__result;											\
+	})
+
+#define atomic_and_val(mem, mask)							\
+	({														\
+		__typeof(*mem) __result;							\
+		MVEE_PREOP(ATOMIC_AND_VAL, mem, 1);					\
+		__result = orig_atomic_and_val(mem);				\
+		MVEE_POSTOP();										\
+		__result;											\
+	})
+
+#define atomic_or_val(mem, mask)							\
+	({														\
+		__typeof(*mem) __result;							\
+		MVEE_PREOP(ATOMIC_OR_VAL, mem, 1);					\
+		__result = orig_atomic_or_val(mem);					\
+		MVEE_POSTOP();										\
+		__result;											\
+	})
+
+#define atomic_forced_read(x)						\
+	({												\
+		typeof(x) ____result;						\
+		MVEE_PREOP(ATOMIC_FORCED_READ, &x, 0);		\
+		____result = orig_atomic_forced_read(x);	\
+		MVEE_POSTOP();								\
+		____result;									\
+	})
+
+#define catomic_compare_and_exchange_val_acq(mem, newval, oldval)		\
+	({																	\
+		typeof(*mem) ____result;										\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		____result = orig_catomic_compare_and_exchange_val_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define catomic_compare_and_exchange_val_rel(mem, newval, oldval)		\
+	({																	\
+		typeof(*mem) ____result;										\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_VAL, mem, 1);			\
+		____result = orig_catomic_compare_and_exchange_val_rel(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define catomic_compare_and_exchange_bool_acq(mem, newval, oldval)		\
+	({																	\
+		bool ____result;												\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		____result = orig_catomic_compare_and_exchange_bool_acq(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define catomic_compare_and_exchange_bool_rel(mem, newval, oldval)		\
+	({																	\
+		bool ____result;												\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		____result = orig_catomic_compare_and_exchange_bool_rel(mem, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+#define catomic_exchange_and_add(mem, value)					\
+	({															\
+		typeof(*mem) ____result;								\
+		MVEE_PREOP(CATOMIC_EXCHANGE_AND_ADD, mem, 1);			\
+		____result = orig_catomic_exchange_and_add(mem, value);	\
+		MVEE_POSTOP();											\
+		____result;												\
+	})
+
+#define catomic_add(mem, value)					\
+	({											\
+		MVEE_PREOP(CATOMIC_ADD, mem, 1);		\
+		orig_catomic_add(mem, value);			\
+		MVEE_POSTOP();							\
+	})
+
+#define catomic_increment(mem)					\
+	({											\
+		MVEE_PREOP(CATOMIC_INCREMENT, mem, 1);	\
+		orig_catomic_increment(mem);			\
+		MVEE_POSTOP();							\
+	})
+
+#define catomic_increment_val(mem)						\
+	({													\
+		typeof(*mem) ____result;						\
+		MVEE_PREOP(CATOMIC_INCREMENT, mem, 1);			\
+		____result = orig_catomic_increment_val(mem);	\
+		MVEE_POSTOP();									\
+		____result;										\
+	})
+
+#define catomic_decrement(mem)					\
+	({											\
+		MVEE_PREOP(CATOMIC_DECREMENT, mem, 1);	\
+		orig_catomic_decrement(mem);			\
+		MVEE_POSTOP();							\
+	})
+
+#define catomic_decrement_val(mem)						\
+	({													\
+		typeof(*mem) ____result;						\
+		MVEE_PREOP(CATOMIC_DECREMENT, mem, 1);			\
+		____result = orig_catomic_decrement_val(mem);	\
+		MVEE_POSTOP();									\
+		____result;										\
+	})
+
+
+#define catomic_and(mem, mask)					\
+	({											\
+		MVEE_PREOP(CATOMIC_AND, mem, 1);		\
+		orig_catomic_and(mem, mask);			\
+		MVEE_POSTOP();							\
+	})
+
+#define catomic_or(mem, mask)					\
+	({											\
+		MVEE_PREOP(CATOMIC_OR, mem, 1);			\
+		orig_catomic_or(mem, mask);				\
+		MVEE_POSTOP();							\
+	})
+
+#define catomic_max(mem, value)					\
+	({											\
+		MVEE_PREOP(CATOMIC_MAX, mem, 1);		\
+		orig_catomic_max(mem, value);			\
+		MVEE_POSTOP();							\
+	})
+
+
+//
+// generic C11-style atomics (include/atomic.h)
+//
+#define atomic_load_relaxed(mem)					\
+	({												\
+		__typeof(*mem) ____result;					\
+		MVEE_PREOP(ATOMIC_LOAD, mem, 0);			\
+		____result = orig_atomic_load_relaxed(mem);	\
+		MVEE_POSTOP();								\
+		____result;									\
+	})
+
+#define atomic_load_acquire(mem)					\
+	({												\
+		__typeof(*mem) ____result;					\
+		MVEE_PREOP(ATOMIC_LOAD, mem, 0);			\
+		____result = orig_atomic_load_acquire(mem);	\
+		MVEE_POSTOP();								\
+		____result;									\
+	})
+
+#define atomic_store_relaxed(mem, val)			\
+	(void)({									\
+		MVEE_PREOP(ATOMIC_STORE, mem, 1);		\
+		orig_atomic_store_relaxed(mem, val);	\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_store_release(mem, val)			\
+	(void)({									\
+		MVEE_PREOP(ATOMIC_STORE, mem, 1);		\
+		orig_atomic_store_release(mem, val);	\
+		MVEE_POSTOP();							\
+	})
+
+#define atomic_compare_exchange_weak_relaxed(mem, expected, desired)	\
+	({																	\
+		bool __result;													\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		__result = orig_atomic_compare_exchange_weak_relaxed(mem, expected, desired); \
+		MVEE_POSTOP();													\
+		__result;														\
+	})
+
+#define atomic_compare_exchange_weak_acquire(mem, expected, desired)	\
+	({																	\
+		bool __result;													\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		__result = orig_atomic_compare_exchange_weak_acquire(mem, expected, desired); \
+		MVEE_POSTOP();													\
+		__result;														\
+	})
+
+#define atomic_compare_exchange_weak_release(mem, expected, desired)	\
+	({																	\
+		bool __result;													\
+		MVEE_PREOP(ATOMIC_COMPARE_AND_EXCHANGE_BOOL, mem, 1);			\
+		__result = orig_atomic_compare_exchange_weak_release(mem, expected, desired); \
+		MVEE_POSTOP();													\
+		__result;														\
+	})
+
+#define atomic_exchange_relaxed(mem, desired)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_EXCHANGE, mem, 1);						\
+		____result = orig_atomic_exchange_relaxed(mem, desired);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_exchange_acquire(mem, desired)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_EXCHANGE, mem, 1);						\
+		____result = orig_atomic_exchange_acquire(mem, desired);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_exchange_release(mem, desired)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_EXCHANGE, mem, 1);						\
+		____result = orig_atomic_exchange_release(mem, desired);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_add_relaxed(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_ADD, mem, 1);						\
+		____result = orig_atomic_fetch_add_relaxed(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_add_acquire(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_ADD, mem, 1);						\
+		____result = orig_atomic_fetch_add_acquire(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_add_release(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_ADD, mem, 1);						\
+		____result = orig_atomic_fetch_add_release(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_add_acq_rel(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_ADD, mem, 1);						\
+		____result = orig_atomic_fetch_add_acq_rel(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_and_relaxed(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_AND, mem, 1);						\
+		____result = orig_atomic_fetch_and_relaxed(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_and_acquire(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_AND, mem, 1);						\
+		____result = orig_atomic_fetch_and_acquire(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_and_release(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_AND, mem, 1);						\
+		____result = orig_atomic_fetch_and_release(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+
+#define atomic_fetch_or_relaxed(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_OR, mem, 1);						\
+		____result = orig_atomic_fetch_or_relaxed(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_or_acquire(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_OR, mem, 1);						\
+		____result = orig_atomic_fetch_or_acquire(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_or_release(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_OR, mem, 1);						\
+		____result = orig_atomic_fetch_or_release(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+#define atomic_fetch_xor_release(mem, operand)						\
+	({																\
+		typeof(*mem) ____result;									\
+		MVEE_PREOP(ATOMIC_FETCH_XOR, mem, 1);						\
+		____result = orig_atomic_fetch_xor_release(mem, operand);	\
+		MVEE_POSTOP();												\
+		____result;													\
+	})
+
+//
+// TLS atomics (tls.h)
+//
+#define THREAD_ATOMIC_CMPXCHG_VAL(descr, member, newval, oldval)		\
+	({																	\
+		__typeof(descr->member) ____result;								\
+		MVEE_PREOP(__THREAD_ATOMIC_CMPXCHG_VAL, &descr->member, 1);		\
+		____result = orig_THREAD_ATOMIC_CMPXCHG_VAL(descr, member, newval, oldval); \
+		MVEE_POSTOP();													\
+		____result;														\
+	})
+
+
+#define THREAD_ATOMIC_AND(descr, member, val)					\
+	(void)({													\
+			MVEE_PREOP(__THREAD_ATOMIC_AND, &descr->member, 1);	\
+			orig_THREAD_ATOMIC_AND(descr, member, val);			\
+			MVEE_POSTOP();										\
+		})
+
+
+#define THREAD_ATOMIC_BIT_SET(descr, member, bit)					\
+	(void)({														\
+			MVEE_PREOP(__THREAD_ATOMIC_BIT_SET, &descr->member, 1);	\
+			orig_THREAD_ATOMIC_BIT_SET(descr, member, bit);			\
+			MVEE_POSTOP();											\
+		})
+
+//
+// MVEE additions
+//
+#define atomic_load(var)						\
+	({											\
+		__typeof(var+0) ____result;				\
+		MVEE_PREOP(ATOMIC_LOAD, (void*)&var, 0);	\
+		____result = var;						\
+		MVEE_POSTOP();							\
+		____result;								\
+	})
+
+#define atomic_store(var, val)					\
+	(void)({									\
+		MVEE_PREOP(ATOMIC_STORE, &var, 1);		\
+		var = val;								\
+		MVEE_POSTOP();							\
+	})
+
+#define THREAD_ATOMIC_GETMEM(descr, member)			\
+	({												\
+		__typeof(descr->member) ____result;			\
+		MVEE_PREOP(ATOMIC_LOAD, &descr->member, 1);	\
+		____result = THREAD_GETMEM(descr, member);	\
+		MVEE_POSTOP();								\
+		____result;									\
+	})
+
+#define THREAD_ATOMIC_SETMEM(descr, member, val)			\
+	(void)({												\
+			MVEE_PREOP(ATOMIC_STORE, &descr->member, 1);	\
+			THREAD_SETMEM(descr, member, val);				\
+			MVEE_POSTOP();									\
+		})
+
+//
+// sys_futex with FUTEX_WAKE_OP usually overwrites the value of the futex.
+// We have to make sure that we include the futex write in our sync buf ordering
+//
+#define lll_futex_wake_unlock(futexp, nr_wake, nr_wake2, futexp2, private) \
+	({																	\
+		INTERNAL_SYSCALL_DECL (__err);									\
+		long int __ret;													\
+		MVEE_PREOP(___UNKNOWN_LOCK_TYPE___, futexp2, 1);				\
+		__ret = INTERNAL_SYSCALL (futex, __err, 6, (futexp),			\
+								  __lll_private_flag (FUTEX_WAKE_OP, private), \
+								  (nr_wake), (nr_wake2), (futexp2),		\
+								  FUTEX_OP_CLEAR_WAKE_IF_GT_ONE);		\
+		if (mvee_should_futex_unlock())									\
+		{																\
+			*futexp2 = 0;												\
+		}																\
+		MVEE_POSTOP();													\
+		INTERNAL_SYSCALL_ERROR_P (__ret, __err);						\
+	})
+
+#endif // !IS_IN (rtld)
+
+
diff -N -r --unified glibc-2.25.orig/sysdeps/x86_64/mvee-totalpartial-agent.h glibc-2.25/sysdeps/x86_64/mvee-totalpartial-agent.h
--- glibc-2.25.orig/sysdeps/x86_64/mvee-totalpartial-agent.h	1970-01-01 00:00:00.000000000 +0000
+++ glibc-2.25/sysdeps/x86_64/mvee-totalpartial-agent.h	2017-09-13 19:55:56.058750758 +0000
@@ -0,0 +1,134 @@
+//
+// MVEE_PARTIAL_ORDER_REPLICATION: when defined, slaves will use
+// queue projection to replay synchronization operations in
+// partial order rather than total order. In other words,
+// the slaves will only respect the order in which the master
+// has performed its synchronization operations on a per-word
+// basis
+//
+#define MVEE_PARTIAL_ORDER_REPLICATION
+//
+// MVEE_EXTENDED_QUEUE: when defined, the locking operation and
+// mutex pointer are also logged in the queue.
+//
+#define MVEE_EXTENDED_QUEUE
+//
+// MVEE_LOG_EIPS: when defined, libc logs return addresses for
+// all locking operations into a seperate queue
+//
+// WARNING: enabling EIP logging _CAN_ trigger crashes! We're
+// using __builtin_return_address(2) to fetch the eip of the 
+// caller of the locking function. Unfortunately, libc uses inline
+// __libc_lock_* operations every now and then. When it does, 
+// the __builtin_... call will return the wrong caller and in some
+// cases (e.g. in do_system) it might try to fetch the eip beyond
+// the end of the stack!
+//
+#define MVEE_LOG_EIPS
+#define MVEE_STACK_DEPTH 5
+//
+// MVEE_CHECK_LOCK_TYPE: if this is defined, the slave will check
+// whether or not it's replaying a lock of the same type
+// (only works with the extended queue)
+//
+#define MVEE_CHECK_LOCK_TYPE
+//
+// MVEE_DEBUG_MALLOC: if this is defined, the slaves will check whether
+// their malloc behavior is synced with the master
+//
+// #define MVEE_DEBUG_MALLOC
+//
+// MVEE_MALLOC_IGNORE_ASLR: if this is defined, the malloc debugger will
+// only compare allocation types, messages and chunk sizes
+// arena and chunk pointers are ignored.
+// #define MVEE_MALLOC_IGNORE_ASLR
+
+#define DEFINE_MVEE_QUEUE(name, has_eip_queue)				\
+  static unsigned long             mvee_##name##_buffer_data_start  = 0; \
+  static volatile unsigned int*    mvee_##name##_buffer_pos         = 0; \
+  static volatile unsigned int*    mvee_##name##_buffer_lock        = 0; \
+  static volatile unsigned int*    mvee_##name##_buffer_flush_cnt   = 0; \
+  static volatile unsigned char*   mvee_##name##_buffer_flushing    = 0; \
+  static unsigned long             mvee_##name##_buffer_slots       = 0; \
+  static void*                     mvee_##name##_eip_buffer         = NULL; \
+  static unsigned char             mvee_##name##_buffer_log_eips    = has_eip_queue;
+
+// this is extremely wasteful but required to prevent false sharing in the producer...
+#ifdef MVEE_PARTIAL_ORDER_REPLICATION
+# ifdef MVEE_EXTENDED_QUEUE
+      #define MVEE_LOCK_QUEUE_SLOT_SIZE (sizeof(long) + sizeof(short) * (mvee_num_childs + 1))
+# else
+      #define MVEE_LOCK_QUEUE_SLOT_SIZE (sizeof(long) + sizeof(short) + (mvee_num_childs - 1))
+# endif
+#else
+# ifdef MVEE_EXTENDED_QUEUE
+      #define MVEE_LOCK_QUEUE_SLOT_SIZE (3*sizeof(long))
+# else
+      #define MVEE_LOCK_QUEUE_SLOT_SIZE sizeof(short)
+# endif
+#endif
+#define mvee_lock_buffer_slot_size 64
+#define mvee_malloc_buffer_slot_size 64
+
+// In the new queue layout, we want each replica's lock, position, 
+// flush_cnt and flushing word on one and the same cache line
+// Therefore, we round up the buffer ptr to a multiple of 64 for the master replica.
+// Each subsequent replica has its variables aligned on the next cache line boundary
+
+#define INIT_MVEE_QUEUE(name, slot_size, queue_ident)			\
+  if (!mvee_##name##_buffer_data_start)					\
+    {									\
+      long tmp_id = syscall(MVEE_GET_SHARED_BUFFER, 0, queue_ident, &mvee_##name##_buffer_slots, MVEE_LOCK_QUEUE_SLOT_SIZE); \
+      mvee_##name##_buffer_slots      = (mvee_##name##_buffer_slots - mvee_num_childs * 64) / mvee_##name##_buffer_slot_size - 2; \
+      void* tmp_buffer                = (void*)syscall(__NR_shmat, tmp_id, NULL, 0); \
+      mvee_##name##_buffer_lock       = (volatile unsigned int*)  (MVEE_ROUND_UP((unsigned long)tmp_buffer, 64) + mvee_child_num * 64); \
+      mvee_##name##_buffer_pos        = (volatile unsigned int*)  (MVEE_ROUND_UP((unsigned long)tmp_buffer, 64) + mvee_child_num * 64 + sizeof(int)); \
+      mvee_##name##_buffer_flush_cnt  = (volatile unsigned int*)  (MVEE_ROUND_UP((unsigned long)tmp_buffer, 64) + mvee_child_num * 64 + sizeof(int) * 2); \
+      mvee_##name##_buffer_flushing   = (volatile unsigned char*) (MVEE_ROUND_UP((unsigned long)tmp_buffer, 64) + mvee_child_num * 64 + sizeof(int) * 3); \
+     *mvee_##name##_buffer_lock       = 1; \
+      mvee_##name##_buffer_data_start = MVEE_ROUND_UP((unsigned long)tmp_buffer, 64) + mvee_num_childs * 64; \
+      if (mvee_##name##_buffer_log_eips)				\
+	{								\
+	  long eip_buffer_id = syscall(MVEE_GET_SHARED_BUFFER, 1,	\
+				      queue_ident, NULL, mvee_num_childs * sizeof(long) * MVEE_STACK_DEPTH, MVEE_STACK_DEPTH); \
+	  mvee_##name##_eip_buffer = (void*)syscall(__NR_shmat, eip_buffer_id, NULL, 0); \
+	}								\
+    }									
+
+#define MVEE_LOG_QUEUE_DATA(name, pos, offset, data)			\
+  *(typeof(data)*)(mvee_##name##_buffer_data_start + mvee_##name##_buffer_slot_size * (pos) + offset) = data;
+
+#define MVEE_READ_QUEUE_DATA(name, pos, offset, result)			\
+  result = *(typeof(result)*)(mvee_##name##_buffer_data_start + mvee_##name##_buffer_slot_size * pos + offset);
+
+#define MVEE_LOG_STACK(name, start_depth, pos)				\
+  mvee_log_stack(mvee_##name##_eip_buffer, sizeof(long) * mvee_num_childs * MVEE_STACK_DEPTH, pos, start_depth);
+
+#ifdef MVEE_DEBUG_MALLOC
+extern void mvee_malloc_hook(int alloc_type, int msg, long chunksize, void* ar_ptr, void* chunk_ptr);
+#define MVEE_MALLOC_HOOK(type, msg, sz, ar_ptr, chunk_ptr)		\
+  mvee_malloc_hook(type, msg, sz, ar_ptr, chunk_ptr)
+#else
+#define MVEE_MALLOC_HOOK(type, msg, sz, ar_ptr, chunk_ptr)
+#endif // !MVEE_DEBUG_MALLOC
+
+extern void mvee_invalidate_buffer      (void);
+extern void mvee_atomic_postop_internal (unsigned char preop_result);
+extern int  mvee_should_sync_tid        (void);
+extern int  mvee_all_heaps_aligned      (char* heap, unsigned long alloc_size); 
+extern void mvee_malloc_hook(int alloc_type, int msg, long chunksize, void* ar_ptr, void* chunk_ptr);
+extern void mvee_write_malloc_info(int alloc_type, int msg, long chunksize, void* ar_ptr, void* chunk_ptr);
+extern void mvee_verify_malloc_info(int alloc_type, int msg, long chunksize, void* ar_ptr, void* chunk_ptr);
+
+#define MVEE_POSTOP() \
+  mvee_atomic_postop_internal(__tmp_mvee_preop);
+
+#ifdef MVEE_EXTENDED_QUEUE
+ extern unsigned char     mvee_atomic_preop_internal             (unsigned char is_store, void* word_ptr, unsigned short op_type);
+# define MVEE_PREOP(op_type, mem, is_store)					\
+	register unsigned char __tmp_mvee_preop = mvee_atomic_preop_internal(is_store, (void*)mem, op_type);
+#else
+ extern unsigned char     mvee_atomic_preop_internal            (unsigned char is_store, void* word_ptr);
+# define MVEE_PREOP(op_type, mem, is_store) \
+	register unsigned char __tmp_mvee_preop = mvee_atomic_preop_internal(is_store, (void*)mem);
+#endif // !MVEE_EXTENDED_QUEUE
diff -N -r --unified glibc-2.25.orig/sysdeps/x86_64/mvee-woc-agent.h glibc-2.25/sysdeps/x86_64/mvee-woc-agent.h
--- glibc-2.25.orig/sysdeps/x86_64/mvee-woc-agent.h	1970-01-01 00:00:00.000000000 +0000
+++ glibc-2.25/sysdeps/x86_64/mvee-woc-agent.h	2017-09-13 19:55:56.060750900 +0000
@@ -0,0 +1,16 @@
+#define MVEE_MAX_COUNTERS 65536
+
+#define MVEE_MALLOC_HOOK(type, msg, sz, ar_ptr, chunk_ptr)
+
+extern void          mvee_atomic_postop_internal (unsigned char preop_result);
+extern unsigned char mvee_atomic_preop_internal  (volatile void* word_ptr);
+extern int           mvee_should_sync_tid        (void);
+extern int           mvee_all_heaps_aligned      (char* heap, unsigned long alloc_size); 
+extern void          mvee_invalidate_buffer      (void);
+extern unsigned char mvee_should_futex_unlock    (void);
+
+#define MVEE_POSTOP()								\
+	mvee_atomic_postop_internal(__tmp_mvee_preop);
+
+#define MVEE_PREOP(op_type, mem, is_store)								\
+	register unsigned char  __tmp_mvee_preop = mvee_atomic_preop_internal(mem);
diff -N -r --unified glibc-2.25.orig/sysdeps/x86_64/nptl/pthread_spin_lock.S glibc-2.25/sysdeps/x86_64/nptl/pthread_spin_lock.S
--- glibc-2.25.orig/sysdeps/x86_64/nptl/pthread_spin_lock.S	2017-09-11 17:15:16.647740005 +0000
+++ glibc-2.25/sysdeps/x86_64/nptl/pthread_spin_lock.S	1970-01-01 00:00:00.000000000 +0000
@@ -1,34 +0,0 @@
-/* Copyright (C) 2012-2017 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, see
-   <http://www.gnu.org/licenses/>.  */
-
-#include <lowlevellock.h>
-#include <sysdep.h>
-
-ENTRY(pthread_spin_lock)
-1:	LOCK
-	decl	0(%rdi)
-	jne	2f
-	xor	%eax, %eax
-	ret
-
-	.align	16
-2:	rep
-	nop
-	cmpl	$0, 0(%rdi)
-	jg	1b
-	jmp	2b
-END(pthread_spin_lock)
diff -N -r --unified glibc-2.25.orig/sysdeps/x86_64/nptl/pthread_spin_trylock.S glibc-2.25/sysdeps/x86_64/nptl/pthread_spin_trylock.S
--- glibc-2.25.orig/sysdeps/x86_64/nptl/pthread_spin_trylock.S	2017-09-11 17:15:16.647740005 +0000
+++ glibc-2.25/sysdeps/x86_64/nptl/pthread_spin_trylock.S	1970-01-01 00:00:00.000000000 +0000
@@ -1,37 +0,0 @@
-/* Copyright (C) 2002-2017 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, see
-   <http://www.gnu.org/licenses/>.  */
-
-#include <pthread-errnos.h>
-#include <sysdep.h>
-
-
-#ifdef UP
-# define LOCK
-#else
-# define LOCK lock
-#endif
-
-ENTRY(pthread_spin_trylock)
-	movl	$1, %eax
-	xorl	%ecx, %ecx
-	LOCK
-	cmpxchgl %ecx, (%rdi)
-	movl	$EBUSY, %eax
-	cmovel	%ecx, %eax
-	retq
-END(pthread_spin_trylock)
diff -N -r --unified glibc-2.25.orig/sysdeps/x86_64/nptl/pthread_spin_unlock.S glibc-2.25/sysdeps/x86_64/nptl/pthread_spin_unlock.S
--- glibc-2.25.orig/sysdeps/x86_64/nptl/pthread_spin_unlock.S	2017-09-11 17:15:16.647740005 +0000
+++ glibc-2.25/sysdeps/x86_64/nptl/pthread_spin_unlock.S	1970-01-01 00:00:00.000000000 +0000
@@ -1,29 +0,0 @@
-/* Copyright (C) 2002-2017 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, see
-   <http://www.gnu.org/licenses/>.  */
-
-#include <sysdep.h>
-
-ENTRY(pthread_spin_unlock)
-	movl	$1, (%rdi)
-	xorl	%eax, %eax
-	retq
-END(pthread_spin_unlock)
-
-	/* The implementation of pthread_spin_init is identical.  */
-	.globl	pthread_spin_init
-pthread_spin_init = pthread_spin_unlock
diff -N -r --unified glibc-2.25.orig/sysdeps/x86_64/nptl/tls.h glibc-2.25/sysdeps/x86_64/nptl/tls.h
--- glibc-2.25.orig/sysdeps/x86_64/nptl/tls.h	2017-09-11 17:15:16.648740076 +0000
+++ glibc-2.25/sysdeps/x86_64/nptl/tls.h	2017-09-13 19:55:56.069751544 +0000
@@ -290,7 +290,7 @@
 
 
 /* Atomic compare and exchange on TLS, returning old value.  */
-# define THREAD_ATOMIC_CMPXCHG_VAL(descr, member, newval, oldval) \
+# define orig_THREAD_ATOMIC_CMPXCHG_VAL(descr, member, newval, oldval) \
   ({ __typeof (descr->member) __ret;					      \
      __typeof (oldval) __old = (oldval);				      \
      if (sizeof (descr->member) == 4)					      \
@@ -305,7 +305,7 @@
 
 
 /* Atomic logical and.  */
-# define THREAD_ATOMIC_AND(descr, member, val) \
+# define orig_THREAD_ATOMIC_AND(descr, member, val) \
   (void) ({ if (sizeof ((descr)->member) == 4)				      \
 	      asm volatile (LOCK_PREFIX "andl %1, %%fs:%P0"		      \
 			    :: "i" (offsetof (struct pthread, member)),	      \
@@ -316,7 +316,7 @@
 
 
 /* Atomic set bit.  */
-# define THREAD_ATOMIC_BIT_SET(descr, member, bit) \
+# define orig_THREAD_ATOMIC_BIT_SET(descr, member, bit) \
   (void) ({ if (sizeof ((descr)->member) == 4)				      \
 	      asm volatile (LOCK_PREFIX "orl %1, %%fs:%P0"		      \
 			    :: "i" (offsetof (struct pthread, member)),	      \
