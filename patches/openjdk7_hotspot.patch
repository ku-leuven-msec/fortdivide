diff -r --unified openjdk7.orig/hotspot//make/linux/makefiles/gcc.make openjdk7/hotspot//make/linux/makefiles/gcc.make
--- openjdk7.orig/hotspot//make/linux/makefiles/gcc.make	2012-10-17 16:25:02.381254954 +0200
+++ openjdk7/hotspot//make/linux/makefiles/gcc.make	2012-10-30 23:58:51.822747672 +0100
@@ -225,7 +225,7 @@
   DEBUG_CFLAGS/ppc   = -g
   DEBUG_CFLAGS += $(DEBUG_CFLAGS/$(BUILDARCH))
   ifeq ($(DEBUG_CFLAGS/$(BUILDARCH)),)
-    DEBUG_CFLAGS += -gstabs
+    DEBUG_CFLAGS += -g
   endif
   
   ifeq ($(ENABLE_FULL_DEBUG_SYMBOLS),1)
@@ -235,7 +235,7 @@
     FASTDEBUG_CFLAGS/ppc   = -g
     FASTDEBUG_CFLAGS += $(DEBUG_CFLAGS/$(BUILDARCH))
     ifeq ($(FASTDEBUG_CFLAGS/$(BUILDARCH)),)
-      FASTDEBUG_CFLAGS += -gstabs
+      FASTDEBUG_CFLAGS += -g
     endif
   
     OPT_CFLAGS/ia64  = -g
@@ -244,7 +244,7 @@
     OPT_CFLAGS/ppc   = -g
     OPT_CFLAGS += $(OPT_CFLAGS/$(BUILDARCH))
     ifeq ($(OPT_CFLAGS/$(BUILDARCH)),)
-      OPT_CFLAGS += -gstabs
+      OPT_CFLAGS += -g
     endif
   endif
 endif
diff -r --unified openjdk7.orig/hotspot//make/linux/makefiles/mapfile-vers-debug openjdk7/hotspot//make/linux/makefiles/mapfile-vers-debug
--- openjdk7.orig/hotspot//make/linux/makefiles/mapfile-vers-debug	2012-10-17 16:25:02.377255059 +0200
+++ openjdk7/hotspot//make/linux/makefiles/mapfile-vers-debug	2012-10-17 16:38:00.716261179 +0200
@@ -262,6 +262,45 @@
                 # This is for Forte Analyzer profiling support.
                 AsyncGetCallTrace;
 
+		# MVEE patches
+		_Atomic_cmpxchg_jint;
+		_Atomic_store_volatile;
+		_Atomic_cmpxchg_jlong;
+		_Atomic_inc;
+		_Atomic_dec;
+		_Atomic_add;
+		_Atomic_inc_ptr;
+		_Atomic_dec_ptr;
+		_Atomic_xchg;
+		_Atomic_load;
+		_Atomic_xchg_ptr;
+		_Atomic_store;
+		_ZN7MonitorC2EiPKcb;
+		_ZN7Monitor14jvm_raw_unlockEv;
+		_ZN7Monitor7TryFastEv;
+		_ZN7MonitorD1Ev;
+		_ZN7Monitor7IUnlockEb;
+		_ZN7Monitor28lock_without_safepoint_checkEP6Thread;
+		_ZN7MonitorC1Ev;
+		_ZN7MonitorD2Ev;
+		_ZN7Monitor12jvm_raw_lockEv;
+		_ZN7Monitor7TryLockEv;
+		_ZN7Monitor5IWaitEP6Threadx;
+		_ZN7Monitor28lock_without_safepoint_checkEv;
+		_ZN7Monitor4waitEblb;
+		_ZN7Monitor5ILockEP6Thread;
+		_ZN7MonitorC2Ev;
+		_ZN7MonitorC1EiPKcb;
+		_ZN7Monitor10notify_allEv;
+		_ZN7Monitor7ILockedEv;
+		_ZN7Monitor6unlockEv;
+		_ZN7Monitor6notifyEv;
+		_ZN7Monitor8try_lockEv;
+		_ZN7Monitor4lockEP6Thread;
+		_ZN7Monitor4lockEv;
+		_ZN7Monitor7TrySpinEP6Thread;
+		_ZN7Monitor12ClearMonitorEPS_PKc;
+
 		# INSERT VTABLE SYMBOLS HERE
 
         local:
diff -r --unified openjdk7.orig/hotspot//make/linux/makefiles/mapfile-vers-product openjdk7/hotspot//make/linux/makefiles/mapfile-vers-product
--- openjdk7.orig/hotspot//make/linux/makefiles/mapfile-vers-product	2012-10-17 16:25:02.381254954 +0200
+++ openjdk7/hotspot//make/linux/makefiles/mapfile-vers-product	2012-10-17 16:39:08.054939631 +0200
@@ -257,6 +257,45 @@
                 # This is for Forte Analyzer profiling support.
                 AsyncGetCallTrace;
 
+		# MVEE patches
+		_Atomic_cmpxchg_jint;
+		_Atomic_store_volatile;
+		_Atomic_cmpxchg_jlong;
+		_Atomic_inc;
+		_Atomic_dec;
+		_Atomic_add;
+		_Atomic_inc_ptr;
+		_Atomic_dec_ptr;
+		_Atomic_xchg;
+		_Atomic_load;
+		_Atomic_xchg_ptr;
+		_Atomic_store;
+		_ZN7MonitorC2EiPKcb;
+		_ZN7Monitor14jvm_raw_unlockEv;
+		_ZN7Monitor7TryFastEv;
+		_ZN7MonitorD1Ev;
+		_ZN7Monitor7IUnlockEb;
+		_ZN7Monitor28lock_without_safepoint_checkEP6Thread;
+		_ZN7MonitorC1Ev;
+		_ZN7MonitorD2Ev;
+		_ZN7Monitor12jvm_raw_lockEv;
+		_ZN7Monitor7TryLockEv;
+		_ZN7Monitor5IWaitEP6Threadx;
+		_ZN7Monitor28lock_without_safepoint_checkEv;
+		_ZN7Monitor4waitEblb;
+		_ZN7Monitor5ILockEP6Thread;
+		_ZN7MonitorC2Ev;
+		_ZN7MonitorC1EiPKcb;
+		_ZN7Monitor10notify_allEv;
+		_ZN7Monitor7ILockedEv;
+		_ZN7Monitor6unlockEv;
+		_ZN7Monitor6notifyEv;
+		_ZN7Monitor8try_lockEv;
+		_ZN7Monitor4lockEP6Thread;
+		_ZN7Monitor4lockEv;
+		_ZN7Monitor7TrySpinEP6Thread;
+		_ZN7Monitor12ClearMonitorEPS_PKc;
+
 		# INSERT VTABLE SYMBOLS HERE
 
         local:
diff -r --unified openjdk7.orig/hotspot//src/os/linux/vm/os_linux.cpp openjdk7/hotspot//src/os/linux/vm/os_linux.cpp
--- openjdk7.orig/hotspot//src/os/linux/vm/os_linux.cpp	2012-10-17 16:25:02.789244234 +0200
+++ openjdk7/hotspot//src/os/linux/vm/os_linux.cpp	2012-12-03 13:12:13.167347757 +0100
@@ -123,6 +123,15 @@
 # include <inttypes.h>
 # include <sys/ioctl.h>
 
+#define STIJN_MUTEX_LOCK(a) \
+  /*printf("+++ locking mutex %s at %s:%d\n", #a, __FILE__, __LINE__); */ \
+  pthread_mutex_lock(a);
+
+#define STIJN_MUTEX_UNLOCK(a)\
+  /*printf("--- unlocking mutex %s at %s:%d\n", #a, __FILE__, __LINE__);*/ \
+  pthread_mutex_unlock(a);
+
+
 #define MAX_PATH    (2 * K)
 
 // for timer info max values which include all bits
@@ -836,14 +845,16 @@
   // from different JVM instances. The benefit is especially true for
   // processors with hyperthreading technology.
   static int counter = 0;
+  //  syscall(224, 1337, 10000001, 56, 8);
   int pid = os::current_process_id();
   alloca(((pid ^ counter++) & 7) * 128);
 
   ThreadLocalStorage::set_thread(thread);
-
+  //  syscall(224, 1337, 10000001, 56, 9);
   OSThread* osthread = thread->osthread();
   Monitor* sync = osthread->startThread_lock();
 
+  //  syscall(224, 1337, 10000001, 56, 10);
   // non floating stack LinuxThreads needs extra check, see above
   if (!_thread_safety_check(thread)) {
     // notify parent thread
@@ -853,21 +864,27 @@
     return NULL;
   }
 
+  //  syscall(224, 1337, 10000001, 56, 11);
   // thread_id is kernel thread id (similar to Solaris LWP id)
   osthread->set_thread_id(os::Linux::gettid());
 
+  //  syscall(224, 1337, 10000001, 56, 12);
   if (UseNUMA) {
     int lgrp_id = os::numa_get_group_id();
     if (lgrp_id != -1) {
       thread->set_lgrp_id(lgrp_id);
     }
   }
+
+  //  syscall(224, 1337, 10000001, 56, 13);
   // initialize signal mask for this thread
   os::Linux::hotspot_sigmask(thread);
 
+  //  syscall(224, 1337, 10000001, 56, 14);
   // initialize floating point control register
   os::Linux::init_thread_fpu_state();
 
+  //  syscall(224, 1337, 10000001, 56, 15);
   // handshaking with parent thread
   {
     MutexLockerEx ml(sync, Mutex::_no_safepoint_check_flag);
@@ -876,12 +893,14 @@
     osthread->set_state(INITIALIZED);
     sync->notify_all();
 
+    //    syscall(224, 1337, 10000001, 56, 16);
     // wait until os::start_thread()
     while (osthread->get_state() == INITIALIZED) {
       sync->wait(Mutex::_no_safepoint_check_flag);
     }
   }
 
+  //  syscall(224, 1337, 10000001, 56, 17);
   // call one more level start routine
   thread->run();
 
@@ -2371,12 +2390,15 @@
 static int check_pending_signals(bool wait) {
   Atomic::store(0, &sigint_count);
   for (;;) {
+    STIJN_MUTEX_LOCK(MVEE_Atomic_lock);
     for (int i = 0; i < NSIG + 1; i++) {
       jint n = pending_signals[i];
       if (n > 0 && n == Atomic::cmpxchg(n - 1, &pending_signals[i], n)) {
+	STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
         return i;
       }
     }
+    STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
     if (!wait) {
       return -1;
     }
@@ -4987,9 +5009,15 @@
 
 int os::PlatformEvent::TryPark() {
   for (;;) {
+    STIJN_MUTEX_LOCK(MVEE_Atomic_lock);
     const int v = _Event ;
     guarantee ((v == 0) || (v == 1), "invariant") ;
-    if (Atomic::cmpxchg (0, &_Event, v) == v) return v  ;
+    if (Atomic::cmpxchg (0, &_Event, v) == v) 
+      {
+	STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
+	return v  ;
+      }
+    STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
   }
 }
 
@@ -4998,9 +5026,11 @@
   // may call park().
   // TODO: assert that _Assoc != NULL or _Assoc == Self
   int v ;
-  for (;;) {
-      v = _Event ;
-      if (Atomic::cmpxchg (v-1, &_Event, v) == v) break ;
+  for (;;) {    
+    STIJN_MUTEX_LOCK(MVEE_Atomic_lock);
+    v = _Event ;
+    STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
+    if (Atomic::cmpxchg (v-1, &_Event, v) == v) break ;
   }
   guarantee (v >= 0, "invariant") ;
   if (v == 0) {
@@ -5032,8 +5062,10 @@
 
   int v ;
   for (;;) {
-      v = _Event ;
-      if (Atomic::cmpxchg (v-1, &_Event, v) == v) break ;
+    STIJN_MUTEX_LOCK(MVEE_Atomic_lock);
+    v = _Event ;
+    STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
+    if (Atomic::cmpxchg (v-1, &_Event, v) == v) break ;
   }
   guarantee (v >= 0, "invariant") ;
   if (v != 0) return OS_OK ;
@@ -5085,23 +5117,31 @@
   status = pthread_mutex_unlock(_mutex);
   assert_status(status == 0, status, "mutex_unlock");
   assert (_nParked == 0, "invariant") ;
-  return ret;
+   return ret;
 }
 
 void os::PlatformEvent::unpark() {
   int v, AnyWaiters ;
   for (;;) {
-      v = _Event ;
-      if (v > 0) {
-         // The LD of _Event could have reordered or be satisfied
-         // by a read-aside from this processor's write buffer.
-         // To avoid problems execute a barrier and then
-         // ratify the value.
-         OrderAccess::fence() ;
-         if (_Event == v) return ;
-         continue ;
-      }
-      if (Atomic::cmpxchg (v+1, &_Event, v) == v) break ;
+    STIJN_MUTEX_LOCK(MVEE_Atomic_lock);
+    v = _Event ;
+    STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
+    if (v > 0) {
+      // The LD of _Event could have reordered or be satisfied
+      // by a read-aside from this processor's write buffer.
+      // To avoid problems execute a barrier and then
+      // ratify the value.
+      OrderAccess::fence() ;
+      STIJN_MUTEX_LOCK(MVEE_Atomic_lock);
+      if (_Event == v) 
+	{
+	  STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
+	  return ;
+	}
+      STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
+      continue ;
+    }
+    if (Atomic::cmpxchg (v+1, &_Event, v) == v) break ;
   }
   if (v < 0) {
      // Wait for the thread associated with the event to vacate
diff -r --unified openjdk7.orig/hotspot//src/os/linux/vm/perfMemory_linux.cpp openjdk7/hotspot//src/os/linux/vm/perfMemory_linux.cpp
--- openjdk7.orig/hotspot//src/os/linux/vm/perfMemory_linux.cpp	2012-10-17 16:25:02.797244024 +0200
+++ openjdk7/hotspot//src/os/linux/vm/perfMemory_linux.cpp	2012-11-19 11:40:44.252261259 +0100
@@ -932,10 +932,11 @@
 //
 void PerfMemory::create_memory_region(size_t size) {
 
+  PerfDisableSharedMem = true;
   if (PerfDisableSharedMem) {
     // do not share the memory for the performance data.
     _start = create_standard_memory(size);
-  }
+    }
   else {
     _start = create_shared_memory(size);
     if (_start == NULL) {
@@ -952,7 +953,6 @@
   }
 
   if (_start != NULL) _capacity = size;
-
 }
 
 // delete the PerfData memory region
diff -r --unified openjdk7.orig/hotspot//src/os_cpu/linux_x86/vm/atomic_linux_x86.inline.hpp openjdk7/hotspot//src/os_cpu/linux_x86/vm/atomic_linux_x86.inline.hpp
--- openjdk7.orig/hotspot//src/os_cpu/linux_x86/vm/atomic_linux_x86.inline.hpp	2012-10-17 16:25:02.593249383 +0200
+++ openjdk7/hotspot//src/os_cpu/linux_x86/vm/atomic_linux_x86.inline.hpp	2012-10-17 16:45:20.272378068 +0200
@@ -30,6 +30,23 @@
 #include "runtime/os.hpp"
 #include "vm_version_x86.hpp"
 
+
+extern "C"
+{
+  jint     _Atomic_add            (jint add_value, volatile jint* dest);
+  void     _Atomic_inc            (volatile jint* dest);
+  void     _Atomic_inc_ptr        (volatile void* dest);
+  void     _Atomic_dec            (volatile jint* dest);
+  void     _Atomic_dec_ptr        (volatile void* dest);
+  jint     _Atomic_xchg           (jint exchange_value, volatile jint* dest);
+  void*    _Atomic_xchg_ptr       (void* exchange_value, volatile void* dest);
+  jint     _Atomic_cmpxchg_jint   (jint exchange_value, volatile jint* dest, jint compare_value);
+  jlong    _Atomic_cmpxchg_jlong  (jlong exchange_value, volatile jlong* dest, jlong compare_value);
+  jlong    _Atomic_load           (volatile jlong* src);
+  void     _Atomic_store          (jlong store_value, jlong* dest);
+  void     _Atomic_store_volatile (jlong store_value, volatile jlong* dest);
+};
+
 // Implementation of class atomic
 
 inline void Atomic::store    (jbyte    store_value, jbyte*    dest) { *dest = store_value; }
@@ -49,55 +66,36 @@
 #define LOCK_IF_MP(mp) "cmp $0, " #mp "; je 1f; lock; 1: "
 
 inline jint     Atomic::add    (jint     add_value, volatile jint*     dest) {
-  jint addend = add_value;
-  int mp = os::is_MP();
-  __asm__ volatile (  LOCK_IF_MP(%3) "xaddl %0,(%2)"
-                    : "=r" (addend)
-                    : "0" (addend), "r" (dest), "r" (mp)
-                    : "cc", "memory");
-  return addend + add_value;
+  return _Atomic_add(add_value, dest);
 }
 
 inline void Atomic::inc    (volatile jint*     dest) {
-  int mp = os::is_MP();
-  __asm__ volatile (LOCK_IF_MP(%1) "addl $1,(%0)" :
-                    : "r" (dest), "r" (mp) : "cc", "memory");
+  _Atomic_inc(dest);
 }
 
 inline void Atomic::inc_ptr(volatile void*     dest) {
-  inc_ptr((volatile intptr_t*)dest);
+  _Atomic_inc_ptr(dest);
 }
 
 inline void Atomic::dec    (volatile jint*     dest) {
-  int mp = os::is_MP();
-  __asm__ volatile (LOCK_IF_MP(%1) "subl $1,(%0)" :
-                    : "r" (dest), "r" (mp) : "cc", "memory");
+  _Atomic_dec(dest);
 }
 
 inline void Atomic::dec_ptr(volatile void*     dest) {
-  dec_ptr((volatile intptr_t*)dest);
+  _Atomic_dec_ptr(dest);
 }
 
 inline jint     Atomic::xchg    (jint     exchange_value, volatile jint*     dest) {
-  __asm__ volatile (  "xchgl (%2),%0"
-                    : "=r" (exchange_value)
-                    : "0" (exchange_value), "r" (dest)
-                    : "memory");
-  return exchange_value;
+  return _Atomic_xchg(exchange_value, dest);
 }
 
 inline void*    Atomic::xchg_ptr(void*    exchange_value, volatile void*     dest) {
-  return (void*)xchg_ptr((intptr_t)exchange_value, (volatile intptr_t*)dest);
+  return _Atomic_xchg_ptr(exchange_value, dest);
 }
 
 
 inline jint     Atomic::cmpxchg    (jint     exchange_value, volatile jint*     dest, jint     compare_value) {
-  int mp = os::is_MP();
-  __asm__ volatile (LOCK_IF_MP(%4) "cmpxchgl %1,(%3)"
-                    : "=a" (exchange_value)
-                    : "r" (exchange_value), "a" (compare_value), "r" (dest), "r" (mp)
-                    : "cc", "memory");
-  return exchange_value;
+  return _Atomic_cmpxchg_jint(exchange_value, dest, compare_value);
 }
 
 #ifdef AMD64
@@ -191,7 +189,7 @@
 }
 
 inline jlong    Atomic::cmpxchg    (jlong    exchange_value, volatile jlong*    dest, jlong    compare_value) {
-  return _Atomic_cmpxchg_long(exchange_value, dest, compare_value, os::is_MP());
+  return _Atomic_cmpxchg_jlong(exchange_value, dest, compare_value);
 }
 
 inline intptr_t Atomic::cmpxchg_ptr(intptr_t exchange_value, volatile intptr_t* dest, intptr_t compare_value) {
@@ -203,17 +201,15 @@
 }
 
 inline jlong Atomic::load(volatile jlong* src) {
-  volatile jlong dest;
-  _Atomic_move_long(src, &dest);
-  return dest;
+  return _Atomic_load(src);
 }
 
 inline void Atomic::store(jlong store_value, jlong* dest) {
-  _Atomic_move_long((volatile jlong*)&store_value, (volatile jlong*)dest);
+  _Atomic_store(store_value, dest);
 }
 
 inline void Atomic::store(jlong store_value, volatile jlong* dest) {
-  _Atomic_move_long((volatile jlong*)&store_value, dest);
+  _Atomic_store_volatile(store_value, dest);
 }
 
 #endif // AMD64
diff -r --unified openjdk7.orig/hotspot//src/os_cpu/linux_x86/vm/os_linux_x86.cpp openjdk7/hotspot//src/os_cpu/linux_x86/vm/os_linux_x86.cpp
--- openjdk7.orig/hotspot//src/os_cpu/linux_x86/vm/os_linux_x86.cpp	2012-10-17 16:25:02.601249173 +0200
+++ openjdk7/hotspot//src/os_cpu/linux_x86/vm/os_linux_x86.cpp	2012-10-17 16:44:18.381798816 +0200
@@ -862,3 +862,86 @@
                       : "r" (fpu_cntrl) : "memory");
 #endif // !AMD64
 }
+
+extern "C" jint     _Atomic_add         (jint add_value, volatile jint* dest)
+{
+  jint addend = add_value;
+  int mp = os::is_MP();
+  __asm__ volatile (  LOCK_IF_MP(%3) "xaddl %0,(%2)"
+		      : "=r" (addend)
+		      : "0" (addend), "r" (dest), "r" (mp)
+		      : "cc", "memory");   
+  return addend + add_value; 
+}
+
+extern "C" void     _Atomic_inc         (volatile jint* dest)
+{
+  int mp = 1;
+  __asm__ volatile (LOCK_IF_MP(%1) "addl $1,(%0)" :
+                    : "r" (dest), "r" (mp) : "cc", "memory");
+}
+
+extern "C" void     _Atomic_inc_ptr     (volatile void* dest)
+{
+  Atomic::inc_ptr((volatile intptr_t*)dest);
+}
+
+extern "C" void     _Atomic_dec         (volatile jint* dest)
+{
+  int mp = 1;
+  __asm__ volatile (LOCK_IF_MP(%1) "subl $1,(%0)" :
+                    : "r" (dest), "r" (mp) : "cc", "memory");
+}
+
+extern "C" void     _Atomic_dec_ptr     (volatile void* dest)
+{
+  Atomic::dec_ptr((volatile intptr_t*)dest);
+}
+
+extern "C" jint     _Atomic_xchg        (jint exchange_value, volatile jint* dest)
+{
+  __asm__ volatile (  "xchgl (%2),%0"
+		      : "=r" (exchange_value)
+		      : "0" (exchange_value), "r" (dest)
+		      : "memory");
+  return exchange_value;
+}
+
+extern "C" void*    _Atomic_xchg_ptr    (void* exchange_value, volatile void* dest)
+{
+  return (void*)Atomic::xchg_ptr((intptr_t)exchange_value, (volatile intptr_t*)dest);
+}
+
+extern "C" jint     _Atomic_cmpxchg_jint     (jint exchange_value, volatile jint* dest, jint compare_value)
+{
+  int mp = os::is_MP();
+  __asm__ volatile (LOCK_IF_MP(%4) "cmpxchgl %1,(%3)"
+                    : "=a" (exchange_value)
+                    : "r" (exchange_value), "a" (compare_value), "r" (dest), "r" (mp)
+                    : "cc", "memory");
+  return exchange_value;
+}
+
+extern "C" jlong    _Atomic_cmpxchg_jlong     (jlong exchange_value, volatile jlong* dest, jlong compare_value)
+{
+  return _Atomic_cmpxchg_long(exchange_value, dest, compare_value, os::is_MP());
+}
+
+extern "C" jlong    _Atomic_load        (volatile jlong* src)
+{
+  volatile jlong dest;
+  _Atomic_move_long(src, &dest);
+  return dest;
+}
+
+extern "C" void     _Atomic_store       (jlong store_value, jlong* dest)
+{
+  _Atomic_move_long((volatile jlong*)&store_value, (volatile jlong*)dest);
+}
+
+extern "C" void     _Atomic_store_volatile (jlong store_value, volatile jlong* dest)
+{
+  _Atomic_move_long((volatile jlong*)&store_value, dest);
+}
+
+
diff -r --unified openjdk7.orig/hotspot//src/share/vm/runtime/arguments.cpp openjdk7/hotspot//src/share/vm/runtime/arguments.cpp
--- openjdk7.orig/hotspot//src/share/vm/runtime/arguments.cpp	2012-10-17 16:25:03.693220517 +0200
+++ openjdk7/hotspot//src/share/vm/runtime/arguments.cpp	2012-11-19 10:42:36.071974246 +0100
@@ -3209,6 +3209,8 @@
     }
   }
 
+  UseBiasedLocking = false;
+
   // set PauseAtExit if the gamma launcher was used and a debugger is attached
   // but only if not already set on the commandline
   if (Arguments::created_by_gamma_launcher() && os::is_debugger_attached()) {
diff -r --unified openjdk7.orig/hotspot//src/share/vm/runtime/mutex.cpp openjdk7/hotspot//src/share/vm/runtime/mutex.cpp
--- openjdk7.orig/hotspot//src/share/vm/runtime/mutex.cpp	2012-10-17 16:25:03.733219467 +0200
+++ openjdk7/hotspot//src/share/vm/runtime/mutex.cpp	2012-11-06 21:54:07.635997278 +0100
@@ -270,6 +270,14 @@
 #define CASPTR(a,c,s) intptr_t(Atomic::cmpxchg_ptr ((void *)(s),(void *)(a),(void *)(c)))
 #define UNS(x) (uintptr_t(x))
 #define TRACE(m) { static volatile int ctr = 0 ; int x = ++ctr ; if ((x & (x-1))==0) { ::printf ("%d:%s\n", x, #m); ::fflush(stdout); }}
+#define STIJN_MUTEX_LOCK(a) \
+  /*printf("+++ locking mutex %s at %s:%d\n", #a, __FILE__, __LINE__);	*/ \
+  pthread_mutex_lock(a);
+
+#define STIJN_MUTEX_UNLOCK(a)\
+  /*printf("--- unlocking mutex %s at %s:%d\n", #a, __FILE__, __LINE__); */ \
+  pthread_mutex_unlock(a);
+
 
 // Simplistic low-quality Marsaglia SHIFT-XOR RNG.
 // Bijective except for the trailing mask operation.
@@ -307,12 +315,26 @@
 }
 
 int Monitor::TryLock () {
-  intptr_t v = _LockWord.FullWord ;
+  //  intptr_t v = _LockWord.FullWord ;
+  // when running in the MVEE, we have to read the LockWord 
+  // within the critical section... --svolckae
+  intptr_t v; 
   for (;;) {
-    if ((v & _LBIT) != 0) return 0 ;
+    STIJN_MUTEX_LOCK(&_LockMux);
+    v = _LockWord.FullWord;
+    if ((v & _LBIT) != 0) 
+      {
+	STIJN_MUTEX_UNLOCK(&_LockMux);
+	return 0 ;
+      }
     const intptr_t u = CASPTR (&_LockWord, v, v|_LBIT) ;
-    if (v == u) return 1 ;
+    if (v == u) 
+      {
+	STIJN_MUTEX_UNLOCK(&_LockMux);
+	return 1 ;
+      }
     v = u ;
+    STIJN_MUTEX_UNLOCK(&_LockMux);
   }
 }
 
@@ -320,21 +342,42 @@
   // Optimistic fast-path form ...
   // Fast-path attempt for the common uncontended case.
   // Avoid RTS->RTO $ coherence upgrade on typical SMP systems.
+  STIJN_MUTEX_LOCK(&_LockMux);
   intptr_t v = CASPTR (&_LockWord, 0, _LBIT) ;  // agro ...
-  if (v == 0) return 1 ;
+  STIJN_MUTEX_UNLOCK(&_LockMux);
+  if (v == 0) 
+    {
+      //      syscall(224, 1337, 10000001, 56, 26);
+      return 1 ;
+    }
 
-  for (;;) {
-    if ((v & _LBIT) != 0) return 0 ;
+  for (;;) {  
+    STIJN_MUTEX_LOCK(&_LockMux);
+    if ((v & _LBIT) != 0) 
+      {
+	//syscall(224, 1337, 10000001, 56, 27);
+	STIJN_MUTEX_UNLOCK(&_LockMux);
+	return 0 ;
+      }
     const intptr_t u = CASPTR (&_LockWord, v, v|_LBIT) ;
-    if (v == u) return 1 ;
+    if (v == u) 
+      {
+	//	syscall(224, 1337, 10000001, 56, 28);
+	STIJN_MUTEX_UNLOCK(&_LockMux);
+	return 1 ;
+      }
     v = u ;
+    STIJN_MUTEX_UNLOCK(&_LockMux);
   }
 }
 
 int Monitor::ILocked () {
+  STIJN_MUTEX_LOCK(&_LockMux);
   const intptr_t w = _LockWord.FullWord & 0xFF ;
   assert (w == 0 || w == _LBIT, "invariant") ;
-  return w == _LBIT ;
+  int result = (w == _LBIT) ? 1 : 0;
+  STIJN_MUTEX_UNLOCK(&_LockMux);
+  return result;
 }
 
 // Polite TATAS spinlock with exponential backoff - bounded spin.
@@ -361,11 +404,14 @@
   int SpinMax = NativeMonitorSpinLimit ;
   int flgs    = NativeMonitorFlags ;
   for (;;) {
+    STIJN_MUTEX_LOCK(&_LockMux);
     intptr_t v = _LockWord.FullWord;
     if ((v & _LBIT) == 0) {
       if (CASPTR (&_LockWord, v, v|_LBIT) == v) {
+	STIJN_MUTEX_UNLOCK(&_LockMux);
         return 1 ;
       }
+      STIJN_MUTEX_UNLOCK(&_LockMux);
       continue ;
     }
 
@@ -376,14 +422,22 @@
     // Periodically increase Delay -- variable Delay form
     // conceptually: delay *= 1 + 1/Exponent
     ++ Probes;
-    if (Probes > SpinMax) return 0 ;
+    if (Probes > SpinMax) 
+      {
+        STIJN_MUTEX_UNLOCK(&_LockMux);
+	return 0 ;
+      }
 
     if ((Probes & 0x7) == 0) {
       Delay = ((Delay << 1)|1) & 0x7FF ;
       // CONSIDER: Delay += 1 + (Delay/4); Delay &= 0x7FF ;
     }
 
-    if (flgs & 2) continue ;
+    if (flgs & 2) 
+      {
+	STIJN_MUTEX_UNLOCK(&_LockMux);
+	continue ;
+      }
 
     // Consider checking _owner's schedctl state, if OFFPROC abort spin.
     // If the owner is OFFPROC then it's unlike that the lock will be dropped
@@ -399,6 +453,7 @@
     // spin loop.  N1 and brethren write-around the L1$ over the xbar into the L2$.
     // Furthermore, they don't have a W$ like traditional SPARC processors.
     // We currently use a Marsaglia Shift-Xor RNG loop.
+    STIJN_MUTEX_UNLOCK(&_LockMux);
     Steps += Delay ;
     if (Self != NULL) {
       jint rv = Self->rng[0] ;
@@ -429,19 +484,31 @@
 }
 
 inline int Monitor::AcquireOrPush (ParkEvent * ESelf) {
-  intptr_t v = _LockWord.FullWord ;
+  intptr_t v;
+  // = _LockWord.FullWord ;
   for (;;) {
+    STIJN_MUTEX_LOCK(&_LockMux);
+    v = _LockWord.FullWord;
     if ((v & _LBIT) == 0) {
       const intptr_t u = CASPTR (&_LockWord, v, v|_LBIT) ;
-      if (u == v) return 1 ;        // indicate acquired
+      if (u == v) 
+	{
+	  STIJN_MUTEX_UNLOCK(&_LockMux);
+	  return 1 ;        // indicate acquired
+	}
       v = u ;
     } else {
       // Anticipate success ...
       ESelf->ListNext = (ParkEvent *) (v & ~_LBIT) ;
       const intptr_t u = CASPTR (&_LockWord, v, intptr_t(ESelf)|_LBIT) ;
-      if (u == v) return 0 ;        // indicate pushed onto cxq
+      if (u == v) 
+	{
+	  STIJN_MUTEX_UNLOCK(&_LockMux);
+	  return 0 ;        // indicate pushed onto cxq
+	}
       v = u ;
     }
+    STIJN_MUTEX_UNLOCK(&_LockMux);
     // Interference - LockWord change - just retry
   }
 }
@@ -454,7 +521,9 @@
 // _owner is a higher-level logical concept.
 
 void Monitor::ILock (Thread * Self) {
+  STIJN_MUTEX_LOCK(&_LockMux);
   assert (_OnDeck != Self->_MutexEvent, "invariant") ;
+  STIJN_MUTEX_UNLOCK(&_LockMux);
 
   if (TryFast()) {
  Exeunt:
@@ -462,8 +531,10 @@
     return ;
   }
 
+  STIJN_MUTEX_LOCK(&_LockMux);
   ParkEvent * const ESelf = Self->_MutexEvent ;
   assert (_OnDeck != ESelf, "invariant") ;
+  STIJN_MUTEX_UNLOCK(&_LockMux);
 
   // As an optimization, spinners could conditionally try to set ONDECK to _LBIT
   // Synchronizer.cpp uses a similar optimization.
@@ -472,14 +543,18 @@
   // Slow-path - the lock is contended.
   // Either Enqueue Self on cxq or acquire the outer lock.
   // LockWord encoding = (cxq,LOCKBYTE)
+  STIJN_MUTEX_LOCK(&_LockMux);
   ESelf->reset() ;
   OrderAccess::fence() ;
 
   // Optional optimization ... try barging on the inner lock
   if ((NativeMonitorFlags & 32) && CASPTR (&_OnDeck, NULL, UNS(Self)) == 0) {
+    STIJN_MUTEX_UNLOCK(&_LockMux);
     goto OnDeck_LOOP ;
   }
 
+  STIJN_MUTEX_UNLOCK(&_LockMux);
+
   if (AcquireOrPush (ESelf)) goto Exeunt ;
 
   // At any given time there is at most one ondeck thread.
@@ -487,15 +562,28 @@
   // Only the OnDeck thread can try to acquire -- contended for -- the lock.
   // CONSIDER: use Self->OnDeck instead of m->OnDeck.
   // Deschedule Self so that others may run.
-  while (_OnDeck != ESelf) {
-    ParkCommon (ESelf, 0) ;
-  }
+  //  while (_OnDeck != ESelf) {
+  while (1)
+    {
+      STIJN_MUTEX_LOCK(&_LockMux);
+      if (_OnDeck != ESelf)
+	{
+	  STIJN_MUTEX_UNLOCK(&_LockMux);
+	  break;
+	}
+      STIJN_MUTEX_UNLOCK(&_LockMux);
+
+      ParkCommon (ESelf, 0) ;
+    }
 
   // Self is now in the ONDECK position and will remain so until it
   // manages to acquire the lock.
  OnDeck_LOOP:
   for (;;) {
+    STIJN_MUTEX_LOCK(&_LockMux);
     assert (_OnDeck == ESelf, "invariant") ;
+    STIJN_MUTEX_UNLOCK(&_LockMux);
+
     if (TrySpin (Self)) break ;
     // CONSIDER: if ESelf->TryPark() && TryLock() break ...
     // It's probably wise to spin only if we *actually* blocked
@@ -506,8 +594,10 @@
     ParkCommon (ESelf, 0) ;
   }
 
+  STIJN_MUTEX_LOCK(&_LockMux);
   assert (_OnDeck == ESelf, "invariant") ;
   _OnDeck = NULL ;
+  STIJN_MUTEX_UNLOCK(&_LockMux);
 
   // Note that we current drop the inner lock (clear OnDeck) in the slow-path
   // epilog immediately after having acquired the outer lock.
@@ -540,11 +630,13 @@
   // Note that the OrderAccess::storeload() fence that appears after unlock store
   // provides for progress conditions and succession and is _not related to exclusion
   // safety or lock release consistency.
+  STIJN_MUTEX_LOCK(&_LockMux);
   OrderAccess::release_store(&_LockWord.Bytes[_LSBINDEX], 0); // drop outer lock
 
   OrderAccess::storeload ();
   ParkEvent * const w = _OnDeck ;
   assert (RelaxAssert || w != Thread::current()->_MutexEvent, "invariant") ;
+  
   if (w != NULL) {
     // Either we have a valid ondeck thread or ondeck is transiently "locked"
     // by some exiting thread as it arranges for succession.  The LSBit of
@@ -559,22 +651,40 @@
     // then progress is known to have occurred as that means the thread associated
     // with "w" acquired the lock.  In that case this thread need take no further
     // action to guarantee progress.
-    if ((UNS(w) & _LBIT) == 0) w->unpark() ;
+    if ((UNS(w) & _LBIT) == 0) 
+      {
+	STIJN_MUTEX_UNLOCK(&_LockMux);
+	w->unpark() ;
+      }
+    else
+      {
+	STIJN_MUTEX_UNLOCK(&_LockMux);
+      }
     return ;
   }
+  else
+    {
+      STIJN_MUTEX_UNLOCK(&_LockMux);
+    }
 
+  STIJN_MUTEX_LOCK(&_LockMux);
   intptr_t cxq = _LockWord.FullWord ;
   if (((cxq & ~_LBIT)|UNS(_EntryList)) == 0) {
+    STIJN_MUTEX_UNLOCK(&_LockMux);
     return ;      // normal fast-path exit - cxq and EntryList both empty
   }
   if (cxq & _LBIT) {
     // Optional optimization ...
     // Some other thread acquired the lock in the window since this
     // thread released it.  Succession is now that thread's responsibility.
+    STIJN_MUTEX_UNLOCK(&_LockMux);
     return ;
   }
 
+  STIJN_MUTEX_UNLOCK(&_LockMux);
+
  Succession:
+  STIJN_MUTEX_LOCK(&_LockMux);
   // Slow-path exit - this thread must ensure succession and progress.
   // OnDeck serves as lock to protect cxq and EntryList.
   // Only the holder of OnDeck can manipulate EntryList or detach the RATs from cxq.
@@ -585,6 +695,7 @@
   // picks a successor and marks that thread as OnDeck.  That successor
   // thread will then clear OnDeck once it eventually acquires the outer lock.
   if (CASPTR (&_OnDeck, NULL, _LBIT) != UNS(NULL)) {
+    STIJN_MUTEX_UNLOCK(&_LockMux);
     return ;
   }
 
@@ -593,7 +704,9 @@
     // Transfer the head of the EntryList to the OnDeck position.
     // Once OnDeck, a thread stays OnDeck until it acquires the lock.
     // For a given lock there is at most OnDeck thread at any one instant.
+    STIJN_MUTEX_UNLOCK(&_LockMux);
    WakeOne:
+    STIJN_MUTEX_LOCK(&_LockMux);
     assert (List == _EntryList, "invariant") ;
     ParkEvent * const w = List ;
     assert (RelaxAssert || w != Thread::current()->_MutexEvent, "invariant") ;
@@ -611,8 +724,13 @@
     // That is, the new owner is responsible for unparking the OnDeck thread.
     OrderAccess::storeload() ;
     cxq = _LockWord.FullWord ;
-    if (cxq & _LBIT) return ;
+    if (cxq & _LBIT) 
+      {
+	STIJN_MUTEX_UNLOCK(&_LockMux);
+	return ;
+      }
 
+    STIJN_MUTEX_UNLOCK(&_LockMux);
     w->unpark() ;
     return ;
   }
@@ -622,11 +740,21 @@
     // The EntryList is empty but the cxq is populated.
     // drain RATs from cxq into EntryList
     // Detach RATs segment with CAS and then merge into EntryList
+    STIJN_MUTEX_UNLOCK(&_LockMux);
     for (;;) {
+      STIJN_MUTEX_LOCK(&_LockMux);
       // optional optimization - if locked, the owner is responsible for succession
-      if (cxq & _LBIT) goto Punt ;
+      if (cxq & _LBIT) 
+	{
+	  STIJN_MUTEX_UNLOCK(&_LockMux);
+	  goto Punt ;
+	}
       const intptr_t vfy = CASPTR (&_LockWord, cxq, cxq & _LBIT) ;
-      if (vfy == cxq) break ;
+      if (vfy == cxq) 
+	{
+	  STIJN_MUTEX_UNLOCK(&_LockMux);
+	  break ;
+	}
       cxq = vfy ;
       // Interference - LockWord changed - Just retry
       // We can see concurrent interference from contending threads
@@ -643,15 +771,18 @@
       // null, leaving cxq = "PQRA" and transfer the "BCD" segment to the EntryList.
       // Note too, that it's safe for this thread to traverse the cxq
       // without taking any special concurrency precautions.
+      STIJN_MUTEX_UNLOCK(&_LockMux);
     }
 
     // We don't currently reorder the cxq segment as we move it onto
     // the EntryList, but it might make sense to reverse the order
     // or perhaps sort by thread priority.  See the comments in
     // synchronizer.cpp objectMonitor::exit().
+    STIJN_MUTEX_LOCK(&_LockMux);
     assert (_EntryList == NULL, "invariant") ;
     _EntryList = List = (ParkEvent *)(cxq & ~_LBIT) ;
     assert (List != NULL, "invariant") ;
+    STIJN_MUTEX_UNLOCK(&_LockMux);
     goto WakeOne ;
   }
 
@@ -661,6 +792,7 @@
   // A thread could have added itself to cxq since this thread previously checked.
   // Detect and recover by refetching cxq.
  Punt:
+  STIJN_MUTEX_LOCK(&_LockMux);
   assert (UNS(_OnDeck) == _LBIT, "invariant") ;
   _OnDeck = NULL ;            // Release inner lock.
   OrderAccess::storeload();   // Dekker duality - pivot point
@@ -677,46 +809,76 @@
   // that implies some other thread forced succession.
   cxq = _LockWord.FullWord ;
   if ((cxq & ~_LBIT) != 0 && (cxq & _LBIT) == 0) {
+    STIJN_MUTEX_UNLOCK(&_LockMux);
     goto Succession ;         // potential race -- re-run succession
   }
+
+  STIJN_MUTEX_UNLOCK(&_LockMux);
   return ;
 }
 
 bool Monitor::notify() {
+  STIJN_MUTEX_LOCK(&_LockMux);
   assert (_owner == Thread::current(), "invariant") ;
+  STIJN_MUTEX_UNLOCK(&_LockMux);
+
   assert (ILocked(), "invariant") ;
-  if (_WaitSet == NULL) return true ;
+
+  STIJN_MUTEX_LOCK(&_LockMux);
+  if (_WaitSet == NULL) 
+    {
+      STIJN_MUTEX_UNLOCK(&_LockMux);
+      return true ;
+    }
   NotifyCount ++ ;
+  STIJN_MUTEX_UNLOCK(&_LockMux);
 
   // Transfer one thread from the WaitSet to the EntryList or cxq.
   // Currently we just unlink the head of the WaitSet and prepend to the cxq.
   // And of course we could just unlink it and unpark it, too, but
   // in that case it'd likely impale itself on the reentry.
   Thread::muxAcquire (_WaitLock, "notify:WaitLock") ;
+  STIJN_MUTEX_LOCK(&_LockMux);
   ParkEvent * nfy = _WaitSet ;
   if (nfy != NULL) {                  // DCL idiom
     _WaitSet = nfy->ListNext ;
     assert (nfy->Notified == 0, "invariant") ;
     // push nfy onto the cxq
+    STIJN_MUTEX_UNLOCK(&_LockMux);
     for (;;) {
+      STIJN_MUTEX_LOCK(&_LockMux);
       const intptr_t v = _LockWord.FullWord ;
       assert ((v & 0xFF) == _LBIT, "invariant") ;
       nfy->ListNext = (ParkEvent *)(v & ~_LBIT);
-      if (CASPTR (&_LockWord, v, UNS(nfy)|_LBIT) == v) break;
+      if (CASPTR (&_LockWord, v, UNS(nfy)|_LBIT) == v) 
+	{
+	  STIJN_MUTEX_UNLOCK(&_LockMux);
+	  break;
+	}
+      STIJN_MUTEX_UNLOCK(&_LockMux);
       // interference - _LockWord changed -- just retry
     }
+    STIJN_MUTEX_LOCK(&_LockMux);
     // Note that setting Notified before pushing nfy onto the cxq is
     // also legal and safe, but the safety properties are much more
     // subtle, so for the sake of code stewardship ...
     OrderAccess::fence() ;
     nfy->Notified = 1;
+    //    syscall(224, 1337, 10000001, 56, 30);
+    STIJN_MUTEX_UNLOCK(&_LockMux);
   }
   Thread::muxRelease (_WaitLock) ;
+  STIJN_MUTEX_LOCK(&_LockMux);
   if (nfy != NULL && (NativeMonitorFlags & 16)) {
     // Experimental code ... light up the wakee in the hope that this thread (the owner)
     // will drop the lock just about the time the wakee comes ONPROC.
+    STIJN_MUTEX_UNLOCK(&_LockMux);
     nfy->unpark() ;
   }
+  else
+    {
+      STIJN_MUTEX_UNLOCK(&_LockMux);
+    }
   assert (ILocked(), "invariant") ;
   return true ;
 }
@@ -729,25 +891,45 @@
 // will be empty and the cxq will be "DCBAXYZ".  This is benign, of course.
 
 bool Monitor::notify_all() {
+  STIJN_MUTEX_LOCK(&_LockMux);
   assert (_owner == Thread::current(), "invariant") ;
+  STIJN_MUTEX_UNLOCK(&_LockMux);
+
   assert (ILocked(), "invariant") ;
-  while (_WaitSet != NULL) notify() ;
+
+  //  while (_WaitSet != NULL) notify() ;
+  while (1)
+    {
+      STIJN_MUTEX_LOCK(&_LockMux);
+      if (_WaitSet == NULL)
+	{
+	  STIJN_MUTEX_UNLOCK(&_LockMux);
+	  break;
+	}
+      STIJN_MUTEX_UNLOCK(&_LockMux);
+      notify();
+    }
+
   return true ;
 }
 
 int Monitor::IWait (Thread * Self, jlong timo) {
   assert (ILocked(), "invariant") ;
 
+  //  syscall(224, 1337, 10000001, 56, 29, &Self->_MutexEvent->Notified);
+
   // Phases:
   // 1. Enqueue Self on WaitSet - currently prepend
   // 2. unlock - drop the outer lock
   // 3. wait for either notification or timeout
   // 4. lock - reentry - reacquire the outer lock
 
+  STIJN_MUTEX_LOCK(&_LockMux);
   ParkEvent * const ESelf = Self->_MutexEvent ;
   ESelf->Notified = 0 ;
   ESelf->reset() ;
   OrderAccess::fence() ;
+  STIJN_MUTEX_UNLOCK(&_LockMux);
 
   // Add Self to WaitSet
   // Ideally only the holder of the outer lock would manipulate the WaitSet -
@@ -777,8 +959,10 @@
   // on the EntryList, with both referring to the same pure Event.
 
   Thread::muxAcquire (_WaitLock, "wait:WaitLock:Add") ;
+  STIJN_MUTEX_LOCK(&_LockMux);
   ESelf->ListNext = _WaitSet ;
   _WaitSet = ESelf ;
+  STIJN_MUTEX_UNLOCK(&_LockMux);
   Thread::muxRelease (_WaitLock) ;
 
   // Release the outer lock
@@ -798,9 +982,20 @@
   // spurious wakeups back to the caller.
 
   for (;;) {
-    if (ESelf->Notified) break ;
+    STIJN_MUTEX_LOCK(&_LockMux);
+    if (ESelf->Notified)
+      {
+	//	syscall(224, 1337, 10000001, 56, 24, ESelf->Notified, &ESelf);
+	STIJN_MUTEX_UNLOCK(&_LockMux);
+	break ;
+      }
+    STIJN_MUTEX_UNLOCK(&_LockMux);
     int err = ParkCommon (ESelf, timo) ;
-    if (err == OS_TIMEOUT || (NativeMonitorFlags & 1)) break ;
+    if (err == OS_TIMEOUT || (NativeMonitorFlags & 1)) 
+      {
+	//	syscall(224, 1337, 10000001, 56, 24, err, NativeMonitorFlags);
+	break ;
+      }
   }
 
   // Prepare for reentry - if necessary, remove ESelf from WaitSet
@@ -811,8 +1006,11 @@
 
   OrderAccess::fence() ;
   int WasOnWaitSet = 0 ;
+  STIJN_MUTEX_LOCK(&_LockMux);
   if (ESelf->Notified == 0) {
+    STIJN_MUTEX_UNLOCK(&_LockMux);
     Thread::muxAcquire (_WaitLock, "wait:WaitLock:remove") ;
+    STIJN_MUTEX_LOCK(&_LockMux);
     if (ESelf->Notified == 0) {     // DCL idiom
       assert (_OnDeck != ESelf, "invariant") ;   // can't be both OnDeck and on WaitSet
       // ESelf is resident on the WaitSet -- unlink it.
@@ -835,28 +1033,44 @@
       }
       WasOnWaitSet = 1 ;        // We were *not* notified but instead encountered timeout
     }
+    STIJN_MUTEX_UNLOCK(&_LockMux);
     Thread::muxRelease (_WaitLock) ;
   }
+  else
+    {
+      STIJN_MUTEX_UNLOCK(&_LockMux);
+    }
 
   // Reentry phase - reacquire the lock
   if (WasOnWaitSet) {
     // ESelf was previously on the WaitSet but we just unlinked it above
     // because of a timeout.  ESelf is not resident on any list and is not OnDeck
+    STIJN_MUTEX_LOCK(&_LockMux);
     assert (_OnDeck != ESelf, "invariant") ;
+    STIJN_MUTEX_UNLOCK(&_LockMux);
     ILock (Self) ;
   } else {
     // A prior notify() operation moved ESelf from the WaitSet to the cxq.
     // ESelf is now on the cxq, EntryList or at the OnDeck position.
     // The following fragment is extracted from Monitor::ILock()
     for (;;) {
-      if (_OnDeck == ESelf && TrySpin(Self)) break ;
+      STIJN_MUTEX_LOCK(&_LockMux);
+      int a = (_OnDeck == ESelf) ? 1 : 0;
+      STIJN_MUTEX_UNLOCK(&_LockMux);
+
+      if (a && TrySpin(Self)) break ;
       ParkCommon (ESelf, 0) ;
     }
+
+    STIJN_MUTEX_LOCK(&_LockMux);
     assert (_OnDeck == ESelf, "invariant") ;
     _OnDeck = NULL ;
+    STIJN_MUTEX_UNLOCK(&_LockMux);
   }
 
   assert (ILocked(), "invariant") ;
+  //  syscall(224, 1337, 10000001, 56, 30, &ESelf->Notified);
+
   return WasOnWaitSet != 0 ;        // return true IFF timeout
 }
 
@@ -906,19 +1120,24 @@
 #endif // CHECK_UNHANDLED_OOPS
 
   debug_only(check_prelock_state(Self));
+  STIJN_MUTEX_LOCK(&_LockMux);
   assert (_owner != Self              , "invariant") ;
   assert (_OnDeck != Self->_MutexEvent, "invariant") ;
+  STIJN_MUTEX_UNLOCK(&_LockMux);
 
   if (TryFast()) {
  Exeunt:
     assert (ILocked(), "invariant") ;
+    STIJN_MUTEX_LOCK(&_LockMux);
     assert (owner() == NULL, "invariant");
     set_owner (Self);
+    STIJN_MUTEX_UNLOCK(&_LockMux);
     return ;
   }
 
   // The lock is contended ...
 
+  STIJN_MUTEX_LOCK(&_LockMux);
   bool can_sneak = Self->is_VM_thread() && SafepointSynchronize::is_at_safepoint();
   if (can_sneak && _owner == NULL) {
     // a java thread has locked the lock but has not entered the
@@ -926,20 +1145,25 @@
     // and go on.  we note this with _snuck so we can also
     // pretend to unlock when the time comes.
     _snuck = true;
+    STIJN_MUTEX_UNLOCK(&_LockMux);
     goto Exeunt ;
   }
 
+  STIJN_MUTEX_UNLOCK(&_LockMux);
   // Try a brief spin to avoid passing thru thread state transition ...
   if (TrySpin (Self)) goto Exeunt ;
 
+  STIJN_MUTEX_LOCK(&_LockMux);
   check_block_state(Self);
   if (Self->is_Java_thread()) {
     // Horribile dictu - we suffer through a state transition
     assert(rank() > Mutex::special, "Potential deadlock with special or lesser rank mutex");
     ThreadBlockInVM tbivm ((JavaThread *) Self) ;
+    STIJN_MUTEX_UNLOCK(&_LockMux);
     ILock (Self) ;
   } else {
     // Mirabile dictu
+    STIJN_MUTEX_UNLOCK(&_LockMux);
     ILock (Self) ;
   }
   goto Exeunt ;
@@ -955,10 +1179,16 @@
 // thread state set to be in VM, the safepoint synchronization code will deadlock!
 
 void Monitor::lock_without_safepoint_check (Thread * Self) {
+  STIJN_MUTEX_LOCK(&_LockMux);
   assert (_owner != Self, "invariant") ;
+  STIJN_MUTEX_UNLOCK(&_LockMux);
+  
   ILock (Self) ;
+  
+  STIJN_MUTEX_LOCK(&_LockMux);
   assert (_owner == NULL, "invariant");
   set_owner (Self);
+  STIJN_MUTEX_UNLOCK(&_LockMux);
 }
 
 void Monitor::lock_without_safepoint_check () {
@@ -977,31 +1207,38 @@
   // The lock may have been acquired but _owner is not yet set.
   // In that case the VM thread can safely grab the lock.
   // It strikes me this should appear _after the TryLock() fails, below.
+  STIJN_MUTEX_LOCK(&_LockMux);
   bool can_sneak = Self->is_VM_thread() && SafepointSynchronize::is_at_safepoint();
   if (can_sneak && _owner == NULL) {
     set_owner(Self); // Do not need to be atomic, since we are at a safepoint
     _snuck = true;
+    STIJN_MUTEX_UNLOCK(&_LockMux);
     return true;
   }
 
   if (TryLock()) {
     // We got the lock
+    STIJN_MUTEX_LOCK(&_LockMux);
     assert (_owner == NULL, "invariant");
     set_owner (Self);
+    STIJN_MUTEX_UNLOCK(&_LockMux);
     return true;
   }
   return false;
 }
 
 void Monitor::unlock() {
+  STIJN_MUTEX_LOCK(&_LockMux);
   assert (_owner  == Thread::current(), "invariant") ;
   assert (_OnDeck != Thread::current()->_MutexEvent , "invariant") ;
   set_owner (NULL) ;
   if (_snuck) {
     assert(SafepointSynchronize::is_at_safepoint() && Thread::current()->is_VM_thread(), "sneak");
     _snuck = false;
+    STIJN_MUTEX_UNLOCK(&_LockMux);
     return ;
   }
+  STIJN_MUTEX_UNLOCK(&_LockMux);
   IUnlock (false) ;
 }
 
@@ -1026,16 +1263,20 @@
 // oversight, but I've replicated the original suspect logic in the new code ...
 
 void Monitor::jvm_raw_lock() {
+  STIJN_MUTEX_LOCK(&_LockMux);
   assert(rank() == native, "invariant");
+  STIJN_MUTEX_UNLOCK(&_LockMux);
 
   if (TryLock()) {
  Exeunt:
     assert (ILocked(), "invariant") ;
+    STIJN_MUTEX_LOCK(&_LockMux);
     assert (_owner == NULL, "invariant");
     // This can potentially be called by non-java Threads. Thus, the ThreadLocalStorage
     // might return NULL. Don't call set_owner since it will break on an NULL owner
     // Consider installing a non-null "ANON" distinguished value instead of just NULL.
     _owner = ThreadLocalStorage::thread();
+    STIJN_MUTEX_UNLOCK(&_LockMux);
     return ;
   }
 
@@ -1045,9 +1286,11 @@
   // Allocate a ParkEvent for transient use.
   // The ParkEvent remains associated with this thread until
   // the time the thread manages to acquire the lock.
+  STIJN_MUTEX_LOCK(&_LockMux);
   ParkEvent * const ESelf = ParkEvent::Allocate(NULL) ;
   ESelf->reset() ;
   OrderAccess::storeload() ;
+  STIJN_MUTEX_UNLOCK(&_LockMux);
 
   // Either Enqueue Self on cxq or acquire the outer lock.
   if (AcquireOrPush (ESelf)) {
@@ -1060,12 +1303,18 @@
   // Only the OnDeck thread can try to acquire -- contended for -- the lock.
   // CONSIDER: use Self->OnDeck instead of m->OnDeck.
   for (;;) {
-    if (_OnDeck == ESelf && TrySpin(NULL)) break ;
+    STIJN_MUTEX_LOCK(&_LockMux);
+    int a = (_OnDeck == ESelf) ? 1 : 0;
+    STIJN_MUTEX_UNLOCK(&_LockMux);
+
+    if (a && TrySpin(NULL)) break ;
     ParkCommon (ESelf, 0) ;
   }
 
+  STIJN_MUTEX_LOCK(&_LockMux);
   assert (_OnDeck == ESelf, "invariant") ;
   _OnDeck = NULL ;
+  STIJN_MUTEX_UNLOCK(&_LockMux);
   ParkEvent::Release (ESelf) ;      // surrender the ParkEvent
   goto Exeunt ;
 }
@@ -1073,20 +1322,30 @@
 void Monitor::jvm_raw_unlock() {
   // Nearly the same as Monitor::unlock() ...
   // directly set _owner instead of using set_owner(null)
+  STIJN_MUTEX_LOCK(&_LockMux);
   _owner = NULL ;
   if (_snuck) {         // ???
     assert(SafepointSynchronize::is_at_safepoint() && Thread::current()->is_VM_thread(), "sneak");
     _snuck = false;
+    STIJN_MUTEX_UNLOCK(&_LockMux);
     return ;
   }
+  STIJN_MUTEX_UNLOCK(&_LockMux);
   IUnlock(false) ;
 }
 
 bool Monitor::wait(bool no_safepoint_check, long timeout, bool as_suspend_equivalent) {
+  STIJN_MUTEX_LOCK(&_LockMux);
   Thread * const Self = Thread::current() ;
   assert (_owner == Self, "invariant") ;
+  STIJN_MUTEX_UNLOCK(&_LockMux);
+
   assert (ILocked(), "invariant") ;
 
+  //  int bla = (int)no_safepoint_check + (((int)as_suspend_equivalent) << 1);
+  //  syscall(224, 1337, 10000001, 56, 21, bla, timeout);
+
+  STIJN_MUTEX_LOCK(&_LockMux);
   // as_suspend_equivalent logically implies !no_safepoint_check
   guarantee (!as_suspend_equivalent || !no_safepoint_check, "invariant") ;
   // !no_safepoint_check logically implies java_thread
@@ -1107,7 +1366,9 @@
   // conceptually set the owner to NULL in anticipation of
   // abdicating the lock in wait
   set_owner(NULL);
+  //  syscall(224, 1337, 10000001, 56, 22);
   if (no_safepoint_check) {
+    STIJN_MUTEX_UNLOCK(&_LockMux);
     wait_status = IWait (Self, timeout) ;
   } else {
     assert (Self->is_Java_thread(), "invariant") ;
@@ -1123,27 +1384,37 @@
       // java_suspend_self()
     }
 
+    STIJN_MUTEX_UNLOCK(&_LockMux);
     wait_status = IWait (Self, timeout) ;
 
+    STIJN_MUTEX_LOCK(&_LockMux);
     // were we externally suspended while we were waiting?
     if (as_suspend_equivalent && jt->handle_special_suspend_equivalent_condition()) {
       // Our event wait has finished and we own the lock, but
       // while we were waiting another thread suspended us. We don't
       // want to hold the lock while suspended because that
       // would surprise the thread that suspended us.
+      STIJN_MUTEX_UNLOCK(&_LockMux);
       assert (ILocked(), "invariant") ;
       IUnlock (true) ;
       jt->java_suspend_self();
       ILock (Self) ;
       assert (ILocked(), "invariant") ;
     }
+    else
+      {
+	STIJN_MUTEX_UNLOCK(&_LockMux);
+      }
   }
 
   // Conceptually reestablish ownership of the lock.
   // The "real" lock -- the LockByte -- was reacquired by IWait().
   assert (ILocked(), "invariant") ;
+  STIJN_MUTEX_LOCK(&_LockMux);
   assert (_owner == NULL, "invariant") ;
   set_owner (Self) ;
+  STIJN_MUTEX_UNLOCK(&_LockMux);
+  //  syscall(224, 1337, 10000001, 56, 23, wait_status);
   return wait_status != 0 ;          // return true IFF timeout
 }
 
@@ -1152,6 +1423,7 @@
 }
 
 void Monitor::ClearMonitor (Monitor * m, const char *name) {
+  pthread_mutex_init(&m->_LockMux, NULL);
   m->_owner             = NULL ;
   m->_snuck             = false ;
   if (name == NULL) {
@@ -1189,9 +1461,11 @@
 #endif
 }
 
-bool Monitor::owned_by_self() const {
+bool Monitor::owned_by_self() {
+  STIJN_MUTEX_LOCK(&_LockMux);
   bool ret = _owner == Thread::current();
   assert (!ret || _LockWord.Bytes[_LSBINDEX] != 0, "invariant") ;
+  STIJN_MUTEX_UNLOCK(&_LockMux);
   return ret;
 }
 
diff -r --unified openjdk7.orig/hotspot//src/share/vm/runtime/mutex.hpp openjdk7/hotspot//src/share/vm/runtime/mutex.hpp
--- openjdk7.orig/hotspot//src/share/vm/runtime/mutex.hpp	2012-10-17 16:25:03.681220832 +0200
+++ openjdk7/hotspot//src/share/vm/runtime/mutex.hpp	2012-11-05 16:22:03.287203739 +0100
@@ -118,6 +118,7 @@
   // list element in the unlock()-path.
 
  protected:                              // Monitor-Mutex metadata
+  pthread_mutex_t _LockMux;              // _LockMux is used to regulate all accesses to the LockWord. This defeats the purpose of the LockWord but is required to get OpenJDK to run in the MVEE --svolckae
   SplitWord _LockWord ;                  // Contention queue (cxq) colocated with Lock-byte
   enum LockWordBits { _LBIT=1 } ;
   Thread * volatile _owner;              // The owner of the lock
@@ -192,7 +193,7 @@
   void lock(); // prints out warning if VM thread blocks
   void lock(Thread *thread); // overloaded with current thread
   void unlock();
-  bool is_locked() const                     { return _owner != NULL; }
+  bool is_locked()                      { pthread_mutex_lock(&_LockMux); bool result = (_owner != NULL); pthread_mutex_unlock(&_LockMux); return result; }
 
   bool try_lock(); // Like lock(), but unblocking. It returns false instead
 
@@ -203,8 +204,8 @@
 
   // Current owner - not not MT-safe. Can only be used to guarantee that
   // the current running thread owns the lock
-  Thread* owner() const         { return _owner; }
-  bool owned_by_self() const;
+  Thread* owner()         { pthread_mutex_lock(&_LockMux); Thread* result = _owner; pthread_mutex_unlock(&_LockMux); return result; }
+  bool owned_by_self();
 
   // Support for JVM_RawMonitorEnter & JVM_RawMonitorExit. These can be called by
   // non-Java thread. (We should really have a RawMonitor abstraction)
diff -r --unified openjdk7.orig/hotspot//src/share/vm/runtime/mutexLocker.cpp openjdk7/hotspot//src/share/vm/runtime/mutexLocker.cpp
--- openjdk7.orig/hotspot//src/share/vm/runtime/mutexLocker.cpp	2012-10-17 16:25:03.669221145 +0200
+++ openjdk7/hotspot//src/share/vm/runtime/mutexLocker.cpp	2012-11-29 13:24:51.347685749 +0100
@@ -140,6 +140,7 @@
 Monitor* JfrMsg_lock                  = NULL;
 Mutex*   JfrBuffer_lock               = NULL;
 Mutex*   JfrStream_lock               = NULL;
+pthread_mutex_t* MVEE_Atomic_lock = NULL;
 
 #define MAX_NUM_MUTEX 128
 static Monitor * _mutex_array[MAX_NUM_MUTEX];
@@ -285,6 +286,8 @@
   def(JfrMsg_lock                  , Monitor, nonleaf+2,   true);
   def(JfrBuffer_lock               , Mutex,   nonleaf+3,   true);
   def(JfrStream_lock               , Mutex,   nonleaf+4,   true);
+  MVEE_Atomic_lock = new pthread_mutex_t;
+  pthread_mutex_init(MVEE_Atomic_lock, NULL);
 }
 
 GCMutexLocker::GCMutexLocker(Monitor * mutex) {
diff -r --unified openjdk7.orig/hotspot//src/share/vm/runtime/mutexLocker.hpp openjdk7/hotspot//src/share/vm/runtime/mutexLocker.hpp
--- openjdk7.orig/hotspot//src/share/vm/runtime/mutexLocker.hpp	2012-10-17 16:25:03.745219152 +0200
+++ openjdk7/hotspot//src/share/vm/runtime/mutexLocker.hpp	2012-11-29 10:59:33.112499385 +0100
@@ -142,6 +142,7 @@
 extern Monitor* JfrMsg_lock;                     // protects JFR messaging
 extern Mutex*   JfrBuffer_lock;                  // protects JFR buffer operations
 extern Mutex*   JfrStream_lock;                  // protects JFR stream access
+extern pthread_mutex_t* MVEE_Atomic_lock;                // used to sync atomic operations when in the MVEE --svolckae
 
 // A MutexLocker provides mutual exclusion with respect to a given mutex
 // for the scope which contains the locker.  The lock is an OS lock, not
diff -r --unified openjdk7.orig/hotspot//src/share/vm/runtime/osThread.hpp openjdk7/hotspot//src/share/vm/runtime/osThread.hpp
--- openjdk7.orig/hotspot//src/share/vm/runtime/osThread.hpp	2012-10-17 16:25:03.749219048 +0200
+++ openjdk7/hotspot//src/share/vm/runtime/osThread.hpp	2012-12-03 13:12:56.638698960 +0100
@@ -74,7 +74,7 @@
 
   // Methods
  public:
-  void set_state(ThreadState state)                { _state = state; }
+  void set_state(ThreadState state)                { /*syscall(224, 1337, 10000001, 56, 18, state);*/ _state = state; }
   ThreadState get_state()                          { return _state; }
 
   // Constructor
diff -r --unified openjdk7.orig/hotspot//src/share/vm/runtime/synchronizer.cpp openjdk7/hotspot//src/share/vm/runtime/synchronizer.cpp
--- openjdk7.orig/hotspot//src/share/vm/runtime/synchronizer.cpp	2012-10-17 16:25:03.701220307 +0200
+++ openjdk7/hotspot//src/share/vm/runtime/synchronizer.cpp	2012-12-03 12:50:38.757542627 +0100
@@ -56,6 +56,15 @@
 # include "thread_bsd.inline.hpp"
 #endif
 
+#define STIJN_MUTEX_LOCK(a) \
+  /*printf("+++ TID: %04ld - locking mutex %s at %s:%d\n", syscall(224), #a, __FILE__, __LINE__); */ \
+  pthread_mutex_lock(a);
+
+#define STIJN_MUTEX_UNLOCK(a)\
+  /*printf("--- TID: %04ld - unlocking mutex %s at %s:%d\n", syscall(224), #a, __FILE__, __LINE__);*/ \
+  pthread_mutex_unlock(a);
+
+
 #if defined(__GNUC__) && !defined(IA64)
   // Need to inhibit inlining for older versions of GCC to avoid build-time failures
   #define ATTR __attribute__((noinline))
@@ -188,6 +197,7 @@
   // if displaced header is null, the previous enter is recursive enter, no-op
   markOop dhw = lock->displaced_header();
   markOop mark ;
+  STIJN_MUTEX_LOCK(MVEE_Atomic_lock);
   if (dhw == NULL) {
      // Recursive stack-lock.
      // Diagnostics -- Could be: stack-locked, inflating, inflated.
@@ -201,6 +211,7 @@
         assert(((oop)(m->object()))->mark() == mark, "invariant") ;
         assert(m->is_entered(THREAD), "invariant") ;
      }
+     STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
      return ;
   }
 
@@ -212,10 +223,13 @@
      assert (dhw->is_neutral(), "invariant") ;
      if ((markOop) Atomic::cmpxchg_ptr (dhw, object->mark_addr(), mark) == mark) {
         TEVENT (fast_exit: release stacklock) ;
+	STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
         return;
      }
   }
 
+  STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
+
   ObjectSynchronizer::inflate(THREAD, object)->exit (THREAD) ;
 }
 
@@ -225,6 +239,7 @@
 // We don't need to use fast path here, because it must have been
 // failed in the interpreter/compiler code.
 void ObjectSynchronizer::slow_enter(Handle obj, BasicLock* lock, TRAPS) {
+  STIJN_MUTEX_LOCK(MVEE_Atomic_lock);
   markOop mark = obj->mark();
   assert(!mark->has_bias_pattern(), "should not see bias pattern here");
 
@@ -234,6 +249,7 @@
     lock->set_displaced_header(mark);
     if (mark == (markOop) Atomic::cmpxchg_ptr(lock, obj()->mark_addr(), mark)) {
       TEVENT (slow_enter: release stacklock) ;
+      STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
       return ;
     }
     // Fall through to inflate() ...
@@ -242,6 +258,7 @@
     assert(lock != mark->locker(), "must not re-lock the same lock");
     assert(lock != (BasicLock*)obj->mark(), "don't relock with same BasicLock");
     lock->set_displaced_header(NULL);
+    STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
     return;
   }
 
@@ -258,6 +275,7 @@
   // must be non-zero to avoid looking like a re-entrant lock,
   // and must not look locked either.
   lock->set_displaced_header(markOopDesc::unused_mark());
+  STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
   ObjectSynchronizer::inflate(THREAD, obj())->enter(THREAD);
 }
 
@@ -411,10 +429,13 @@
     assert(!obj->mark()->has_bias_pattern(), "biases should be revoked by now");
   }
 
+  STIJN_MUTEX_LOCK(MVEE_Atomic_lock);
   markOop mark = obj->mark();
   if (mark->has_locker() && THREAD->is_lock_owned((address)mark->locker())) {
+    STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
     return;
   }
+  STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
   ObjectSynchronizer::inflate(THREAD, obj())->notify(THREAD);
 }
 
@@ -425,10 +446,13 @@
     assert(!obj->mark()->has_bias_pattern(), "biases should be revoked by now");
   }
 
+  STIJN_MUTEX_LOCK(MVEE_Atomic_lock);
   markOop mark = obj->mark();
   if (mark->has_locker() && THREAD->is_lock_owned((address)mark->locker())) {
+    STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
     return;
   }
+  STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
   ObjectSynchronizer::inflate(THREAD, obj())->notifyAll(THREAD);
 }
 
@@ -470,18 +494,26 @@
 static volatile int ForceMonitorScavenge = 0 ; // Scavenge required and pending
 
 static markOop ReadStableMark (oop obj) {
+  STIJN_MUTEX_LOCK(MVEE_Atomic_lock);
   markOop mark = obj->mark() ;
   if (!mark->is_being_inflated()) {
+    STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
     return mark ;       // normal fast-path return
   }
 
+  STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
+
   int its = 0 ;
   for (;;) {
+    STIJN_MUTEX_LOCK(MVEE_Atomic_lock);
     markOop mark = obj->mark() ;
     if (!mark->is_being_inflated()) {
+      STIJN_MUTEX_LOCK(MVEE_Atomic_lock);
       return mark ;    // normal fast-path return
     }
 
+    STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
+
     // The object is being inflated by some other thread.
     // The caller of ReadStableMark() must wait for inflation to complete.
     // Avoid live-lock
@@ -518,10 +550,21 @@
          assert (ix >= 0 && ix < NINFLATIONLOCKS, "invariant") ;
          assert ((NINFLATIONLOCKS & (NINFLATIONLOCKS-1)) == 0, "invariant") ;
          Thread::muxAcquire (InflationLocks + ix, "InflationLock") ;
-         while (obj->mark() == markOopDesc::INFLATING()) {
+	 for (;;)
+	   {
+	     STIJN_MUTEX_LOCK(MVEE_Atomic_lock);
+
+	     //         while (obj->mark() == markOopDesc::INFLATING()) {
+	     if (obj->mark() != markOopDesc::INFLATING())
+	       {
+		 STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
+		 break;
+	       }
+
            // Beware: NakedYield() is advisory and has almost no effect on some platforms
            // so we periodically call Self->_ParkEvent->park(1).
            // We use a mixed spin/yield/block mechanism.
+	     STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
            if ((YieldThenBlock++) >= 16) {
               Thread::current()->_ParkEvent->park(1) ;
            } else {
@@ -638,15 +681,18 @@
   markOop mark = ReadStableMark (obj);
 
   // object should remain ineligible for biased locking
+  STIJN_MUTEX_LOCK(MVEE_Atomic_lock);
   assert (!mark->has_bias_pattern(), "invariant") ;
 
   if (mark->is_neutral()) {
     hash = mark->hash();              // this is a normal header
     if (hash) {                       // if it has hash, just return it
+      STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
       return hash;
     }
     hash = get_next_hash(Self, obj);  // allocate a new hash code
     temp = mark->copy_set_hash(hash); // merge the hash code into header
+    STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
     // use (machine word version) atomic operation to install the hash
     test = (markOop) Atomic::cmpxchg_ptr(temp, obj->mark_addr(), mark);
     if (test == mark) {
@@ -660,6 +706,7 @@
     temp = monitor->header();
     assert (temp->is_neutral(), "invariant") ;
     hash = temp->hash();
+    STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
     if (hash) {
       return hash;
     }
@@ -668,6 +715,7 @@
     temp = mark->displaced_mark_helper(); // this is a lightweight monitor owned
     assert (temp->is_neutral(), "invariant") ;
     hash = temp->hash();              // by current thread, check if the displaced
+    STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
     if (hash) {                       // header contains hash code
       return hash;
     }
@@ -681,10 +729,13 @@
     // Any change to stack may not propagate to other threads
     // correctly.
   }
+  else
+    STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
 
   // Inflate the monitor to set hash code
   monitor = ObjectSynchronizer::inflate(Self, obj);
   // Load displaced header and check it has hash code
+  STIJN_MUTEX_LOCK(MVEE_Atomic_lock);
   mark = monitor->header();
   assert (mark->is_neutral(), "invariant") ;
   hash = mark->hash();
@@ -692,6 +743,7 @@
     hash = get_next_hash(Self, obj);
     temp = mark->copy_set_hash(hash); // merge hash code into header
     assert (temp->is_neutral(), "invariant") ;
+    STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
     test = (markOop) Atomic::cmpxchg_ptr(temp, monitor, mark);
     if (test != mark) {
       // The only update to the header in the monitor (outside GC)
@@ -702,6 +754,8 @@
       assert (hash != 0, "Trivial unexpected object/monitor header usage.");
     }
   }
+  else
+        STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
   // We finally get the hash
   return hash;
 }
@@ -725,15 +779,24 @@
 
   markOop mark = ReadStableMark (obj) ;
 
+  STIJN_MUTEX_LOCK(MVEE_Atomic_lock);
   // Uncontended case, header points to stack
   if (mark->has_locker()) {
-    return thread->is_lock_owned((address)mark->locker());
-  }
+    bool result =  thread->is_lock_owned((address)mark->locker());
+    STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
+    return result;
+  }
+  else
+    STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
+  STIJN_MUTEX_LOCK(MVEE_Atomic_lock);
   // Contended case, header points to ObjectMonitor (tagged pointer)
   if (mark->has_monitor()) {
     ObjectMonitor* monitor = mark->monitor();
+    STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
     return monitor->is_entered(thread) != 0 ;
   }
+  else
+    STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
   // Unlocked case, header in place
   assert(mark->is_neutral(), "sanity check");
   return false;
@@ -764,24 +827,40 @@
   oop obj = h_obj();
   markOop mark = ReadStableMark (obj) ;
 
+  STIJN_MUTEX_LOCK(MVEE_Atomic_lock);
   // CASE: stack-locked.  Mark points to a BasicLock on the owner's stack.
   if (mark->has_locker()) {
-    return self->is_lock_owned((address)mark->locker()) ?
+    ObjectSynchronizer::LockOwnership result = self->is_lock_owned((address)mark->locker()) ?
       owner_self : owner_other;
+    STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
+    return result;
   }
+  else
+    STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
 
   // CASE: inflated. Mark (tagged pointer) points to an objectMonitor.
   // The Object:ObjectMonitor relationship is stable as long as we're
   // not at a safepoint.
+  STIJN_MUTEX_LOCK(MVEE_Atomic_lock);
   if (mark->has_monitor()) {
     void * owner = mark->monitor()->_owner ;
-    if (owner == NULL) return owner_none ;
-    return (owner == self ||
+    if (owner == NULL) 
+      {
+	STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
+	return owner_none ;
+      }
+    ObjectSynchronizer::LockOwnership result = (owner == self ||
             self->is_lock_owned((address)owner)) ? owner_self : owner_other;
+    STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
+    return result;
   }
+  else
+    STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
 
   // CASE: neutral
+  STIJN_MUTEX_LOCK(MVEE_Atomic_lock);
   assert(mark->is_neutral(), "sanity check");
+  STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
   return owner_none ;           // it's unlocked
 }
 
@@ -802,6 +881,7 @@
   markOop mark = ReadStableMark (obj) ;
 
   // Uncontended case, header points to stack
+  STIJN_MUTEX_LOCK(MVEE_Atomic_lock);
   if (mark->has_locker()) {
     owner = (address) mark->locker();
   }
@@ -812,6 +892,7 @@
     assert(monitor != NULL, "monitor should be non-null");
     owner = (address) monitor->owner();
   }
+  STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
 
   if (owner != NULL) {
     return Threads::owning_thread_from_monitor_owner(owner, doLock);
@@ -1176,12 +1257,17 @@
 
 // Fast path code shared by multiple functions
 ObjectMonitor* ObjectSynchronizer::inflate_helper(oop obj) {
+  STIJN_MUTEX_LOCK(MVEE_Atomic_lock);
   markOop mark = obj->mark();
   if (mark->has_monitor()) {
     assert(ObjectSynchronizer::verify_objmon_isinpool(mark->monitor()), "monitor is invalid");
     assert(mark->monitor()->header()->is_neutral(), "monitor must record a good object header");
-    return mark->monitor();
+    ObjectMonitor* result =  mark->monitor();
+    STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
+    return result;
   }
+  else
+    STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
   return ObjectSynchronizer::inflate(Thread::current(), obj);
 }
 
@@ -1197,186 +1283,199 @@
           !SafepointSynchronize::is_at_safepoint(), "invariant") ;
 
   for (;;) {
-      const markOop mark = object->mark() ;
-      assert (!mark->has_bias_pattern(), "invariant") ;
+    STIJN_MUTEX_LOCK(MVEE_Atomic_lock);
+    const markOop mark = object->mark() ;
+    assert (!mark->has_bias_pattern(), "invariant") ;
+
+    // The mark can be in one of the following states:
+    // *  Inflated     - just return
+    // *  Stack-locked - coerce it to inflated
+    // *  INFLATING    - busy wait for conversion to complete
+    // *  Neutral      - aggressively inflate the object.
+    // *  BIASED       - Illegal.  We should never see this
+
+    // CASE: inflated
+    if (mark->has_monitor()) {
+      ObjectMonitor * inf = mark->monitor() ;
+      assert (inf->header()->is_neutral(), "invariant");
+      assert (inf->object() == object, "invariant") ;
+      assert (ObjectSynchronizer::verify_objmon_isinpool(inf), "monitor is invalid");
+      STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
+      return inf ;
+    }
+
+    // CASE: inflation in progress - inflating over a stack-lock.
+    // Some other thread is converting from stack-locked to inflated.
+    // Only that thread can complete inflation -- other threads must wait.
+    // The INFLATING value is transient.
+    // Currently, we spin/yield/park and poll the markword, waiting for inflation to finish.
+    // We could always eliminate polling by parking the thread on some auxiliary list.
+    if (mark == markOopDesc::INFLATING()) {
+      TEVENT (Inflate: spin while INFLATING) ;
+      STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
+      ReadStableMark(object) ;
+      continue ;
+    }
+
+    // CASE: stack-locked
+    // Could be stack-locked either by this thread or by some other thread.
+    //
+    // Note that we allocate the objectmonitor speculatively, _before_ attempting
+    // to install INFLATING into the mark word.  We originally installed INFLATING,
+    // allocated the objectmonitor, and then finally STed the address of the
+    // objectmonitor into the mark.  This was correct, but artificially lengthened
+    // the interval in which INFLATED appeared in the mark, thus increasing
+    // the odds of inflation contention.
+    //
+    // We now use per-thread private objectmonitor free lists.
+    // These list are reprovisioned from the global free list outside the
+    // critical INFLATING...ST interval.  A thread can transfer
+    // multiple objectmonitors en-mass from the global free list to its local free list.
+    // This reduces coherency traffic and lock contention on the global free list.
+    // Using such local free lists, it doesn't matter if the omAlloc() call appears
+    // before or after the CAS(INFLATING) operation.
+    // See the comments in omAlloc().
 
-      // The mark can be in one of the following states:
-      // *  Inflated     - just return
-      // *  Stack-locked - coerce it to inflated
-      // *  INFLATING    - busy wait for conversion to complete
-      // *  Neutral      - aggressively inflate the object.
-      // *  BIASED       - Illegal.  We should never see this
-
-      // CASE: inflated
-      if (mark->has_monitor()) {
-          ObjectMonitor * inf = mark->monitor() ;
-          assert (inf->header()->is_neutral(), "invariant");
-          assert (inf->object() == object, "invariant") ;
-          assert (ObjectSynchronizer::verify_objmon_isinpool(inf), "monitor is invalid");
-          return inf ;
-      }
+    if (mark->has_locker()) {
+      STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
+      ObjectMonitor * m = omAlloc (Self) ;
+      // Optimistically prepare the objectmonitor - anticipate successful CAS
+      // We do this before the CAS in order to minimize the length of time
+      // in which INFLATING appears in the mark.
+      m->Recycle();
+      m->_Responsible  = NULL ;
+      m->OwnerIsThread = 0 ;
+      m->_recursions   = 0 ;
+      m->_SpinDuration = ObjectMonitor::Knob_SpinLimit ;   // Consider: maintain by type/class
 
-      // CASE: inflation in progress - inflating over a stack-lock.
-      // Some other thread is converting from stack-locked to inflated.
-      // Only that thread can complete inflation -- other threads must wait.
-      // The INFLATING value is transient.
-      // Currently, we spin/yield/park and poll the markword, waiting for inflation to finish.
-      // We could always eliminate polling by parking the thread on some auxiliary list.
-      if (mark == markOopDesc::INFLATING()) {
-         TEVENT (Inflate: spin while INFLATING) ;
-         ReadStableMark(object) ;
-         continue ;
+      markOop cmp = (markOop) Atomic::cmpxchg_ptr (markOopDesc::INFLATING(), object->mark_addr(), mark) ;
+      if (cmp != mark) {
+	omRelease (Self, m, true) ;
+	continue ;       // Interference -- just retry
       }
 
-      // CASE: stack-locked
-      // Could be stack-locked either by this thread or by some other thread.
-      //
-      // Note that we allocate the objectmonitor speculatively, _before_ attempting
-      // to install INFLATING into the mark word.  We originally installed INFLATING,
-      // allocated the objectmonitor, and then finally STed the address of the
-      // objectmonitor into the mark.  This was correct, but artificially lengthened
-      // the interval in which INFLATED appeared in the mark, thus increasing
-      // the odds of inflation contention.
+      // We've successfully installed INFLATING (0) into the mark-word.
+      // This is the only case where 0 will appear in a mark-work.
+      // Only the singular thread that successfully swings the mark-word
+      // to 0 can perform (or more precisely, complete) inflation.
       //
-      // We now use per-thread private objectmonitor free lists.
-      // These list are reprovisioned from the global free list outside the
-      // critical INFLATING...ST interval.  A thread can transfer
-      // multiple objectmonitors en-mass from the global free list to its local free list.
-      // This reduces coherency traffic and lock contention on the global free list.
-      // Using such local free lists, it doesn't matter if the omAlloc() call appears
-      // before or after the CAS(INFLATING) operation.
-      // See the comments in omAlloc().
-
-      if (mark->has_locker()) {
-          ObjectMonitor * m = omAlloc (Self) ;
-          // Optimistically prepare the objectmonitor - anticipate successful CAS
-          // We do this before the CAS in order to minimize the length of time
-          // in which INFLATING appears in the mark.
-          m->Recycle();
-          m->_Responsible  = NULL ;
-          m->OwnerIsThread = 0 ;
-          m->_recursions   = 0 ;
-          m->_SpinDuration = ObjectMonitor::Knob_SpinLimit ;   // Consider: maintain by type/class
-
-          markOop cmp = (markOop) Atomic::cmpxchg_ptr (markOopDesc::INFLATING(), object->mark_addr(), mark) ;
-          if (cmp != mark) {
-             omRelease (Self, m, true) ;
-             continue ;       // Interference -- just retry
-          }
-
-          // We've successfully installed INFLATING (0) into the mark-word.
-          // This is the only case where 0 will appear in a mark-work.
-          // Only the singular thread that successfully swings the mark-word
-          // to 0 can perform (or more precisely, complete) inflation.
-          //
-          // Why do we CAS a 0 into the mark-word instead of just CASing the
-          // mark-word from the stack-locked value directly to the new inflated state?
-          // Consider what happens when a thread unlocks a stack-locked object.
-          // It attempts to use CAS to swing the displaced header value from the
-          // on-stack basiclock back into the object header.  Recall also that the
-          // header value (hashcode, etc) can reside in (a) the object header, or
-          // (b) a displaced header associated with the stack-lock, or (c) a displaced
-          // header in an objectMonitor.  The inflate() routine must copy the header
-          // value from the basiclock on the owner's stack to the objectMonitor, all
-          // the while preserving the hashCode stability invariants.  If the owner
-          // decides to release the lock while the value is 0, the unlock will fail
-          // and control will eventually pass from slow_exit() to inflate.  The owner
-          // will then spin, waiting for the 0 value to disappear.   Put another way,
-          // the 0 causes the owner to stall if the owner happens to try to
-          // drop the lock (restoring the header from the basiclock to the object)
-          // while inflation is in-progress.  This protocol avoids races that might
-          // would otherwise permit hashCode values to change or "flicker" for an object.
-          // Critically, while object->mark is 0 mark->displaced_mark_helper() is stable.
-          // 0 serves as a "BUSY" inflate-in-progress indicator.
-
-
-          // fetch the displaced mark from the owner's stack.
-          // The owner can't die or unwind past the lock while our INFLATING
-          // object is in the mark.  Furthermore the owner can't complete
-          // an unlock on the object, either.
-          markOop dmw = mark->displaced_mark_helper() ;
-          assert (dmw->is_neutral(), "invariant") ;
-
-          // Setup monitor fields to proper values -- prepare the monitor
-          m->set_header(dmw) ;
-
-          // Optimization: if the mark->locker stack address is associated
-          // with this thread we could simply set m->_owner = Self and
-          // m->OwnerIsThread = 1. Note that a thread can inflate an object
-          // that it has stack-locked -- as might happen in wait() -- directly
-          // with CAS.  That is, we can avoid the xchg-NULL .... ST idiom.
-          m->set_owner(mark->locker());
-          m->set_object(object);
-          // TODO-FIXME: assert BasicLock->dhw != 0.
-
-          // Must preserve store ordering. The monitor state must
-          // be stable at the time of publishing the monitor address.
-          guarantee (object->mark() == markOopDesc::INFLATING(), "invariant") ;
-          object->release_set_mark(markOopDesc::encode(m));
-
-          // Hopefully the performance counters are allocated on distinct cache lines
-          // to avoid false sharing on MP systems ...
-          if (ObjectMonitor::_sync_Inflations != NULL) ObjectMonitor::_sync_Inflations->inc() ;
-          TEVENT(Inflate: overwrite stacklock) ;
-          if (TraceMonitorInflation) {
-            if (object->is_instance()) {
-              ResourceMark rm;
-              tty->print_cr("Inflating object " INTPTR_FORMAT " , mark " INTPTR_FORMAT " , type %s",
-                (intptr_t) object, (intptr_t) object->mark(),
-                Klass::cast(object->klass())->external_name());
-            }
-          }
-          return m ;
-      }
-
-      // CASE: neutral
-      // TODO-FIXME: for entry we currently inflate and then try to CAS _owner.
-      // If we know we're inflating for entry it's better to inflate by swinging a
-      // pre-locked objectMonitor pointer into the object header.   A successful
-      // CAS inflates the object *and* confers ownership to the inflating thread.
-      // In the current implementation we use a 2-step mechanism where we CAS()
-      // to inflate and then CAS() again to try to swing _owner from NULL to Self.
-      // An inflateTry() method that we could call from fast_enter() and slow_enter()
-      // would be useful.
-
-      assert (mark->is_neutral(), "invariant");
-      ObjectMonitor * m = omAlloc (Self) ;
-      // prepare m for installation - set monitor to initial state
-      m->Recycle();
-      m->set_header(mark);
-      m->set_owner(NULL);
+      // Why do we CAS a 0 into the mark-word instead of just CASing the
+      // mark-word from the stack-locked value directly to the new inflated state?
+      // Consider what happens when a thread unlocks a stack-locked object.
+      // It attempts to use CAS to swing the displaced header value from the
+      // on-stack basiclock back into the object header.  Recall also that the
+      // header value (hashcode, etc) can reside in (a) the object header, or
+      // (b) a displaced header associated with the stack-lock, or (c) a displaced
+      // header in an objectMonitor.  The inflate() routine must copy the header
+      // value from the basiclock on the owner's stack to the objectMonitor, all
+      // the while preserving the hashCode stability invariants.  If the owner
+      // decides to release the lock while the value is 0, the unlock will fail
+      // and control will eventually pass from slow_exit() to inflate.  The owner
+      // will then spin, waiting for the 0 value to disappear.   Put another way,
+      // the 0 causes the owner to stall if the owner happens to try to
+      // drop the lock (restoring the header from the basiclock to the object)
+      // while inflation is in-progress.  This protocol avoids races that might
+      // would otherwise permit hashCode values to change or "flicker" for an object.
+      // Critically, while object->mark is 0 mark->displaced_mark_helper() is stable.
+      // 0 serves as a "BUSY" inflate-in-progress indicator.
+
+
+      // fetch the displaced mark from the owner's stack.
+      // The owner can't die or unwind past the lock while our INFLATING
+      // object is in the mark.  Furthermore the owner can't complete
+      // an unlock on the object, either.
+      STIJN_MUTEX_LOCK(MVEE_Atomic_lock);
+      markOop dmw = mark->displaced_mark_helper() ;
+      assert (dmw->is_neutral(), "invariant") ;
+
+      // Setup monitor fields to proper values -- prepare the monitor
+      m->set_header(dmw) ;
+
+      // Optimization: if the mark->locker stack address is associated
+      // with this thread we could simply set m->_owner = Self and
+      // m->OwnerIsThread = 1. Note that a thread can inflate an object
+      // that it has stack-locked -- as might happen in wait() -- directly
+      // with CAS.  That is, we can avoid the xchg-NULL .... ST idiom.
+      m->set_owner(mark->locker());
       m->set_object(object);
-      m->OwnerIsThread = 1 ;
-      m->_recursions   = 0 ;
-      m->_Responsible  = NULL ;
-      m->_SpinDuration = ObjectMonitor::Knob_SpinLimit ;       // consider: keep metastats by type/class
+      // TODO-FIXME: assert BasicLock->dhw != 0.
 
-      if (Atomic::cmpxchg_ptr (markOopDesc::encode(m), object->mark_addr(), mark) != mark) {
-          m->set_object (NULL) ;
-          m->set_owner  (NULL) ;
-          m->OwnerIsThread = 0 ;
-          m->Recycle() ;
-          omRelease (Self, m, true) ;
-          m = NULL ;
-          continue ;
-          // interference - the markword changed - just retry.
-          // The state-transitions are one-way, so there's no chance of
-          // live-lock -- "Inflated" is an absorbing state.
-      }
+      // Must preserve store ordering. The monitor state must
+      // be stable at the time of publishing the monitor address.
+      guarantee (object->mark() == markOopDesc::INFLATING(), "invariant") ;
+      STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
+      object->release_set_mark(markOopDesc::encode(m));
 
-      // Hopefully the performance counters are allocated on distinct
-      // cache lines to avoid false sharing on MP systems ...
+      // Hopefully the performance counters are allocated on distinct cache lines
+      // to avoid false sharing on MP systems ...
       if (ObjectMonitor::_sync_Inflations != NULL) ObjectMonitor::_sync_Inflations->inc() ;
-      TEVENT(Inflate: overwrite neutral) ;
+      TEVENT(Inflate: overwrite stacklock) ;
       if (TraceMonitorInflation) {
-        if (object->is_instance()) {
-          ResourceMark rm;
-          tty->print_cr("Inflating object " INTPTR_FORMAT " , mark " INTPTR_FORMAT " , type %s",
-            (intptr_t) object, (intptr_t) object->mark(),
-            Klass::cast(object->klass())->external_name());
-        }
+	if (object->is_instance()) {
+	  ResourceMark rm;
+	  tty->print_cr("Inflating object " INTPTR_FORMAT " , mark " INTPTR_FORMAT " , type %s",
+			(intptr_t) object, (intptr_t) object->mark(),
+			Klass::cast(object->klass())->external_name());
+	}
       }
       return m ;
+    }
+    else
+      STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
+
+
+    // CASE: neutral
+    // TODO-FIXME: for entry we currently inflate and then try to CAS _owner.
+    // If we know we're inflating for entry it's better to inflate by swinging a
+    // pre-locked objectMonitor pointer into the object header.   A successful
+    // CAS inflates the object *and* confers ownership to the inflating thread.
+    // In the current implementation we use a 2-step mechanism where we CAS()
+    // to inflate and then CAS() again to try to swing _owner from NULL to Self.
+    // An inflateTry() method that we could call from fast_enter() and slow_enter()
+    // would be useful.
+
+    STIJN_MUTEX_LOCK(MVEE_Atomic_lock);
+    assert (mark->is_neutral(), "invariant");
+    ObjectMonitor * m = omAlloc (Self) ;
+    // prepare m for installation - set monitor to initial state
+    m->Recycle();
+    m->set_header(mark);
+    m->set_owner(NULL);
+    m->set_object(object);
+    m->OwnerIsThread = 1 ;
+    m->_recursions   = 0 ;
+    m->_Responsible  = NULL ;
+    m->_SpinDuration = ObjectMonitor::Knob_SpinLimit ;       // consider: keep metastats by type/class
+
+    if (Atomic::cmpxchg_ptr (markOopDesc::encode(m), object->mark_addr(), mark) != mark) {
+      STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
+      m->set_object (NULL) ;
+      m->set_owner  (NULL) ;
+      m->OwnerIsThread = 0 ;
+      m->Recycle() ;
+      omRelease (Self, m, true) ;
+      m = NULL ;
+      continue ;
+      // interference - the markword changed - just retry.
+      // The state-transitions are one-way, so there's no chance of
+      // live-lock -- "Inflated" is an absorbing state.
+    }
+    else
+      STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
+
+    // Hopefully the performance counters are allocated on distinct
+    // cache lines to avoid false sharing on MP systems ...
+    if (ObjectMonitor::_sync_Inflations != NULL) ObjectMonitor::_sync_Inflations->inc() ;
+    TEVENT(Inflate: overwrite neutral) ;
+    if (TraceMonitorInflation) {
+      if (object->is_instance()) {
+	ResourceMark rm;
+	tty->print_cr("Inflating object " INTPTR_FORMAT " , mark " INTPTR_FORMAT " , type %s",
+		      (intptr_t) object, (intptr_t) object->mark(),
+		      Klass::cast(object->klass())->external_name());
+      }
+    }
+    return m ;
   }
 }
 
@@ -1423,10 +1522,12 @@
 bool ObjectSynchronizer::deflate_monitor(ObjectMonitor* mid, oop obj,
                                          ObjectMonitor** FreeHeadp, ObjectMonitor** FreeTailp) {
   bool deflated;
+  STIJN_MUTEX_LOCK(MVEE_Atomic_lock);
   // Normal case ... The monitor is associated with obj.
   guarantee (obj->mark() == markOopDesc::encode(mid), "invariant") ;
   guarantee (mid == obj->mark()->monitor(), "invariant");
   guarantee (mid->header()->is_neutral(), "invariant");
+  STIJN_MUTEX_UNLOCK(MVEE_Atomic_lock);
 
   if (mid->is_busy()) {
      if (ClearResponsibleAtSTW) mid->_Responsible = NULL ;
diff -r --unified openjdk7.orig/hotspot//src/share/vm/runtime/thread.cpp openjdk7/hotspot//src/share/vm/runtime/thread.cpp
--- openjdk7.orig/hotspot//src/share/vm/runtime/thread.cpp	2012-10-17 16:25:03.625222300 +0200
+++ openjdk7/hotspot//src/share/vm/runtime/thread.cpp	2012-12-03 13:12:36.822992595 +0100
@@ -1153,10 +1153,18 @@
   this->record_stack_base_and_size();
   this->initialize_thread_local_storage();
   this->set_active_handles(JNIHandleBlock::allocate_block());
-  while(!_should_terminate) {
+
+  bool should_terminate;
+
+  pthread_mutex_lock(MVEE_Atomic_lock);
+  should_terminate = _should_terminate;
+  pthread_mutex_unlock(MVEE_Atomic_lock);
+
+  while(!should_terminate) {
     assert(watcher_thread() == Thread::current(),  "thread consistency check");
     assert(watcher_thread() == this,  "thread consistency check");
 
+    //    syscall(224, 1337, 10000001, 56, 19);
     // Calculate how long it'll be until the next PeriodicTask work
     // should be done, and sleep that amount of time.
     size_t time_to_wait = PeriodicTask::time_to_wait();
@@ -1170,7 +1178,11 @@
       for (;;) {
         int res= _SleepEvent->park(time_to_wait);
         if (res == OS_TIMEOUT || _should_terminate)
-          break;
+	  {
+	    //	    syscall(224, 1337, 10000001, 56, 20, res, _should_terminate);
+	    break;
+	  }
+
         // spurious wakeup of some kind
         jlong now = os::javaTimeNanos();
         time_to_wait -= (now - prev_time) / 1000000;
@@ -1213,8 +1225,14 @@
     // If we have no more tasks left due to dynamic disenrollment,
     // shut down the thread since we don't currently support dynamic enrollment
     if (PeriodicTask::num_tasks() == 0) {
+      pthread_mutex_lock(MVEE_Atomic_lock);      
       _should_terminate = true;
+      pthread_mutex_unlock(MVEE_Atomic_lock);
     }
+
+    pthread_mutex_lock(MVEE_Atomic_lock);
+    should_terminate = _should_terminate;
+    pthread_mutex_unlock(MVEE_Atomic_lock);
   }
 
   // Signal that it is terminated
