diff -ur orig/arch/x86/entry/syscalls/syscall_64.tbl linux-source-5.4.0/arch/x86/entry/syscalls/syscall_64.tbl
--- orig/arch/x86/entry/syscalls/syscall_64.tbl
+++ linux-source-5.4.0/arch/x86/entry/syscalls/syscall_64.tbl
@@ -364,6 +364,9 @@
 # on-the-fly for compat_sys_*() compatibility system calls if X86_X32
 # is defined.
 #
+509	common	pmvee_switch	__x64_sys_pmvee_switch
+510	common	pmvee_check 	__x64_sys_pmvee_check
+511	common	ipmon_invoke	__x64_sys_ipmon_invoke
 512	x32	rt_sigaction		__x32_compat_sys_rt_sigaction
 513	x32	rt_sigreturn		sys32_x32_rt_sigreturn
 514	x32	ioctl			__x32_compat_sys_ioctl
diff -ur orig/arch/x86/mm/pat.c linux-source-5.4.0/arch/x86/mm/pat.c
--- orig/arch/x86/mm/pat.c
+++ linux-source-5.4.0/arch/x86/mm/pat.c
@@ -1010,6 +1010,7 @@
 
 	return 0;
 }
+EXPORT_SYMBOL(track_pfn_copy);
 
 /*
  * prot is passed in as a parameter for the new mapping. If the vma has
@@ -1094,6 +1095,7 @@
 	if (vma)
 		vma->vm_flags &= ~VM_PAT;
 }
+EXPORT_SYMBOL(untrack_pfn);
 
 /*
  * untrack_pfn_moved is called, while mremapping a pfnmap for a new region,
diff -ur orig/arch/x86/mm/tlb.c linux-source-5.4.0/arch/x86/mm/tlb.c
--- orig/arch/x86/mm/tlb.c
+++ linux-source-5.4.0/arch/x86/mm/tlb.c
@@ -806,6 +806,7 @@
 	put_flush_tlb_info();
 	put_cpu();
 }
+EXPORT_SYMBOL(flush_tlb_mm_range);
 
 
 static void do_flush_tlb_all(void *info)
diff -ur orig/fs/exec.c linux-source-5.4.0/fs/exec.c
--- orig/fs/exec.c
+++ linux-source-5.4.0/fs/exec.c
@@ -1906,6 +1906,8 @@
 		putname(filename);
 	if (displaced)
 		put_files_struct(displaced);
+	current->ipmon_info.ipmon_enabled = 0;
+	current->ipmon_info.ipmon_parent_rb = NULL;
 	return retval;
 
 out:
diff -ur orig/include/linux/sched.h linux-source-5.4.0/include/linux/sched.h
--- orig/include/linux/sched.h
+++ linux-source-5.4.0/include/linux/sched.h
@@ -625,6 +625,58 @@
 	struct wake_q_node *next;
 };
 
+#ifndef ROUND_UP
+  #define ROUND_UP(x, multiple) ( (((long)(x)) + multiple - 1) & (~(multiple - 1)) )
+#endif
+
+#ifndef __NR_syscalls
+  /* this is the highest known syscall no for x86_64 in this kernel... */
+  #define __NR_syscalls 436
+#endif
+
+/* MVEE IP-MON patch */
+struct ipmon_info {
+  /* This is a bitmask in which every bit represents exactly one syscall.  If
+     the bit is set to 1, then the corresponding syscall may bypass the ptracer
+     under two conditions:
+
+     1) The syscall must have originated from IP-MON
+     2) IP-MON must pass the IP-MON key as the 7th syscall argument
+  */
+  unsigned char ipmon_masked_syscalls[ROUND_UP(510, 8) / 8];
+
+  /* Is IP-MON enabled within this thread */
+  unsigned char ipmon_enabled;
+
+  /* latest IP-MON key */
+  unsigned int  ipmon_key;
+
+  /* latest syscall we forwarded to IP-MON */
+  unsigned long ipmon_orig_syscall;
+
+  /* This is a pointer to the vm_area_struct of the IP-MON executable code
+	 section.  This area is marked as VM_IMMUTABLE and cannot be unmapped or
+	 modified in any way.  The kernel will however unmap the area when the
+	 associated mm_struct is freed.
+  */
+  struct vm_area_struct *ipmon_exec;
+
+  /* This is a pointer to the vm_area_struct of the IP-MON Replication Buffer
+     (RB). The sys_ipmon_rbcontrol syscall changes the protection flags on this
+     region. 
+  */
+  struct vm_area_struct *ipmon_rb;	
+
+  /* We need this to detach from the parent's RB in the children
+     after a fork/clone.
+  */
+  struct vm_area_struct *ipmon_parent_rb;
+
+  /* pointer to the ipmon syscall enclave entrypoint */
+  void __user *ipmon_enclave_entrypoint;
+};
+
+
 struct task_struct {
 #ifdef CONFIG_THREAD_INFO_IN_TASK
 	/*
@@ -730,6 +782,8 @@
 
 	struct mm_struct		*mm;
 	struct mm_struct		*active_mm;
+	struct ipmon_info		ipmon_info;
+	unsigned long			pmvee_last_call;
 
 	/* Per-thread vma caching: */
 	struct vmacache			vmacache;
@@ -751,6 +805,8 @@
 	/* Scheduler bits, serialized by scheduler locks: */
 	unsigned			sched_reset_on_fork:1;
 	unsigned			sched_contributes_to_load:1;
+	unsigned			ptrace_ignored_current:1;
+	unsigned			pmvee_ignored_current:1;
 	unsigned			sched_migrated:1;
 	unsigned			sched_remote_wakeup:1;
 #ifdef CONFIG_PSI
@@ -1290,6 +1346,10 @@
 	 */
 };
 
+/* Defined in kernel/sys.c */
+unsigned char ipmon_handle_bypass(struct pt_regs *regs);
+unsigned char pmvee_should_skip(struct pt_regs *regs, int entering);
+
 static inline struct pid *task_pid(struct task_struct *task)
 {
 	return task->thread_pid;
diff -ur orig/include/linux/tracehook.h linux-source-5.4.0/include/linux/tracehook.h
--- orig/include/linux/tracehook.h
+++ linux-source-5.4.0/include/linux/tracehook.h
@@ -101,6 +101,15 @@
 static inline __must_check int tracehook_report_syscall_entry(
 	struct pt_regs *regs)
 {
+    if ((current->ptrace & PT_PTRACED) && pmvee_should_skip(regs, 1))
+    {
+            return 0;
+    }
+    if ((current->ptrace & PT_PTRACED) && ipmon_handle_bypass(regs))
+    {
+            current->ptrace_ignored_current = 1;
+            return 0;
+    }
 	return ptrace_report_syscall(regs, PTRACE_EVENTMSG_SYSCALL_ENTRY);
 }
 
@@ -126,7 +135,18 @@
 	if (step)
 		user_single_step_report(regs);
 	else
-		ptrace_report_syscall(regs, PTRACE_EVENTMSG_SYSCALL_EXIT);
+	{
+        if ((current->ptrace & PT_PTRACED) && pmvee_should_skip(regs, 0))
+        {
+            return;
+        }
+        if (current->ptrace_ignored_current)
+        {
+            current->ptrace_ignored_current = 0;
+            return;
+        }
+        ptrace_report_syscall(regs, PTRACE_EVENTMSG_SYSCALL_EXIT);
+	}
 }
 
 /**
diff -ur orig/include/uapi/asm-generic/errno.h linux-source-5.4.0/include/uapi/asm-generic/errno.h
--- orig/include/uapi/asm-generic/errno.h
+++ linux-source-5.4.0/include/uapi/asm-generic/errno.h
@@ -120,4 +120,6 @@
 
 #define EHWPOISON	133	/* Memory page has hardware error */
 
+#define ENOIPMON    256 /* MVEE IP-MON patch */
+
 #endif
diff -ur orig/include/uapi/linux/prctl.h linux-source-5.4.0/include/uapi/linux/prctl.h
--- orig/include/uapi/linux/prctl.h
+++ linux-source-5.4.0/include/uapi/linux/prctl.h
@@ -234,4 +234,6 @@
 #define PR_GET_TAGGED_ADDR_CTRL		56
 # define PR_TAGGED_ADDR_ENABLE		(1UL << 0)
 
+#define PR_REGISTER_IPMON 0xb00b135 /* MVEE IP-MON patch */
+
 #endif /* _LINUX_PRCTL_H */
diff -ur orig/kernel/events/uprobes.c linux-source-5.4.0/kernel/events/uprobes.c
--- orig/kernel/events/uprobes.c
+++ linux-source-5.4.0/kernel/events/uprobes.c
@@ -1443,6 +1443,7 @@
 	if (vma_has_uprobes(vma, start, end))
 		set_bit(MMF_RECALC_UPROBES, &vma->vm_mm->flags);
 }
+EXPORT_SYMBOL(uprobe_munmap);
 
 /* Slot allocation for XOL */
 static int xol_add_vma(struct mm_struct *mm, struct xol_area *area)
@@ -1568,11 +1569,13 @@
 {
 	percpu_down_read(&dup_mmap_sem);
 }
+EXPORT_SYMBOL(uprobe_start_dup_mmap);
 
 void uprobe_end_dup_mmap(void)
 {
 	percpu_up_read(&dup_mmap_sem);
 }
+EXPORT_SYMBOL(uprobe_end_dup_mmap);
 
 void uprobe_dup_mmap(struct mm_struct *oldmm, struct mm_struct *newmm)
 {
@@ -1582,6 +1585,7 @@
 		set_bit(MMF_RECALC_UPROBES, &newmm->flags);
 	}
 }
+EXPORT_SYMBOL(uprobe_dup_mmap);
 
 /*
  *  - search for a free slot.
diff -ur orig/kernel/fork.c linux-source-5.4.0/kernel/fork.c
--- orig/kernel/fork.c
+++ linux-source-5.4.0/kernel/fork.c
@@ -366,6 +366,7 @@
 	}
 	return new;
 }
+EXPORT_SYMBOL(vm_area_dup);
 
 void vm_area_free(struct vm_area_struct *vma)
 {
@@ -967,6 +968,7 @@
 }
 
 __cacheline_aligned_in_smp DEFINE_SPINLOCK(mmlist_lock);
+EXPORT_SYMBOL(mmlist_lock);
 
 static unsigned long default_dump_filter = MMF_DUMP_FILTER_DEFAULT;
 
@@ -1887,6 +1889,11 @@
 	if (!p)
 		goto fork_out;
 
+	// Invalidate IP-MON RB in new tasks but leave IP-MON enabled.
+	// Also update parent's rb since we need to detach from that.
+	p->ipmon_info.ipmon_parent_rb = p->ipmon_info.ipmon_rb;
+	p->ipmon_info.ipmon_rb        = NULL;
+
 	/*
 	 * This _must_ happen before we call free_task(), i.e. before we jump
 	 * to any of the bad_fork_* labels. This is to avoid freeing
diff -ur orig/kernel/sys.c linux-source-5.4.0/kernel/sys.c
--- orig/kernel/sys.c
+++ linux-source-5.4.0/kernel/sys.c
@@ -60,6 +60,8 @@
 #include <linux/rcupdate.h>
 #include <linux/uidgid.h>
 #include <linux/cred.h>
+ 
+#include <linux/random.h>
 
 #include <linux/nospec.h>
 
@@ -2243,6 +2245,290 @@
 	return error;
 }
 
+ 
+static inline unsigned char ipmon_is_unchecked_syscall(unsigned char *mask, unsigned long syscall_no)
+{
+  unsigned long no_to_byte, bit_in_byte;
+
+  /* This is not very concise but the compiler will optimize it anyway... */
+  if (syscall_no > ROUND_UP(__NR_pmvee_check, 8))
+    return 0;
+
+  no_to_byte  = syscall_no / 8;
+  bit_in_byte = syscall_no % 8;
+
+  if (mask[no_to_byte] & (1 << (7 - bit_in_byte)))
+    return 1;
+  return 0;
+}
+
+static inline void ipmon_set_unchecked_syscall(unsigned char *mask, unsigned long syscall_no, unsigned char unchecked)
+{
+  unsigned long no_to_byte, bit_in_byte;
+
+  if (syscall_no > ROUND_UP(__NR_pmvee_check, 8))
+    return;
+
+  no_to_byte  = syscall_no / 8;
+  bit_in_byte = syscall_no % 8;
+
+  if (unchecked)
+    mask[no_to_byte] |= (1 << (7 - bit_in_byte));
+  else
+    mask[no_to_byte] &= ~(1 << (7 - bit_in_byte));
+}
+
+// This syscall normally does not get called directly. We change the syscall
+// number to __NR_ipmon_invoke to land here (see ipmon_handle_bypass func below)
+SYSCALL_DEFINE0(ipmon_invoke)
+{
+	if (!current->ipmon_info.ipmon_enabled)
+	{
+		printk("sys_ipmon_invoke DENIED - no IP-MON registered for this thread: %d\n", current->pid);
+        return -ENOIPMON;
+	}
+
+	// Generate key
+	current->ipmon_info.ipmon_key = get_random_int();	
+
+	// TODO: The code below can be optimized! The stack pointer is still valid
+	// upon switching to kernel space and the user-mode stack is still
+	// accessible.
+
+	// push the syscall return site onto the stack as the return address
+	if (copy_to_user((void __user*)(task_pt_regs(current)->sp - sizeof(unsigned long)),
+					 &task_pt_regs(current)->ip,
+					 sizeof(unsigned long)))
+	{
+		return -EFAULT;
+	}
+
+	// adjust the stack pointer
+	task_pt_regs(current)->sp -= sizeof(unsigned long);
+
+	// return the replication buffer pointer in register r11
+	if (current->ipmon_info.ipmon_rb)
+	{
+		task_pt_regs(current)->r11 = current->ipmon_info.ipmon_rb->vm_start;
+	}
+	else
+	{
+		// set the highest bit to inform IP-MON that it needs to detach from the parent's RB and attach to a new RB
+		// Note: the highest bit of user space addresses is always 0
+		task_pt_regs(current)->r11 = current->ipmon_info.ipmon_parent_rb->vm_start | (1UL << (sizeof(unsigned long)*8 - 1));
+	}
+
+	// return the magic key in register rcx
+	task_pt_regs(current)->cx = current->ipmon_info.ipmon_key;
+
+	// adjust the instruction pointer to jump to the enclave
+	task_pt_regs(current)->ip = (unsigned long)current->ipmon_info.ipmon_enclave_entrypoint;
+	
+
+	return current->ipmon_info.ipmon_orig_syscall;
+}
+
+// This gets called at each syscall entry site from include/linux/tracehook.h.
+// If we return 1, the syscall will not be reported to the ptracer.
+unsigned char ipmon_handle_bypass(struct pt_regs *regs)
+{
+	unsigned char result = 0;
+	unsigned long syscall_no = regs->orig_ax;
+
+	// Never report sched_yield
+	if (syscall_no == __NR_sched_yield)
+	{
+		result = 1;
+		goto out;
+	}
+
+	if (likely(current->ipmon_info.ipmon_enabled))
+	{
+		unsigned long r12        = regs->r12;
+		unsigned long pc         = regs->ip;
+		unsigned char in_ipmon   = (pc < current->ipmon_info.ipmon_exec->vm_start || 
+									pc >= current->ipmon_info.ipmon_exec->vm_end) ? 0 : 1;
+		unsigned char ipmon_wants_bypass = (r12 == current->ipmon_info.ipmon_key) ? 1 : 0;
+		unsigned char whitelisted_call = ipmon_is_unchecked_syscall(current->ipmon_info.ipmon_masked_syscalls, syscall_no) ? 1 : 0;		
+
+		// Call from outside IP-MON. If the syscall number is in the unchecked
+		// list, then don't execute the syscall and return into the IP-MON
+		// enclave instead.
+        //
+		// If the syscall number is NOT in the unchecked list, then report to
+		// the ptracer, and leave the syscall number untouched.
+		if (!in_ipmon)
+		{
+			if (!whitelisted_call)
+				goto out;
+
+			current->ipmon_info.ipmon_orig_syscall = syscall_no;
+			regs->orig_ax = __NR_ipmon_invoke;
+			// we obviously don't want to report the sys_ipmon_invoke call to
+			// the ptracer
+			result = 1;
+		}
+		else
+		{
+			if (!ipmon_wants_bypass)
+				goto out;
+
+			// OK. Bypass approved. We can now revoke IP-MON's right to execute
+			// any syscalls other than sched_yield and futex
+			current->ipmon_info.ipmon_orig_syscall = (unsigned long)-1;
+			result = 1;
+		}
+	}
+
+out:
+	return result;
+}
+
+/* Register IP-MON with the kernel so it gets super duper privileges 
+   
+   Checks if:
+   * replication_buffer_base is in a valid region
+   * enclave_entrypoint is within the executable code region that registered IP-MON
+*/
+static int prctl_register_ipmon(unsigned char __user *syscall_mask, 
+								unsigned long syscall_mask_size,
+								void __user *replication_buffer_base,
+								void __user *enclave_entrypoint)
+{
+	int error = 0;
+	struct vm_area_struct *ipmon_vma = NULL, *ipmon_rb_vma = NULL;
+	unsigned long kernel_syscall_mask_size = ROUND_UP(__NR_pmvee_check, 8) / 8;
+	unsigned char syscall_mask_copy[kernel_syscall_mask_size];
+	unsigned char syscall_mask_check[kernel_syscall_mask_size];
+	unsigned long program_counter;
+
+	/* This only makes sense if we're being ptraced.  If we allow IP-MON to
+	   register before the process is being traced, then the tracer does not get
+	   a chance to inspect the syscall mask and to modify it if needed. */
+	if (!current->ptrace)
+    {
+		printk("process %s (%d) attempted to register an IP-MON but is not being ptraced\n", current->comm, current->pid);
+		return -ESRCH;
+    }
+
+	/* see if we already have an IP-MON */
+	if (current->ipmon_info.ipmon_enabled && /* we now leave ipmon enabled across sys_clone(CLONE_VM) */
+		current->ipmon_info.ipmon_rb)        /* but invalidate the RB pointer in that case */
+    {
+		printk("process %s (%d) attempted to register an IP-MON it has registered an IP-MON before\n", current->comm, current->pid);
+		return -EPERM;
+    }
+
+	/* IP-MON may not mask calls that this kernel doesn't know - Do some sanity
+	 * checks here on the reported syscall mask */
+	if (syscall_mask_size > kernel_syscall_mask_size)
+    {
+		printk("process %s (%d) attempted to register an IP-MON but the syscall_mask_size is invalid (%lu)\n", current->comm, current->pid, syscall_mask_size);
+		return -EINVAL;
+    }
+
+	/* OK. We did not have an initialized IP-MON yet and the mask size seems valid. */
+	if (copy_from_user(syscall_mask_copy, syscall_mask, syscall_mask_size))
+    {
+		printk("process %s (%d) attempted to register an IP-MON but the syscall_mask_address is invalid (%lx)\n", current->comm, current->pid, (unsigned long)syscall_mask);
+		return -EFAULT;
+    }
+
+	if (syscall_mask_size < kernel_syscall_mask_size)
+		memset(syscall_mask_copy + syscall_mask_size, 0, kernel_syscall_mask_size - syscall_mask_size);
+
+	/* Now check if the IP-MON is trying to be naughty */
+	memcpy(syscall_mask_check, syscall_mask_copy, kernel_syscall_mask_size);
+
+	/* ok so the IP-MON has not been initialized yet and the program is not trying to
+	   unblock any system calls that we will not allow */
+	down_read(&current->mm->mmap_sem);
+
+	/* Fetch the program counter and map it to the VMA. IP-MON must register
+	   itself!  Therefore, the program counter we see here must point to
+	   IP-MON. It is the ptracer's responsibility to verify that the expected
+	   IP-MON is registering, rather than something that impersonates IP-MON! */
+	program_counter = task_pt_regs(current)->ip;
+
+	ipmon_vma = find_vma(current->mm, program_counter);
+	if (!ipmon_vma || ipmon_vma->vm_start > program_counter)
+    {
+		error = -EINVAL;
+		printk("process %s (%d) attempted to register an IP-MON but we could not find the VMA associated with PC: %lx\n", current->comm, current->pid, program_counter);
+		goto out;
+    }
+
+	/* See if we can find the repication buffer */
+	ipmon_rb_vma = find_vma(current->mm, (unsigned long)replication_buffer_base);
+	if (!ipmon_rb_vma || ipmon_rb_vma->vm_start > (unsigned long)replication_buffer_base)
+    {
+		error = -EINVAL;
+		printk("process %s (%d) attempted to register an IP-MON but we could not find the VMA associated with Replication Buffer base: %lx\n", current->comm, current->pid, (unsigned long)replication_buffer_base);
+		goto out;
+    }
+
+	/* Finally, check if the enclave entrypoint is within the vma that registered IP-MON */
+	if ((unsigned long)enclave_entrypoint < ipmon_vma->vm_start ||
+		(unsigned long)enclave_entrypoint >= ipmon_vma->vm_end)
+	{
+		error = -EINVAL;
+		printk("process %s (%d) attempted to register an IP-MON but this enclave entrypoint is invalid: %lx\n", current->comm, current->pid, (unsigned long)enclave_entrypoint);
+		goto out;
+	}
+  
+	/* OK! We have everything... */
+	current->ipmon_info.ipmon_exec               = ipmon_vma;
+	current->ipmon_info.ipmon_rb                 = ipmon_rb_vma;
+	current->ipmon_info.ipmon_enclave_entrypoint = enclave_entrypoint;	
+	current->ipmon_info.ipmon_enabled            = 1;
+	current->ipmon_info.ipmon_orig_syscall       = (unsigned long)-1;	
+	memcpy(current->ipmon_info.ipmon_masked_syscalls, syscall_mask_copy, kernel_syscall_mask_size);
+
+	printk("process %s (%d) registered an IP-MON at: %lx-%lx\n", current->comm, current->pid, 
+		   ipmon_vma->vm_start, ipmon_vma->vm_end);
+out:
+	up_read(&current->mm->mmap_sem);
+	return 0;
+}
+
+
+long (*pmvee_switch_stub) (pid_t source, pid_t destination, unsigned long from, unsigned long size_one, unsigned long size_two, unsigned long flags) = NULL;
+EXPORT_SYMBOL(pmvee_switch_stub);
+
+SYSCALL_DEFINE6(pmvee_switch, pid_t, source, pid_t, destination, unsigned long, from, unsigned long, size_one, unsigned long, size_two, unsigned long, flags)
+{
+	if (pmvee_switch_stub)
+		return pmvee_switch_stub(source, destination, from, size_one, size_two, flags);
+	else
+		printk("system call pmvee_switch not implemented.\n");
+
+	return ENOSYS;
+}
+
+
+long (*pmvee_check_stub) (pid_t source, pid_t destination, unsigned long from, unsigned long size_one, unsigned long size_two, unsigned long flags) = NULL;
+EXPORT_SYMBOL(pmvee_check_stub);
+
+SYSCALL_DEFINE6(pmvee_check, pid_t, source, pid_t, destination, unsigned long, from, unsigned long, size_one, unsigned long, size_two, unsigned long, flags)
+{
+	if (pmvee_check_stub)
+		return pmvee_check_stub(source, destination, from, size_one, size_two, flags);
+	else
+		printk("system call pmvee_check not implemented.\n");
+
+	return ENOSYS;
+}
+
+
+unsigned char (*pmvee_should_skip_stub) (struct pt_regs *regs, int entering) = NULL;
+EXPORT_SYMBOL(pmvee_should_skip_stub);
+
+unsigned char pmvee_should_skip(struct pt_regs *regs, int entering)
+{
+	return (pmvee_should_skip_stub && pmvee_should_skip_stub(regs, entering));
+}
+
+
 #ifdef CONFIG_CHECKPOINT_RESTORE
 static int prctl_get_tid_address(struct task_struct *me, int __user **tid_addr)
 {
@@ -2511,6 +2797,9 @@
 			return -EINVAL;
 		error = GET_TAGGED_ADDR_CTRL();
 		break;
+	case PR_REGISTER_IPMON:
+		error = prctl_register_ipmon((unsigned char __user*)arg2, arg3, (void __user*)arg4, (void __user*)arg5);
+		break;
 	default:
 		error = -EINVAL;
 		break;
diff -ur orig/mm/interval_tree.c linux-source-5.4.0/mm/interval_tree.c
--- orig/mm/interval_tree.c
+++ linux-source-5.4.0/mm/interval_tree.c
@@ -57,6 +57,7 @@
 	rb_insert_augmented(&node->shared.rb, &root->rb_root,
 			    &vma_interval_tree_augment);
 }
+EXPORT_SYMBOL(vma_interval_tree_insert_after);
 
 static inline unsigned long avc_start_pgoff(struct anon_vma_chain *avc)
 {
diff -ur orig/mm/memory.c linux-source-5.4.0/mm/memory.c
--- orig/mm/memory.c
+++ linux-source-5.4.0/mm/memory.c
@@ -167,6 +167,7 @@
 	}
 	current->rss_stat.events = 0;
 }
+EXPORT_SYMBOL(sync_mm_rss);
 
 static void add_mm_counter_fast(struct mm_struct *mm, int member, int val)
 {
@@ -637,6 +638,7 @@
 out:
 	return pfn_to_page(pfn);
 }
+EXPORT_SYMBOL(vm_normal_page);
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 struct page *vm_normal_page_pmd(struct vm_area_struct *vma, unsigned long addr,
@@ -1012,6 +1014,7 @@
 		mmu_notifier_invalidate_range_end(&range);
 	return ret;
 }
+EXPORT_SYMBOL(copy_page_range);
 
 /* Whether we should zap all COWed (private) pages too */
 static inline bool should_zap_cows(struct zap_details *details)
diff -ur orig/mm/mmap.c linux-source-5.4.0/mm/mmap.c
--- orig/mm/mmap.c
+++ linux-source-5.4.0/mm/mmap.c
@@ -132,6 +132,7 @@
 	/* remove_protection_ptes reads vma->vm_page_prot without mmap_sem */
 	WRITE_ONCE(vma->vm_page_prot, vm_page_prot);
 }
+EXPORT_SYMBOL(vma_set_page_prot);
 
 /*
  * Requires inode->i_mapping->i_mmap_rwsem
@@ -608,6 +609,7 @@
 	vma_gap_update(vma);
 	vma_rb_insert(vma, &mm->mm_rb);
 }
+EXPORT_SYMBOL(__vma_link_rb);
 
 static void __vma_link_file(struct vm_area_struct *vma)
 {
@@ -2865,6 +2867,7 @@
 
 	return downgrade ? 1 : 0;
 }
+EXPORT_SYMBOL(__do_munmap);
 
 int do_munmap(struct mm_struct *mm, unsigned long start, size_t len,
 	      struct list_head *uf)
diff -ur orig/mm/mmu_gather.c linux-source-5.4.0/mm/mmu_gather.c
--- orig/mm/mmu_gather.c
+++ linux-source-5.4.0/mm/mmu_gather.c
@@ -88,6 +88,7 @@
 
 	return false;
 }
+EXPORT_SYMBOL(__tlb_remove_page_size);
 
 #endif /* HAVE_MMU_GATHER_NO_GATHER */
 
@@ -195,6 +196,7 @@
 	tlb_flush_mmu_tlbonly(tlb);
 	tlb_flush_mmu_free(tlb);
 }
+EXPORT_SYMBOL(tlb_flush_mmu);
 
 /**
  * tlb_gather_mmu - initialize an mmu_gather structure for page-table tear-down
@@ -235,6 +237,7 @@
 	__tlb_reset_range(tlb);
 	inc_tlb_flush_pending(tlb->mm);
 }
+EXPORT_SYMBOL(tlb_gather_mmu);
 
 /**
  * tlb_finish_mmu - finish an mmu_gather structure
@@ -281,3 +284,4 @@
 #endif
 	dec_tlb_flush_pending(tlb->mm);
 }
+EXPORT_SYMBOL(tlb_finish_mmu);
diff -ur orig/mm/mmu_notifier.c linux-source-5.4.0/mm/mmu_notifier.c
--- orig/mm/mmu_notifier.c
+++ linux-source-5.4.0/mm/mmu_notifier.c
@@ -189,6 +189,7 @@
 
 	return ret;
 }
+EXPORT_SYMBOL(__mmu_notifier_invalidate_range_start);
 
 void __mmu_notifier_invalidate_range_end(struct mmu_notifier_range *range,
 					 bool only_end)
@@ -227,6 +228,7 @@
 	srcu_read_unlock(&srcu, id);
 	lock_map_release(&__mmu_notifier_invalidate_range_start_map);
 }
+EXPORT_SYMBOL(__mmu_notifier_invalidate_range_end);
 
 void __mmu_notifier_invalidate_range(struct mm_struct *mm,
 				  unsigned long start, unsigned long end)
@@ -241,6 +243,7 @@
 	}
 	srcu_read_unlock(&srcu, id);
 }
+EXPORT_SYMBOL(__mmu_notifier_invalidate_range);
 
 /*
  * Same as mmu_notifier_register but here the caller must hold the
diff -ur orig/mm/mprotect.c linux-source-5.4.0/mm/mprotect.c
--- orig/mm/mprotect.c
+++ linux-source-5.4.0/mm/mprotect.c
@@ -327,6 +327,7 @@
 
 	return pages;
 }
+EXPORT_SYMBOL(change_protection);
 
 unsigned long change_protection(struct vm_area_struct *vma, unsigned long start,
 		       unsigned long end, pgprot_t newprot,
diff -ur orig/mm/pgtable-generic.c linux-source-5.4.0/mm/pgtable-generic.c
--- orig/mm/pgtable-generic.c
+++ linux-source-5.4.0/mm/pgtable-generic.c
@@ -23,24 +23,28 @@
 	pgd_ERROR(*pgd);
 	pgd_clear(pgd);
 }
+EXPORT_SYMBOL(pgd_clear_bad);
 
 void p4d_clear_bad(p4d_t *p4d)
 {
 	p4d_ERROR(*p4d);
 	p4d_clear(p4d);
 }
+EXPORT_SYMBOL(p4d_clear_bad);
 
 void pud_clear_bad(pud_t *pud)
 {
 	pud_ERROR(*pud);
 	pud_clear(pud);
 }
+EXPORT_SYMBOL(pud_clear_bad);
 
 void pmd_clear_bad(pmd_t *pmd)
 {
 	pmd_ERROR(*pmd);
 	pmd_clear(pmd);
 }
+EXPORT_SYMBOL(pmd_clear_bad);
 
 #ifndef __HAVE_ARCH_PTEP_SET_ACCESS_FLAGS
 /*
diff -ur orig/mm/prfile.c linux-source-5.4.0/mm/prfile.c
--- orig/mm/prfile.c
+++ linux-source-5.4.0/mm/prfile.c
@@ -53,6 +53,7 @@
 	if (f && pr)
 		get_file(pr);
 }
+EXPORT_SYMBOL(vma_do_get_file);
 
 void vma_do_fput(struct vm_area_struct *vma, const char func[], int line)
 {
diff -ur orig/mm/rmap.c linux-source-5.4.0/mm/rmap.c
--- orig/mm/rmap.c
+++ linux-source-5.4.0/mm/rmap.c
@@ -381,6 +381,7 @@
 	unlink_anon_vmas(vma);
 	return -ENOMEM;
 }
+EXPORT_SYMBOL(anon_vma_fork);
 
 void unlink_anon_vmas(struct vm_area_struct *vma)
 {
@@ -1347,6 +1348,7 @@
 	 * faster for those pages still in swapcache.
 	 */
 }
+EXPORT_SYMBOL(page_remove_rmap);
 
 /*
  * @arg: enum ttu_flags will be passed to this argument
diff -ur orig/mm/swap.c linux-source-5.4.0/mm/swap.c
--- orig/mm/swap.c
+++ linux-source-5.4.0/mm/swap.c
@@ -689,6 +689,7 @@
 	lru_add_drain_cpu(get_cpu());
 	put_cpu();
 }
+EXPORT_SYMBOL(lru_add_drain);
 
 #ifdef CONFIG_SMP
 
diff -ur orig/mm/swapfile.c linux-source-5.4.0/mm/swapfile.c
--- orig/mm/swapfile.c
+++ linux-source-5.4.0/mm/swapfile.c
@@ -1768,6 +1768,7 @@
 	}
 	return p != NULL;
 }
+EXPORT_SYMBOL(free_swap_and_cache);
 
 #ifdef CONFIG_HIBERNATION
 /*
@@ -3491,6 +3492,7 @@
 		err = add_swap_count_continuation(entry, GFP_ATOMIC);
 	return err;
 }
+EXPORT_SYMBOL(swap_duplicate);
 
 /*
  * @entry: swap entry for which we allocate swap cache.
@@ -3649,6 +3651,7 @@
 		__free_page(page);
 	return ret;
 }
+EXPORT_SYMBOL(add_swap_count_continuation);
 
 /*
  * swap_count_continued - when the original swap_map count is incremented
diff -ur orig/mm/util.c linux-source-5.4.0/mm/util.c
--- orig/mm/util.c
+++ linux-source-5.4.0/mm/util.c
@@ -660,6 +660,7 @@
 	page = compound_head(page);
 	return __page_rmapping(page);
 }
+EXPORT_SYMBOL(page_rmapping);
 
 /*
  * Return true if this page is mapped into pagetables.
