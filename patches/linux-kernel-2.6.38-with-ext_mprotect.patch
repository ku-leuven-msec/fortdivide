diff -r --unified linux-source-2.6.38/arch/x86/kernel/ptrace.c linux-source-2.6.38-MVEE//arch/x86/kernel/ptrace.c
--- linux-source-2.6.38/arch/x86/kernel/ptrace.c	2011-07-29 21:04:36.000000000 +0200
+++ linux-source-2.6.38-MVEE//arch/x86/kernel/ptrace.c	2011-10-17 16:04:14.857269989 +0200
@@ -813,6 +813,31 @@
 #endif
 }
 
+/*
+int ptrace_ext_syscallctl(struct task_struct* child, unsigned long addr, unsigned long data)
+{
+	if (addr < 0 || addr > MAX_SYSCALLS)
+		return -EINVAL;
+	if (data)
+	{
+#ifdef PTRACE_EXT_DEBUG
+		printk(KERN_INFO "ptrace_ext - masked syscall: %d for PID: %d\n", addr, child->pid);
+#endif
+		child->ptrace_syscall_mask[addr / sizeof(unsigned int)]
+			|= (0x1 << addr % sizeof(unsigned int));
+	}
+	else
+	{
+#ifdef PTRACE_EXT_DEBUG
+		printk(KERN_INFO "ptrace_ext - unmasked syscall: %d for PID: %d\n", addr, child->pid);
+#endif
+		child->ptrace_syscall_mask[addr / sizeof(unsigned int)]
+		        &= ~(0x1 << addr % sizeof(unsigned int));
+	}
+	return 0;
+}
+*/
+
 #if defined CONFIG_X86_32 || defined CONFIG_IA32_EMULATION
 static const struct user_regset_view user_x86_32_view; /* Initialized below. */
 #endif
@@ -926,6 +951,12 @@
 		break;
 #endif
 
+	/*
+	case PTRACE_EXT_SYSCALLCTL:
+		ret = ptrace_ext_syscallctl(child, data, addr);
+		break;
+	*/
+
 	default:
 		ret = ptrace_request(child, request, addr, data);
 		break;
diff -r --unified linux-source-2.6.38/fs/exec.c linux-source-2.6.38-MVEE//fs/exec.c
--- linux-source-2.6.38/fs/exec.c	2011-07-29 21:04:36.000000000 +0200
+++ linux-source-2.6.38-MVEE//fs/exec.c	2011-10-12 17:19:24.019858386 +0200
@@ -672,7 +672,7 @@
 	vm_flags |= mm->def_flags;
 	vm_flags |= VM_STACK_INCOMPLETE_SETUP;
 
-	ret = mprotect_fixup(vma, &prev, vma->vm_start, vma->vm_end,
+	ret = mprotect_fixup(current, vma, &prev, vma->vm_start, vma->vm_end,
 			vm_flags);
 	if (ret)
 		goto out_unlock;
diff -r --unified linux-source-2.6.38/include/linux/mm.h linux-source-2.6.38-MVEE//include/linux/mm.h
--- linux-source-2.6.38/include/linux/mm.h	2011-07-29 21:04:36.000000000 +0200
+++ linux-source-2.6.38-MVEE//include/linux/mm.h	2011-10-12 16:27:57.476890670 +0200
@@ -1027,9 +1027,11 @@
 extern unsigned long do_mremap(unsigned long addr,
 			       unsigned long old_len, unsigned long new_len,
 			       unsigned long flags, unsigned long new_addr);
-extern int mprotect_fixup(struct vm_area_struct *vma,
+extern int mprotect_fixup(struct task_struct* task, struct vm_area_struct *vma,
 			  struct vm_area_struct **pprev, unsigned long start,
 			  unsigned long end, unsigned long newflags);
+extern int do_mprotect(struct task_struct* task, unsigned long start,
+		size_t len, unsigned long prot);
 
 /*
  * doesn't attempt to fault and will return short.
diff -r --unified linux-source-2.6.38/include/linux/ptrace.h linux-source-2.6.38-MVEE//include/linux/ptrace.h
--- linux-source-2.6.38/include/linux/ptrace.h	2011-07-29 21:04:36.000000000 +0200
+++ linux-source-2.6.38-MVEE//include/linux/ptrace.h	2011-10-06 15:27:35.343054861 +0200
@@ -1,5 +1,8 @@
 #ifndef _LINUX_PTRACE_H
 #define _LINUX_PTRACE_H
+
+#include <linux/types.h>
+
 /* ptrace.h */
 /* structs and defines to help the user use the ptrace system call. */
 
@@ -47,6 +50,42 @@
 #define PTRACE_GETREGSET	0x4204
 #define PTRACE_SETREGSET	0x4205
 
+/*
+ * Ptrace extensions for multi-variant execution
+ */
+#define PTRACE_EXT_COPYMEM      0x4206
+#define PTRACE_EXT_COPYSTRING	0x4207
+#define PTRACE_EXT_SYSCALLCTL   0x4208
+#define PTRACE_EXT_MPROTECT	0x4209
+// #define PTRACE_EXT_DEBUG
+
+/* Passed to sys_ptrace through the data field */
+struct pt_copymem
+{
+	pid_t          source_pid;   		/* PID of the source process */
+	unsigned long  source_va;    		/* Virtual Address of the source buffer */
+	pid_t          dest_pid;     		/* PID of the destination process */
+	unsigned long  dest_va;      		/* Virtual Address of the destination buffer */
+	unsigned long  copy_size;    		/* */
+};
+
+/* Passed to sys_ptrace through the data field */
+struct pt_copystring
+{
+	unsigned long	source_va;		/* Virtual Address of the source string */
+	unsigned long	dest_buffer_va;		/* Virtual Address of the destination buffer */
+	unsigned long	dest_buffer_size;	/* Size of the destination buffer - if the source string doesn't fit in here, an error is returned */
+	unsigned long	out_string_size;	/* The kernel will write the string size here */
+};
+
+/* Passed to sys_ptrace through the data field */
+struct pt_mprotect
+{
+	unsigned long	mem_start;		/* Start Virtual Address of the memrange to protect */
+	unsigned long	mem_len;		/* Length of the range in bytes */
+	unsigned long	mem_prot;		/* New protection flags */
+};
+
 /* options set using PTRACE_SETOPTIONS */
 #define PTRACE_O_TRACESYSGOOD	0x00000001
 #define PTRACE_O_TRACEFORK	0x00000002
@@ -113,6 +152,7 @@
 			  struct task_struct *new_parent);
 extern void __ptrace_unlink(struct task_struct *child);
 extern void exit_ptrace(struct task_struct *tracer);
+extern struct task_struct *ptrace_get_task_struct(pid_t pid);
 #define PTRACE_MODE_READ   1
 #define PTRACE_MODE_ATTACH 2
 /* Returns 0 on success, -errno on denial. */
diff -r --unified linux-source-2.6.38/include/linux/sched.h linux-source-2.6.38-MVEE//include/linux/sched.h
--- linux-source-2.6.38/include/linux/sched.h	2011-07-29 21:04:36.000000000 +0200
+++ linux-source-2.6.38-MVEE//include/linux/sched.h	2012-12-12 15:25:48.932218640 +0100
@@ -1541,6 +1541,16 @@
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
 	atomic_t ptrace_bp_refcnt;
 #endif
+	/*
+         * ptrace MVEE extension. 1 bit per syscall num. If the syscall is masked, the
+         * process will not be stopped upon entering/exiting that syscall, even if the
+         * process is being ptraced.
+         */
+        /* there ought to be a better way to do this. x86_64 has __NR_syscall_max... */
+        //#define MAX_SYSCALLS 350
+        /* did the current syscall bypass ptrace? */
+        unsigned int ptrace_ignored_current;
+  /*unsigned int ptrace_syscall_mask[MAX_SYSCALLS/sizeof(unsigned int)];*/
 };
 
 /* Future-safe accessor for struct task_struct's cpus_allowed. */
diff -r --unified linux-source-2.6.38/include/linux/tracehook.h linux-source-2.6.38-MVEE//include/linux/tracehook.h
--- linux-source-2.6.38/include/linux/tracehook.h	2011-03-15 02:20:32.000000000 +0100
+++ linux-source-2.6.38-MVEE//include/linux/tracehook.h	2012-12-12 15:27:52.181962779 +0100
@@ -52,6 +52,44 @@
 struct linux_binprm;
 
 /**
+ * ptrace_ext_syscall_masked - returns the masked bit for the specified
+ * syscall in the current task
+ *
+ * @syscall_nr:		number of the syscall to check
+ *
+ * Return masked bit
+ */
+static inline int ptrace_ext_syscall_masked(unsigned int nr)
+{
+  /* sched_yield = unsynced */
+  return nr==158 ? 1 : 0;
+}
+/*
+static inline int ptrace_ext_syscall_masked(unsigned int syscall_nr)
+{
+	return (get_current()->ptrace_syscall_mask[(syscall_nr / sizeof(unsigned int))]
+	               >> (syscall_nr % sizeof(unsigned int)))
+	       & 0x1;
+}
+*/
+
+/**
+ * ptrace_ext_can_bypass - checks whether or not this syscall can
+ * bypass the debugger.
+ *
+ * @syscall_nr:		number of the syscall the task is trying
+ * to execute
+ *
+ * Return nonzero if task can bypass ptrace.
+ */
+static inline int ptrace_ext_can_bypass(unsigned int syscall_nr)
+{
+  /*	if (syscall_nr < 0 || syscall_nr > MAX_SYSCALLS)
+	return 0;*/
+	return ptrace_ext_syscall_masked(syscall_nr);
+}
+
+/**
  * tracehook_expect_breakpoints - guess if task memory might be touched
  * @task:		current task, making a new mapping
  *
@@ -111,6 +149,12 @@
 static inline __must_check int tracehook_report_syscall_entry(
 	struct pt_regs *regs)
 {
+	/* check if the call is allowed to bypass ptrace */	
+	if (ptrace_ext_can_bypass(regs->orig_ax))
+	{
+		get_current()->ptrace_ignored_current = 1;
+		return 0;
+	}	
 	ptrace_report_syscall(regs);
 	return 0;
 }
@@ -140,6 +184,12 @@
 		force_sig_info(SIGTRAP, &info, current);
 		return;
 	}
+       
+	if (get_current()->ptrace_ignored_current)
+	{
+		get_current()->ptrace_ignored_current = 0;
+		return;
+	}
 
 	ptrace_report_syscall(regs);
 }
diff -r --unified linux-source-2.6.38/kernel/ptrace.c linux-source-2.6.38-MVEE//kernel/ptrace.c
--- linux-source-2.6.38/kernel/ptrace.c	2011-07-29 21:04:36.000000000 +0200
+++ linux-source-2.6.38-MVEE//kernel/ptrace.c	2012-12-12 15:51:23.815248116 +0100
@@ -561,6 +561,315 @@
 
 #endif
 
+/* FIXME: copymem doesn't unpin source pages properly (multipage copies) */
+int ptrace_copymem(void __user* datavp)
+{
+	struct task_struct* 	source		= NULL;
+	struct task_struct* 	dest		= NULL;
+	struct mm_struct*	source_mm	= NULL;
+	struct mm_struct*	dest_mm		= NULL;
+	struct page* 		source_page	= NULL;
+	struct page* 		dest_page_low	= NULL;
+	struct page*		dest_page_high	= NULL;
+	struct vm_area_struct*	source_vma	= NULL;
+	struct vm_area_struct*	dest_vma_low	= NULL;
+	struct vm_area_struct*	dest_vma_high	= NULL;
+	void*			source_addr	= NULL;
+	void*			dest_addr_low	= NULL;
+	void*			dest_addr_high	= NULL;
+	unsigned long 		source_offset	= 0;
+	unsigned long		dest_offset	= 0;
+	unsigned long		to_copy		= 0;
+	unsigned long		to_copy_low	= 0;
+	unsigned long		to_copy_high	= 0;
+	unsigned long		bytes_copied	= 0;
+	unsigned long 		bytes_remaining = 0;
+	int			ret		= 0;
+	int			from_current	= 0;		// Are we copying from the VA of the current process?
+	int			to_current	= 0;		// Are we copying to the VA of the current process?
+	struct pt_copymem 	mem;
+
+	/* Fetch source/dest pids and addresses from issuing process' VA space */
+	if (unlikely(copy_from_user(&mem, datavp, sizeof(struct pt_copymem))))
+		return -EFAULT;
+
+	/* Check if we're copying from/to the current process */
+	if (current->pid == mem.source_pid)
+		from_current = 1;
+	else if (current->pid == mem.dest_pid)
+		to_current = 1;
+
+	/* Get source/dest task structs and mm references */
+	if (!from_current)
+	{
+		source = ptrace_get_task_struct(mem.source_pid);
+		source_mm = get_task_mm(source);
+		if (unlikely(!source_mm))
+			goto out_ret;
+		down_read(&source_mm->mmap_sem);
+	}
+	if (!to_current)
+	{
+		dest   	  = ptrace_get_task_struct(mem.dest_pid);
+		dest_mm   = get_task_mm(dest);
+		if (unlikely(!dest_mm))
+		{
+			to_current = 1;
+			put_task_struct(dest);
+			goto out;
+		}
+		down_read(&dest_mm->mmap_sem);
+	}
+
+	bytes_remaining = mem.copy_size;
+	while (bytes_remaining > 0)
+	{
+		if (!from_current)
+		{
+			/* pin source page */
+			ret = get_user_pages(source, source_mm, mem.source_va
+				+ bytes_copied, 1, 0, 1, &source_page, &source_vma);
+
+			if (ret <= 0)
+				goto out;
+
+			/* map into kmem */
+			source_addr = kmap(source_page);
+		}
+
+		/* offset of the data block within the source page */
+		source_offset = (mem.source_va + bytes_copied) & (PAGE_SIZE-1);
+
+		/* bytes we should copy from this page */
+		to_copy = ((PAGE_SIZE - source_offset) >= bytes_remaining) ?
+			bytes_remaining : PAGE_SIZE - source_offset;
+		to_copy_low = to_copy;
+		to_copy_high = 0;
+
+		if (!to_current)
+		{
+			/* now pin the target page(s) */
+			ret = get_user_pages(dest, dest_mm, mem.dest_va
+				+ bytes_copied, 1, 1, 1, &dest_page_low, &dest_vma_low);
+
+			if (ret <= 0)
+				goto out;
+
+			/* map into kmem */
+			dest_addr_low = kmap(dest_page_low);
+		}
+
+		/* see if we need to map the high page as well */
+		dest_offset = (mem.dest_va + bytes_copied) & (PAGE_SIZE-1);
+
+		if (!to_current && PAGE_SIZE-dest_offset < to_copy)
+		{
+			ret = get_user_pages(dest, dest_mm, mem.dest_va
+				+ bytes_copied + PAGE_SIZE, 1, 1, 1,
+				&dest_page_high, &dest_vma_high);
+
+			if (ret <= 0)
+				goto out;
+
+			to_copy_low 	= PAGE_SIZE - dest_offset;
+			to_copy_high 	= to_copy - to_copy_low;
+
+			/* map into kmem */
+			dest_addr_high = kmap(dest_page_high);
+		}
+
+		/* flush cache entries from datacache */
+		if (source_vma)
+			flush_cache_page(source_vma, mem.source_va + bytes_copied, page_to_pfn(source_page));
+		if (dest_vma_low)
+			flush_cache_page(dest_vma_low, mem.dest_va + bytes_copied, page_to_pfn(dest_page_low));
+		if (dest_vma_high)
+			flush_cache_page(dest_vma_high, mem.dest_va + bytes_copied + PAGE_SIZE, page_to_pfn(dest_page_high));
+
+		/* copy bytes */
+		if (from_current)
+		{
+			copy_from_user(dest_addr_low + dest_offset, (void __user*)mem.source_va + bytes_copied, to_copy_low);
+			if (to_copy_high > 0)
+				copy_from_user(dest_addr_high, (void __user*)mem.source_va + bytes_copied + to_copy_low, to_copy_high);
+		}
+		else if (to_current)
+		{
+			copy_to_user((void __user*)mem.dest_va + bytes_copied, source_addr + source_offset, to_copy);
+		}
+		else
+		{
+			memcpy(dest_addr_low + dest_offset, source_addr + source_offset, to_copy_low);
+			if (to_copy_high > 0)
+				memcpy(dest_addr_high, source_addr + source_offset + to_copy_low, to_copy_high);
+		}
+
+		/* flush instruction cache entries and mark pages dirty (if needed) */
+		if (!to_current)
+		{
+			flush_icache_user_range(dest_vma_low, dest_page_low, mem.dest_va + bytes_copied, to_copy_low);
+			set_page_dirty_lock(dest_page_low);
+
+			if (to_copy_high > 0)
+			{
+				flush_icache_user_range(dest_vma_high, dest_page_high, mem.dest_va + bytes_copied + to_copy_low, to_copy_high);
+				set_page_dirty_lock(dest_page_high);
+			}
+		}
+
+		/* unmap pages */
+		if (!from_current)
+		  {
+			kunmap(source_page);
+			put_page(source_page);
+		  }
+		if (!to_current)
+		{
+			kunmap(dest_page_low);
+			put_page(dest_page_low);
+			if (to_copy_high > 0)
+			  {
+				kunmap(dest_page_high);
+				put_page(dest_page_high);
+			  }
+		}
+
+		/* adjust counters */
+		bytes_copied 	+= to_copy;
+		bytes_remaining -= to_copy;
+
+		/* cleanup */
+		source_page = dest_page_low = dest_page_high = NULL;
+	}
+
+	ret = bytes_copied;
+
+out:
+	/* release all locks and references */
+	if (!to_current)
+	{
+		up_read(&dest_mm->mmap_sem);
+		put_task_struct(dest);
+		mmput(dest_mm);
+	}
+	if (!from_current)
+	{
+		up_read(&source_mm->mmap_sem);
+		put_task_struct(source);
+		mmput(source_mm);
+	}
+
+out_ret:
+	return ret;
+}
+
+int ptrace_copystring(struct task_struct* child, void __user* datavp)
+{
+	struct mm_struct*	source_mm	= NULL;
+	struct page* 		source_page	= NULL;
+	struct vm_area_struct*	source_vma	= NULL;
+	void*			source_addr	= NULL;
+	unsigned long 		source_offset	= 0;
+	unsigned long		len		= 0;		/* length of the string, including the NUL byte */
+	unsigned long		i 		= 0;
+	int			ret		= 0;
+	struct pt_copystring	mem;
+
+	/* Fetch dest buffer info */
+	if (unlikely(copy_from_user(&mem, datavp, sizeof(struct pt_copystring))))
+		return -EFAULT;
+
+	/* Get mm ref */
+	source_mm = get_task_mm(child);
+	if (unlikely(!source_mm))
+		goto out_ret;
+	down_read(&source_mm->mmap_sem);
+
+	/* determine string length */
+	while (true)
+	{
+		ret = get_user_pages(child, source_mm, mem.source_va
+			+ len, 1, 0, 1, &source_page, &source_vma);
+
+		if (unlikely(ret <= 0))
+			goto out;
+
+		flush_cache_page(source_vma, mem.source_va + len, page_to_pfn(source_page));
+
+		source_addr = kmap(source_page);
+		source_offset = (mem.source_va + len) & (PAGE_SIZE-1);
+
+		for (i = source_offset; i < PAGE_SIZE; ++i)
+			if (*(char*)((unsigned long)source_addr + i) == '\0')
+				break;
+
+		len += i - source_offset;
+		if (i < PAGE_SIZE)
+		{
+			len++;
+			kunmap(source_page);
+			put_page(source_page);
+			break;
+		}
+
+		kunmap(source_page);
+		put_page(source_page);
+	}
+
+	//printk("string copy - determined length: %d - dest buffer size: %d\n", len, mem.dest_buffer_size);
+
+	/* pass string size to user space */
+	mem.out_string_size = len;
+	copy_to_user(datavp, &mem, sizeof(struct pt_copystring));
+
+	/* see if it will fit into our destination buffer */
+	if (unlikely(len > mem.dest_buffer_size))
+	{
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	/* ok, it fits. Determine how many pages we need to map in to copy the entire thing in one go */
+	int bytes_remaining = len;
+	int bytes_copied = 0;
+	int to_copy = 0;
+
+	while (bytes_remaining > 0)
+	{
+		ret = get_user_pages(child, source_mm, mem.source_va
+			+ bytes_copied, 1, 0, 1, &source_page, &source_vma);
+		if (unlikely(ret <= 0))
+			goto out;
+
+		source_addr 	= kmap(source_page);
+		source_offset 	= (mem.source_va + bytes_copied) & (PAGE_SIZE-1);
+		to_copy 	= bytes_remaining > (PAGE_SIZE - source_offset) ? (PAGE_SIZE - source_offset) : bytes_remaining;
+
+		if (unlikely(copy_to_user((void*)(mem.dest_buffer_va + bytes_copied),
+			(const void*)((unsigned long)source_addr + source_offset),
+			to_copy)))
+		{
+			ret = -EFAULT;
+			kunmap(source_page);
+			put_page(source_page);
+			goto out;
+		}
+
+		bytes_remaining -= to_copy;
+		bytes_copied 	+= to_copy;
+
+		kunmap(source_page);
+		put_page(source_page);
+	}
+
+out:
+	up_read(&source_mm->mmap_sem);
+	mmput(source_mm);
+
+out_ret:
+	return ret;
+}
+
 int ptrace_request(struct task_struct *child, long request,
 		   unsigned long addr, unsigned long data)
 {
@@ -669,6 +978,20 @@
 		break;
 	}
 #endif
+	case PTRACE_EXT_MPROTECT:
+	{
+		struct pt_mprotect _pt_mprotect;
+
+		if (unlikely(copy_from_user(&_pt_mprotect, datavp, sizeof(struct pt_mprotect))))
+		{
+			ret = -EFAULT;
+			break;
+		}
+
+		ret = do_mprotect(child, _pt_mprotect.mem_start, _pt_mprotect.mem_len, _pt_mprotect.mem_prot);
+		break;
+	}
+
 	default:
 		break;
 	}
@@ -676,7 +999,7 @@
 	return ret;
 }
 
-static struct task_struct *ptrace_get_task_struct(pid_t pid)
+struct task_struct *ptrace_get_task_struct(pid_t pid)
 {
 	struct task_struct *child;
 
@@ -707,6 +1030,13 @@
 			arch_ptrace_attach(current);
 		goto out;
 	}
+	/*
+	 * Architecture independent extension for Multi-Variant Execution
+	 */
+	else if (request == PTRACE_EXT_COPYMEM)
+	{
+		return ptrace_copymem((void __user*)data);
+	}
 
 	child = ptrace_get_task_struct(pid);
 	if (IS_ERR(child)) {
@@ -714,7 +1044,12 @@
 		goto out;
 	}
 
-	if (request == PTRACE_ATTACH) {
+	if (request == PTRACE_EXT_COPYSTRING)
+	{
+		return ptrace_copystring(child, (void __user*)data);
+	}
+	else if (request == PTRACE_ATTACH)
+	{
 		ret = ptrace_attach(child);
 		/*
 		 * Some architectures need to do book-keeping after
diff -r --unified linux-source-2.6.38/Makefile linux-source-2.6.38-MVEE//Makefile
--- linux-source-2.6.38/Makefile	2011-07-29 21:04:36.000000000 +0200
+++ linux-source-2.6.38-MVEE//Makefile	2011-09-21 01:25:59.064893925 +0200
@@ -1,7 +1,7 @@
 VERSION = 2
 PATCHLEVEL = 6
 SUBLEVEL = 38
-EXTRAVERSION = .8
+EXTRAVERSION = .8-MVEE
 NAME = Flesh-Eating Bats with Fangs
 
 # *DOCUMENTATION*
diff -r --unified linux-source-2.6.38/mm/mprotect.c linux-source-2.6.38-MVEE//mm/mprotect.c
--- linux-source-2.6.38/mm/mprotect.c	2011-07-29 21:04:36.000000000 +0200
+++ linux-source-2.6.38-MVEE//mm/mprotect.c	2011-10-13 00:14:02.378776638 +0200
@@ -147,7 +147,8 @@
 }
 
 int
-mprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,
+mprotect_fixup(struct task_struct* task,
+	struct vm_area_struct *vma, struct vm_area_struct **pprev,
 	unsigned long start, unsigned long end, unsigned long newflags)
 {
 	struct mm_struct *mm = vma->vm_mm;
@@ -219,7 +220,7 @@
 	}
 
 	if (oldflags & VM_EXEC)
-		arch_remove_exec_range(current->mm, old_end);
+		arch_remove_exec_range(task->mm, old_end);
 
 	mmu_notifier_invalidate_range_start(mm, start, end);
 	if (is_vm_hugetlb_page(vma))
@@ -237,8 +238,7 @@
 	return error;
 }
 
-SYSCALL_DEFINE3(mprotect, unsigned long, start, size_t, len,
-		unsigned long, prot)
+int do_mprotect(struct task_struct* task, unsigned long start, size_t len, unsigned long prot)
 {
 	unsigned long vm_flags, nstart, end, tmp, reqprot;
 	struct vm_area_struct *vma, *prev;
@@ -263,14 +263,14 @@
 	/*
 	 * Does the application expect PROT_READ to imply PROT_EXEC:
 	 */
-	if ((prot & PROT_READ) && (current->personality & READ_IMPLIES_EXEC))
+	if ((prot & PROT_READ) && (task->personality & READ_IMPLIES_EXEC))
 		prot |= PROT_EXEC;
 
 	vm_flags = calc_vm_prot_bits(prot);
 
-	down_write(&current->mm->mmap_sem);
+	down_write(&task->mm->mmap_sem);
 
-	vma = find_vma_prev(current->mm, start, &prev);
+	vma = find_vma_prev(task->mm, start, &prev);
 	error = -ENOMEM;
 	if (!vma)
 		goto out;
@@ -315,7 +315,7 @@
 		tmp = vma->vm_end;
 		if (tmp > end)
 			tmp = end;
-		error = mprotect_fixup(vma, &prev, nstart, tmp, newflags);
+		error = mprotect_fixup(task, vma, &prev, nstart, tmp, newflags);
 		if (error)
 			goto out;
 		nstart = tmp;
@@ -332,6 +332,12 @@
 		}
 	}
 out:
-	up_write(&current->mm->mmap_sem);
+	up_write(&task->mm->mmap_sem);
 	return error;
 }
+
+SYSCALL_DEFINE3(mprotect, unsigned long, start, size_t, len,
+		unsigned long, prot)
+{
+	return do_mprotect(current, start, len, prot);
+}
